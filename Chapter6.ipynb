{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48e02e8",
      "metadata": {
        "id": "f48e02e8"
      },
      "source": [
        "## 6장 로컬에 배포하기\n",
        "\n",
        "### 스포일러\n",
        "이 장에서는 다음과 같은 내용을 배웁니다.\n",
        "\n",
        "- 빠른 추론을 위해 어댑터를 로드하여 베이스 모델과 병합합니다.\n",
        "- 모델을 사용해 응답 또는 완성을 생성합니다.\n",
        "- 미세 튜닝된 모델을 llama.cpp에서 사용하는 GGUF 파일 포맷으로 변환합니다.\n",
        "- Ollama와 llama.cpp를 사용하고 웹 인터페이스와 REST API를 통해 모델을 서빙(serving)합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245ef1a5",
      "metadata": {
        "id": "245ef1a5"
      },
      "source": [
        "### 패키지 설치\n",
        "\n",
        "훈련 재현성을 위해 이 책에서 사용하는 다음 버전과 동일 버전을 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c353825",
      "metadata": {
        "id": "6c353825"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.55.2 peft==0.17.0 accelerate==1.10.0 trl==0.21.0 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba6a8fd",
      "metadata": {
        "id": "aba6a8fd"
      },
      "source": [
        "### 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd09a846",
      "metadata": {
        "id": "bd09a846"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import asdict\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM, get_model_status, \\\n",
        "    get_layer_status, prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1228fd66",
      "metadata": {
        "id": "1228fd66"
      },
      "outputs": [],
      "source": [
        "# 깃허브에서 helper_functions.py 파일을 다운로드합니다.\n",
        "!wget https://raw.githubusercontent.com/rickiepark/fine-tuning-llm/refs/heads/main/helper_functions.py\n",
        "\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810fecf6",
      "metadata": {
        "id": "810fecf6"
      },
      "source": [
        "### 목표\n",
        "\n",
        "(GPU가 없는) 개인용 하드웨어에서 실행할 수 있도록 미세 튜닝된 모델과 어댑터를 GGUF 포맷으로 바꾼 다음 양자화합니다. 그다음 이런 모델과 어댑터를 서빙하기 위해 Ollama나 llama.cpp로 로드합니다. 이렇게 하면 웹 인터페이스나 REST API를 사용해 모델에게 직접 쿼리(query)를 보낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d4d2fca",
      "metadata": {
        "id": "5d4d2fca"
      },
      "source": [
        "### 준비 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afd7b7e",
      "metadata": {
        "id": "3afd7b7e"
      },
      "outputs": [],
      "source": [
        "# 2장\n",
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "model_q4 = AutoModelForCausalLM.from_pretrained(\n",
        "  \"facebook/opt-350m\", device_map='cuda:0', quantization_config=nf4_config\n",
        ")\n",
        "# 3장\n",
        "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "peft_model = get_peft_model(model_q4, config)\n",
        "\n",
        "# 4장\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = modify_tokenizer(tokenizer)\n",
        "tokenizer = add_template(tokenizer)\n",
        "\n",
        "peft_model = modify_model(peft_model, tokenizer)\n",
        "\n",
        "32 dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "33 dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "34 dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "35 dataset = dataset.remove_columns([\"translation\"])\n",
        "# 프롬프트/완성 쌍을 대화 메시지로 변환합니다.\n",
        "dataset = dataset.map(format_dataset)\n",
        "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
        "\n",
        "# 5장\n",
        "min_effective_batch_size = 8\n",
        "lr = 3e-4\n",
        "max_seq_length = 64\n",
        "collator_fn = None\n",
        "packing = (collator_fn is None)\n",
        "steps = 20\n",
        "num_train_epochs = 10\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir='./future_name_on_the_hub',\n",
        "    # 데이터셋\n",
        "    packing=packing,\n",
        "    packing_strategy='wrapped',\n",
        "    max_length=max_seq_length,\n",
        "    # 그레이디언트 / 메모리\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "    gradient_accumulation_steps=2,\n",
        "    per_device_train_batch_size=min_effective_batch_size,\n",
        "    auto_find_batch_size=True,\n",
        "    # 훈련\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=lr,\n",
        "    # 환경 및 로깅\n",
        "    report_to='tensorboard',\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=steps,\n",
        "    save_strategy='steps',\n",
        "    save_steps=steps,\n",
        "    bf16=supported\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model.base_model.model,\n",
        "    peft_config=config,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collator_fn,\n",
        "    args=sft_config\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model('yoda-adapter') # trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ecc5796",
      "metadata": {
        "id": "6ecc5796"
      },
      "source": [
        "### 모델과 어댑터를 로드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c331d06",
      "metadata": {
        "id": "2c331d06"
      },
      "outputs": [],
      "source": [
        "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(repo_or_folder,\n",
        "                                                 device_map='auto',\n",
        "                                                 adapter_name='yoda')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "**중요**: 현재는 토크나이저의 어휘사전 크기가 임베딩 층의 크기를 넘어서는 경우에만 임베딩 층의 크기가 변경됩니다. 따라서 책에서 언급한 오랜 문제가 해결되었습니다.\n",
        "\n",
        "이 문제 때문에 다음처럼 어댑터의 LoRA 설정에 따라 베이스 모델을 로드하고, PeftModel 클래스를 사용해 모델과 어댑터를 병합해야 했습니다.\n",
        "\n",
        "```python\n",
        "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
        "config = PeftConfig.from_pretrained(repo_or_folder)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "  config.base_model_name_or_path,\n",
        "  device_map='auto'\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "  base_model,\n",
        "  repo_or_folder,\n",
        "  adapter_name='yoda'\n",
        ")\n",
        "```\n",
        "****"
      ],
      "metadata": {
        "id": "95fe9d58"
      },
      "id": "95fe9d58"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83cda575",
      "metadata": {
        "id": "83cda575"
      },
      "outputs": [],
      "source": [
        "model.merge_adapter(['yoda'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf03e85",
      "metadata": {
        "id": "cbf03e85"
      },
      "outputs": [],
      "source": [
        "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_or_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411bc34f",
      "metadata": {
        "id": "411bc34f",
        "outputId": "c4582c29-bbb5-4616-d281-4ad61a41fa3e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>module_type</th>\n",
              "      <th>enabled</th>\n",
              "      <th>active_adapters</th>\n",
              "      <th>merged_adapters</th>\n",
              "      <th>requires_grad</th>\n",
              "      <th>available_adapters</th>\n",
              "      <th>devices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model.model.decoder.layers.0.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>model.model.decoder.layers.0.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>model.model.decoder.layers.1.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>model.model.decoder.layers.1.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>model.model.decoder.layers.2.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>model.model.decoder.layers.2.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>model.model.decoder.layers.3.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>model.model.decoder.layers.3.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>model.model.decoder.layers.4.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>model.model.decoder.layers.4.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>model.model.decoder.layers.5.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>model.model.decoder.layers.5.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>model.model.decoder.layers.6.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>model.model.decoder.layers.6.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>model.model.decoder.layers.7.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>model.model.decoder.layers.7.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>model.model.decoder.layers.8.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>model.model.decoder.layers.8.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>model.model.decoder.layers.9.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>model.model.decoder.layers.9.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>model.model.decoder.layers.10.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>model.model.decoder.layers.10.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>model.model.decoder.layers.11.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>model.model.decoder.layers.11.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>model.model.decoder.layers.12.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>model.model.decoder.layers.12.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>model.model.decoder.layers.13.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>model.model.decoder.layers.13.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>model.model.decoder.layers.14.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>model.model.decoder.layers.14.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>model.model.decoder.layers.15.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>model.model.decoder.layers.15.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>model.model.decoder.layers.16.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>model.model.decoder.layers.16.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>model.model.decoder.layers.17.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>model.model.decoder.layers.17.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>model.model.decoder.layers.18.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>model.model.decoder.layers.18.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>model.model.decoder.layers.19.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>model.model.decoder.layers.19.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>model.model.decoder.layers.20.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>model.model.decoder.layers.20.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>model.model.decoder.layers.21.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>model.model.decoder.layers.21.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>model.model.decoder.layers.22.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>model.model.decoder.layers.22.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>model.model.decoder.layers.23.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>model.model.decoder.layers.23.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              name  module_type  enabled  \\\n",
              "0    model.model.decoder.layers.0.self_attn.v_proj  lora.Linear     True   \n",
              "1    model.model.decoder.layers.0.self_attn.q_proj  lora.Linear     True   \n",
              "2    model.model.decoder.layers.1.self_attn.v_proj  lora.Linear     True   \n",
              "3    model.model.decoder.layers.1.self_attn.q_proj  lora.Linear     True   \n",
              "4    model.model.decoder.layers.2.self_attn.v_proj  lora.Linear     True   \n",
              "5    model.model.decoder.layers.2.self_attn.q_proj  lora.Linear     True   \n",
              "6    model.model.decoder.layers.3.self_attn.v_proj  lora.Linear     True   \n",
              "7    model.model.decoder.layers.3.self_attn.q_proj  lora.Linear     True   \n",
              "8    model.model.decoder.layers.4.self_attn.v_proj  lora.Linear     True   \n",
              "9    model.model.decoder.layers.4.self_attn.q_proj  lora.Linear     True   \n",
              "10   model.model.decoder.layers.5.self_attn.v_proj  lora.Linear     True   \n",
              "11   model.model.decoder.layers.5.self_attn.q_proj  lora.Linear     True   \n",
              "12   model.model.decoder.layers.6.self_attn.v_proj  lora.Linear     True   \n",
              "13   model.model.decoder.layers.6.self_attn.q_proj  lora.Linear     True   \n",
              "14   model.model.decoder.layers.7.self_attn.v_proj  lora.Linear     True   \n",
              "15   model.model.decoder.layers.7.self_attn.q_proj  lora.Linear     True   \n",
              "16   model.model.decoder.layers.8.self_attn.v_proj  lora.Linear     True   \n",
              "17   model.model.decoder.layers.8.self_attn.q_proj  lora.Linear     True   \n",
              "18   model.model.decoder.layers.9.self_attn.v_proj  lora.Linear     True   \n",
              "19   model.model.decoder.layers.9.self_attn.q_proj  lora.Linear     True   \n",
              "20  model.model.decoder.layers.10.self_attn.v_proj  lora.Linear     True   \n",
              "21  model.model.decoder.layers.10.self_attn.q_proj  lora.Linear     True   \n",
              "22  model.model.decoder.layers.11.self_attn.v_proj  lora.Linear     True   \n",
              "23  model.model.decoder.layers.11.self_attn.q_proj  lora.Linear     True   \n",
              "24  model.model.decoder.layers.12.self_attn.v_proj  lora.Linear     True   \n",
              "25  model.model.decoder.layers.12.self_attn.q_proj  lora.Linear     True   \n",
              "26  model.model.decoder.layers.13.self_attn.v_proj  lora.Linear     True   \n",
              "27  model.model.decoder.layers.13.self_attn.q_proj  lora.Linear     True   \n",
              "28  model.model.decoder.layers.14.self_attn.v_proj  lora.Linear     True   \n",
              "29  model.model.decoder.layers.14.self_attn.q_proj  lora.Linear     True   \n",
              "30  model.model.decoder.layers.15.self_attn.v_proj  lora.Linear     True   \n",
              "31  model.model.decoder.layers.15.self_attn.q_proj  lora.Linear     True   \n",
              "32  model.model.decoder.layers.16.self_attn.v_proj  lora.Linear     True   \n",
              "33  model.model.decoder.layers.16.self_attn.q_proj  lora.Linear     True   \n",
              "34  model.model.decoder.layers.17.self_attn.v_proj  lora.Linear     True   \n",
              "35  model.model.decoder.layers.17.self_attn.q_proj  lora.Linear     True   \n",
              "36  model.model.decoder.layers.18.self_attn.v_proj  lora.Linear     True   \n",
              "37  model.model.decoder.layers.18.self_attn.q_proj  lora.Linear     True   \n",
              "38  model.model.decoder.layers.19.self_attn.v_proj  lora.Linear     True   \n",
              "39  model.model.decoder.layers.19.self_attn.q_proj  lora.Linear     True   \n",
              "40  model.model.decoder.layers.20.self_attn.v_proj  lora.Linear     True   \n",
              "41  model.model.decoder.layers.20.self_attn.q_proj  lora.Linear     True   \n",
              "42  model.model.decoder.layers.21.self_attn.v_proj  lora.Linear     True   \n",
              "43  model.model.decoder.layers.21.self_attn.q_proj  lora.Linear     True   \n",
              "44  model.model.decoder.layers.22.self_attn.v_proj  lora.Linear     True   \n",
              "45  model.model.decoder.layers.22.self_attn.q_proj  lora.Linear     True   \n",
              "46  model.model.decoder.layers.23.self_attn.v_proj  lora.Linear     True   \n",
              "47  model.model.decoder.layers.23.self_attn.q_proj  lora.Linear     True   \n",
              "\n",
              "   active_adapters merged_adapters    requires_grad available_adapters  \\\n",
              "0           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "1           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "2           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "3           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "4           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "5           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "6           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "7           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "8           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "9           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "10          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "11          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "12          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "13          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "14          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "15          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "16          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "17          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "18          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "19          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "20          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "21          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "22          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "23          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "24          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "25          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "26          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "27          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "28          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "29          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "30          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "31          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "32          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "33          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "34          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "35          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "36          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "37          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "38          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "39          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "40          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "41          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "42          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "43          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "44          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "45          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "46          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "47          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "\n",
              "               devices  \n",
              "0   {'yoda': ['cuda']}  \n",
              "1   {'yoda': ['cuda']}  \n",
              "2   {'yoda': ['cuda']}  \n",
              "3   {'yoda': ['cuda']}  \n",
              "4   {'yoda': ['cuda']}  \n",
              "5   {'yoda': ['cuda']}  \n",
              "6   {'yoda': ['cuda']}  \n",
              "7   {'yoda': ['cuda']}  \n",
              "8   {'yoda': ['cuda']}  \n",
              "9   {'yoda': ['cuda']}  \n",
              "10  {'yoda': ['cuda']}  \n",
              "11  {'yoda': ['cuda']}  \n",
              "12  {'yoda': ['cuda']}  \n",
              "13  {'yoda': ['cuda']}  \n",
              "14  {'yoda': ['cuda']}  \n",
              "15  {'yoda': ['cuda']}  \n",
              "16  {'yoda': ['cuda']}  \n",
              "17  {'yoda': ['cuda']}  \n",
              "18  {'yoda': ['cuda']}  \n",
              "19  {'yoda': ['cuda']}  \n",
              "20  {'yoda': ['cuda']}  \n",
              "21  {'yoda': ['cuda']}  \n",
              "22  {'yoda': ['cuda']}  \n",
              "23  {'yoda': ['cuda']}  \n",
              "24  {'yoda': ['cuda']}  \n",
              "25  {'yoda': ['cuda']}  \n",
              "26  {'yoda': ['cuda']}  \n",
              "27  {'yoda': ['cuda']}  \n",
              "28  {'yoda': ['cuda']}  \n",
              "29  {'yoda': ['cuda']}  \n",
              "30  {'yoda': ['cuda']}  \n",
              "31  {'yoda': ['cuda']}  \n",
              "32  {'yoda': ['cuda']}  \n",
              "33  {'yoda': ['cuda']}  \n",
              "34  {'yoda': ['cuda']}  \n",
              "35  {'yoda': ['cuda']}  \n",
              "36  {'yoda': ['cuda']}  \n",
              "37  {'yoda': ['cuda']}  \n",
              "38  {'yoda': ['cuda']}  \n",
              "39  {'yoda': ['cuda']}  \n",
              "40  {'yoda': ['cuda']}  \n",
              "41  {'yoda': ['cuda']}  \n",
              "42  {'yoda': ['cuda']}  \n",
              "43  {'yoda': ['cuda']}  \n",
              "44  {'yoda': ['cuda']}  \n",
              "45  {'yoda': ['cuda']}  \n",
              "46  {'yoda': ['cuda']}  \n",
              "47  {'yoda': ['cuda']}  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(asdict(layer) for layer in get_layer_status(model))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba40822b",
      "metadata": {
        "id": "ba40822b",
        "outputId": "05de9e17-b212-411e-95cc-ac01f92b05e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TunerModelStatus(base_model_type='OPTForCausalLM', adapter_model_type='LoraModel', peft_types={'yoda': 'LORA'}, trainable_params=0, total_params=331982848, num_adapter_layers=48, enabled=True, active_adapters=['yoda'], merged_adapters=['yoda'], requires_grad={'yoda': False}, available_adapters=['yoda'], devices={'yoda': ['cuda']})\n"
          ]
        }
      ],
      "source": [
        "print(get_model_status(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74c72fb",
      "metadata": {
        "id": "a74c72fb",
        "outputId": "93d752ce-3449-41f4-f6f4-76036a30976e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fb1c59",
      "metadata": {
        "id": "66fb1c59"
      },
      "source": [
        "### 모델에 쿼리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c663f21f",
      "metadata": {
        "id": "c663f21f"
      },
      "outputs": [],
      "source": [
        "def gen_prompt(tokenizer, sentence):\n",
        "    converted_sample = [\n",
        "        {\"role\": \"user\", \"content\": sentence},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
        "                                           tokenize=False,\n",
        "                                           add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41066fe7",
      "metadata": {
        "id": "41066fe7",
        "outputId": "07d78c9b-3e9b-4c85-c81f-d95cabcfce61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "There is bacon in this sandwich.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = gen_prompt(tokenizer, 'There is bacon in this sandwich.')\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bb4da4",
      "metadata": {
        "id": "97bb4da4"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt,\n",
        "             max_new_tokens=64,\n",
        "             skip_special_tokens=False,\n",
        "             response_only=False):\n",
        "    # 포맷팅된 프롬프트를 토큰화합니다.\n",
        "    tokenized_input = tokenizer(prompt,\n",
        "                                add_special_tokens=False,\n",
        "                                return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    # 혼합 정밀도를 사용해 훈련하는 경우 autocast 컨택스트를 사용합니다.\n",
        "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
        "        if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
        "    with ctx:\n",
        "        generation_output = model.generate(**tokenized_input,\n",
        "                                           eos_token_id=tokenizer.eos_token_id,\n",
        "                                           max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # 필요한 경우 프롬프트에 속한 토큰을 제외합니다.\n",
        "    if response_only:\n",
        "        input_length = tokenized_input['input_ids'].shape[1]\n",
        "        generation_output = generation_output[:, input_length:]\n",
        "\n",
        "    # 토큰을 다시 텍스트로 디코딩합니다.\n",
        "    output = tokenizer.batch_decode(generation_output,\n",
        "                                    skip_special_tokens=skip_special_tokens)[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4b5b77",
      "metadata": {
        "id": "2b4b5b77",
        "outputId": "6b70fcf8-98d2-4932-d87c-b601c113d88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "There is bacon in this sandwich.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "In this sandwich, bacon there is.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, tokenizer,prompt, skip_special_tokens=False, response_only=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452a830e",
      "metadata": {
        "id": "452a830e",
        "outputId": "ec4f1fea-666a-436a-e1f6-fc4799282f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In this sandwich, bacon there is.\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, tokenizer,prompt, skip_special_tokens=True, response_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b919d66",
      "metadata": {
        "id": "5b919d66"
      },
      "outputs": [],
      "source": [
        "sentences  = ['There is bacon in this sandwich.', 'Add some cheddar to it.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17acc29f",
      "metadata": {
        "id": "17acc29f"
      },
      "outputs": [],
      "source": [
        "def batch_generate(model, tokenizer, sentences,\n",
        "             max_new_tokens=64,\n",
        "             skip_special_tokens=False,\n",
        "             response_only=False):\n",
        "\n",
        "    # Converts prompts into conversational format\n",
        "    converted_samples = [[{\"role\": \"user\", \"content\": sentence}]\n",
        "                         for sentence in sentences]\n",
        "\n",
        "    # Applies the chat template to format the prompts\n",
        "    prompts = tokenizer.apply_chat_template(converted_samples,\n",
        "                                            tokenize=False,\n",
        "                                            add_generation_prompt=True)\n",
        "\n",
        "    # Forces padding to the left for batch generation\n",
        "    tokenizer.padding_side = 'left'\n",
        "    # Tokenizes the formatted prompts with padding\n",
        "    tokenized_inputs = tokenizer(prompts,\n",
        "                                 padding=True,\n",
        "                                 add_special_tokens=False,\n",
        "                                 return_tensors='pt').to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    # Generates the responses/completions\n",
        "    generation_output = model.generate(**tokenized_inputs,\n",
        "                                       eos_token_id=tokenizer.eos_token_id,\n",
        "                                       pad_token_id=tokenizer.pad_token_id,\n",
        "                                       max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # If required, removes the tokens belonging to the prompts\n",
        "    if response_only:\n",
        "        input_length = tokenized_inputs['input_ids'].shape[1]\n",
        "        generation_output = generation_output[:, input_length:]\n",
        "\n",
        "    # Decodes the tokens back into text\n",
        "    output = tokenizer.batch_decode(generation_output,\n",
        "                                    skip_special_tokens=skip_special_tokens)\n",
        "    if isinstance(sentences, str):\n",
        "        output = output[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee75635c",
      "metadata": {
        "id": "ee75635c",
        "outputId": "aaba9101-fa7c-488a-a576-7f6b2a954e5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In this sandwich, bacon there is.', 'To it, add some cheddar, you must.']"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_generate(model, tokenizer, sentences, skip_special_tokens=True, response_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dd75791",
      "metadata": {
        "id": "7dd75791"
      },
      "source": [
        "### Llama.cpp\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp.png?raw=True)\n",
        "\n",
        "<center>Figure 6.1 - Screenshot of llama.cpp’s GitHub Repo</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d9754e",
      "metadata": {
        "id": "07d9754e"
      },
      "source": [
        "#### Converting Adapters\n",
        "\n",
        "In order to convert an adapter to the GGUF format, we need to do the following:\n",
        "\n",
        "- save the adapter to a local folder, either by calling the `save_model()` method after training, as we did in the last chapter, or by downloading it from the Hugging Face Hub (see the aside for details)\n",
        "- clone the llama.cpp repository from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197d2fad",
      "metadata": {
        "id": "197d2fad"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fdbc969",
      "metadata": {
        "id": "0fdbc969"
      },
      "source": [
        "- install the `gguf-py` package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c870a1",
      "metadata": {
        "id": "52c870a1"
      },
      "outputs": [],
      "source": [
        "!pip install llama.cpp/gguf-py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ccc7e0",
      "metadata": {
        "id": "f9ccc7e0"
      },
      "source": [
        "***\n",
        "**Downloading Models from the Hub**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1493a2f",
      "metadata": {
        "id": "f1493a2f"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d37117",
      "metadata": {
        "id": "f3d37117"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"dvgodoy/phi3-mini-yoda-adapter\", local_dir='./phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bccb35f0",
      "metadata": {
        "id": "bccb35f0"
      },
      "source": [
        "***\n",
        "\n",
        "- run the `convert_lora_to_gguf.py` script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a621c7be",
      "metadata": {
        "id": "a621c7be"
      },
      "outputs": [],
      "source": [
        "!python ./llama.cpp/convert_lora_to_gguf.py \\\n",
        "        ./phi3-mini-yoda-adapter \\\n",
        "        --outfile adapter.gguf \\\n",
        "        --outtype q8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13bdca4",
      "metadata": {
        "id": "f13bdca4"
      },
      "source": [
        "- the `outtype` may be one of the following choices: `f32`, `f16`, `bf16`, `q8_0`, or `auto`, which defaults to the highest-fidelity 16-bit float type depending on the first loaded tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f669cfb0",
      "metadata": {
        "id": "f669cfb0"
      },
      "source": [
        "#### Converting Full Models\n",
        "\n",
        "##### Using \"GGUF My Repo\"\n",
        "\n",
        "https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/gguf_my_repo.png?raw=True)\n",
        "\n",
        "<center>Figure 6.2 - Screenshot of \"GGUF My Repo\" space on Hugging Face</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3fa6074",
      "metadata": {
        "id": "b3fa6074"
      },
      "source": [
        "##### Using Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5aa020",
      "metadata": {
        "id": "6b5aa020"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16340ec",
      "metadata": {
        "id": "d16340ec",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained('dvgodoy/phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55572d7",
      "metadata": {
        "id": "f55572d7"
      },
      "source": [
        "```\n",
        "==((====))==  Unsloth 2024.10.0: Fast Mistral patching. Transformers = 4.44.2.\n",
        "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
        "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
        "\n",
        "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]\n",
        "\n",
        "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]\n",
        "\n",
        "tokenizer_config.json:   0%|          | 0.00/3.34k [00:00<?, ?B/s]\n",
        "\n",
        "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
        "\n",
        "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]\n",
        "\n",
        "special_tokens_map.json:   0%|          | 0.00/458 [00:00<?, ?B/s]\n",
        "\n",
        "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
        "\n",
        "adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]\n",
        "\n",
        "Unsloth 2024.10.0 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf0539a",
      "metadata": {
        "id": "ebf0539a"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f3fa7d",
      "metadata": {
        "id": "c3f3fa7d"
      },
      "source": [
        "```\n",
        "PeftModelForCausalLM(\n",
        "  (base_model): LoraModel(\n",
        "    (model): MistralForCausalLM(\n",
        "      (model): MistralModel(\n",
        "        (embed_tokens): Embedding(32064, 3072, padding_idx=32009)\n",
        "        (layers): ModuleList(\n",
        "          (0-31): 32 x MistralDecoderLayer(\n",
        "            (self_attn): MistralAttention(\n",
        "              (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "              (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "              (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "              (o_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Dropout(p=0.05, inplace=False)\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (rotary_emb): LlamaRotaryEmbedding()\n",
        "            )\n",
        "            (mlp): MistralMLP(\n",
        "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
        "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
        "              (down_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Dropout(p=0.05, inplace=False)\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
        "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
        "      )\n",
        "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
        "    )\n",
        "  )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdcfe3eb",
      "metadata": {
        "id": "fdcfe3eb"
      },
      "outputs": [],
      "source": [
        "# This command may fail for several reasons, as it depends on the environment\n",
        "# and the stability of llama.cpp (which is installed during its execution)\n",
        "\n",
        "# Removing the llama.cpp folder we cloned above, so Unsloth can install it on its own\n",
        "!rm -rf llama.cpp/\n",
        "\n",
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927ce724",
      "metadata": {
        "id": "927ce724"
      },
      "source": [
        " ```\n",
        "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
        "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
        "To force `safe_serialization`, set it to `None` instead.\n",
        "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
        "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
        "Unsloth: Will remove a cached repo with size 2.3G\n",
        "\n",
        "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
        "Unsloth: Will use up to 5.02 out of 12.67 RAM for saving.\n",
        "Unsloth: Saving tokenizer... Done.\n",
        "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
        "Unsloth: Saving gguf_model/pytorch_model-00001-of-00002.bin...\n",
        "Unsloth: Saving gguf_model/pytorch_model-00002-of-00002.bin...\n",
        "Done.\n",
        "\n",
        "Unsloth: Converting mistral model. Can use fast conversion = True.\n",
        "\n",
        "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
        "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
        "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
        "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
        " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
        "\n",
        "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
        "\n",
        "Unsloth: Extending gguf_model/tokenizer.model with added_tokens.json.\n",
        "Originally tokenizer.model is of size (32000).\n",
        "But we need to extend to sentencepiece vocab size (32011).\n",
        "\n",
        "Unsloth: [1] Converting model at gguf_model into f16 GGUF format.\n",
        "The output location will be /content/gguf_model/unsloth.F16.gguf\n",
        "This will take 3 minutes...\n",
        "INFO:hf-to-gguf:Loading model: gguf_model\n",
        "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
        "INFO:hf-to-gguf:Exporting model...\n",
        "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
        "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
        "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 32064}\n",
        "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
        "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
        "...\n",
        "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
        "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
        "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
        "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {3072, 32064}\n",
        "INFO:hf-to-gguf:Set meta model\n",
        "INFO:hf-to-gguf:Set model parameters\n",
        "INFO:hf-to-gguf:gguf: context length = 4096\n",
        "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
        "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
        "INFO:hf-to-gguf:gguf: head count = 32\n",
        "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
        "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
        "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
        "INFO:hf-to-gguf:gguf: file type = 1\n",
        "INFO:hf-to-gguf:Set model tokenizer\n",
        "INFO:gguf.vocab:Setting special token type bos to 1\n",
        "INFO:gguf.vocab:Setting special token type eos to 32000\n",
        "INFO:gguf.vocab:Setting special token type unk to 0\n",
        "INFO:gguf.vocab:Setting special token type pad to 32009\n",
        "INFO:gguf.vocab:Setting add_bos_token to False\n",
        "INFO:gguf.vocab:Setting add_eos_token to False\n",
        "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
        "' + message['content'] + '<|end|>\n",
        "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
        "' + message['content'] + '<|end|>\n",
        "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
        "' + message['content'] + '<|end|>\n",
        "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
        "' }}{% else %}{{ eos_token }}{% endif %}\n",
        "INFO:hf-to-gguf:Set model quantization version\n",
        "INFO:gguf.gguf_writer:Writing the following files:\n",
        "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.F16.gguf: n_tensors = 291, total_size = 7.6G\n",
        "Writing: 100%|██████████| 7.64G/7.64G [01:56<00:00, 65.5Mbyte/s]\n",
        "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.F16.gguf\n",
        "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.F16.gguf\n",
        "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
        "main: build = 3934 (3752217e)\n",
        "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
        "main: quantizing '/content/gguf_model/unsloth.F16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
        "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from /content/gguf_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
        "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
        "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
        "llama_model_loader: - kv   1:                               general.type str              = model\n",
        "llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 4k Instruct Bnb 4bit\n",
        "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
        "llama_model_loader: - kv   4:                           general.finetune str              = 4k-instruct-bnb-4bit\n",
        "llama_model_loader: - kv   5:                           general.basename str              = phi-3\n",
        "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
        "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
        "llama_model_loader: - kv   8:                       llama.context_length u32              = 4096\n",
        "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
        "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
        "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
        "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
        "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
        "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
        "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 96\n",
        "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 96\n",
        "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
        "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32064\n",
        "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 96\n",
        "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
        "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
        "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
        "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
        "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
        "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
        "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
        "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 32000\n",
        "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
        "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32009\n",
        "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = false\n",
        "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
        "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
        "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
        "llama_model_loader: - type  f32:   65 tensors\n",
        "llama_model_loader: - type  f16:  226 tensors\n",
        "[   1/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
        "[   2/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
        "...\n",
        "[ 290/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
        "[ 291/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
        "llama_model_quantize_internal: model size  =  7288.51 MB\n",
        "llama_model_quantize_internal: quant size  =  2210.78 MB\n",
        "\n",
        "main: quantize time = 426187.37 ms\n",
        "main:    total time = 426187.37 ms\n",
        "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94ddbf5",
      "metadata": {
        "id": "b94ddbf5"
      },
      "source": [
        "##### Using Docker Images\n",
        "\n",
        "To convert the model, we need to run the command below:\n",
        "\n",
        "```\n",
        "docker run --rm\n",
        "           -v \"/path/to/saved_model\":/repo\n",
        "           ghcr.io/ggerganov/llama.cpp:full\n",
        "           --convert \"/repo\"\n",
        "           --outtype f32\n",
        "           --outfile /repo/gguf-model-f32.gguf\n",
        "```\n",
        "\n",
        "1. `--rm`: It automatically removes the container from execution after it finishes running, which can be very\n",
        "useful in cases such as ours, where we’re only interested in running a script once.\n",
        "2. `-v [local path]:[path inside container]`: It maps a folder on your computer to a folder inside the\n",
        "container. This allows the container to \"see\" your local folder as if it were located inside the container\n",
        "itself.\n",
        "3. `[docker image]`: We’re using llama.cpp’s Docker image, ghcr.io/ggerganov/llama.cpp:full\n",
        "4. `--convert [path inside container]`: This is the command we’re executing—it isn’t a Docker command,\n",
        "but rather a command that’s available in the particular image we’re using.\n",
        "5. `--outtype [GGUF type]`: This is an argument of the --convert command that specifies the data type of the\n",
        "resulting GGUF file.\n",
        "6. `--outfile [GGUF filename]`: This is yet another argument of the --convert command. It specifies the\n",
        "name of the GGUF file (note that it points to a path inside the container—/repo—which was mapped to a\n",
        "folder on your local computer, so in the end, the file is generated directly in your local folder).\n",
        "\n",
        "To quantize the converted model, we need to run the following command:\n",
        "\n",
        "```\n",
        "docker run --rm\n",
        "           -v \"/path/to/saved_model\":/repo\n",
        "           ghcr.io/ggerganov/llama.cpp:full\n",
        "           --quantize \"/repo/gguf-model-f32.gguf\"\n",
        "           \"/repo/gguf-model-Q4_K_M.gguf\"\n",
        "           \"Q4_K_M\"\n",
        "```\n",
        "\n",
        "7. `--quantize [GGUF filename]`: This is the new command we’re executing, it is a command available in this\n",
        "particular image only, and it should specify which GGUF file is to be quantized (usually the outfile from\n",
        "the convert command).\n",
        "8. `[quantized GGFUF filename]`: the name of the quantized file after the script finishes; make sure to point to\n",
        "the mapped folder so you can access it directly in your local folder as well\n",
        "9. `[quantization type]`: For a full list of quantization types, please check the [documentation](https://github.com/ggerganov/llama.cpp/blob/main/examples/quantize/README.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94d1a2b",
      "metadata": {
        "id": "f94d1a2b"
      },
      "source": [
        "##### Building llama.cpp\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!pip install llama.cpp/gguf-py\n",
        "!pip install -r llama.cpp/requirements.txt\n",
        "```\n",
        "\n",
        "```python\n",
        "!python ./llama.cpp/convert_hf_to_gguf.py /path/to/saved_model --outtype f16\n",
        "```\n",
        "\n",
        "```python\n",
        "!cd llama.cpp && make clean && make\n",
        "```\n",
        "\n",
        "```python\n",
        "!./llama.cpp/quantize\n",
        "    ./path/to/saved_model/ggml-model-f16.gguf\n",
        "    ./path/to/saved_model/ggml-model-q4_0.gguf\n",
        "    q4_0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e120bbcd",
      "metadata": {
        "id": "e120bbcd"
      },
      "source": [
        "### Serving Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f76ca5a",
      "metadata": {
        "id": "0f76ca5a"
      },
      "source": [
        "#### Ollama\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/ollama.png?raw=True)\n",
        "<center>Figure 6.3 - Screenshot of Ollama’s page</center>\n",
        "\n",
        "```\n",
        "ollama run phi3:mini\n",
        "```\n",
        "\n",
        "##### Installing Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49c9062",
      "metadata": {
        "id": "b49c9062"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1574e2e2",
      "metadata": {
        "id": "1574e2e2"
      },
      "source": [
        "```\n",
        ">>> Installing ollama to /usr/local/bin...\n",
        ">>> Creating ollama user...\n",
        ">>> Adding ollama user to video group...\n",
        ">>> Adding current user to ollama group...\n",
        ">>> Creating ollama systemd service...\n",
        "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
        ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
        ">>> Install complete. Run \"ollama\" from the command line.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc9f0b5",
      "metadata": {
        "id": "dcc9f0b5"
      },
      "source": [
        "##### Running Ollama in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853f2ed6",
      "metadata": {
        "id": "853f2ed6"
      },
      "outputs": [],
      "source": [
        "# Adapter from https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import threading\n",
        "\n",
        "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
        "# Set environment variable for NVIDIA library\n",
        "# Set environment variables for CUDA\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75df9604",
      "metadata": {
        "id": "75df9604"
      },
      "outputs": [],
      "source": [
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee13ef1c",
      "metadata": {
        "id": "ee13ef1c"
      },
      "source": [
        "##### Model Files\n",
        "\n",
        "| Instruction |\tDescription |\n",
        "|---|---|\n",
        "|FROM (required) | Defines the base model to use. |\n",
        "|PARAMETER | Sets the parameters for how Ollama will run the model. |\n",
        "|TEMPLATE | The full prompt template to be sent to the model. |\n",
        "|SYSTEM | Specifies the system message that will be set in the template. |\n",
        "|ADAPTER | Defines the (Q)LoRA adapters to apply to the model. |\n",
        "|LICENSE | Specifies the legal license. |\n",
        "|MESSAGE | Specify message history. |\n",
        "\n",
        "```\n",
        "ollama show --modelfile phi3:mini\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# Modelfile generated by \"ollama show\"\n",
        "# To build a new Modelfile based on this, replace FROM with:\n",
        "# FROM phi3:mini\n",
        "\n",
        "FROM /usr/share/ollama/.ollama/models/blobs/sha256-633fc...\n",
        "TEMPLATE \"{{ if .System }}<|system|>\n",
        "{{ .System }}<|end|>\n",
        "{{ end }}{{ if .Prompt }}<|user|>\n",
        "{{ .Prompt }}<|end|>\n",
        "{{ end }}<|assistant|>\n",
        "{{ .Response }}<|end|>\"\n",
        "PARAMETER stop <|end|>\n",
        "PARAMETER stop <|user|>\n",
        "PARAMETER stop <|assistant|>\n",
        "LICENSE \"\"\"Microsoft.\n",
        "Copyright (c) Microsoft Corporation.\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03abae0",
      "metadata": {
        "id": "c03abae0",
        "outputId": "fb28b4c6-1494-4a36-de29-af9030551e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n"
          ]
        }
      ],
      "source": [
        "tokenizer_phi3 = AutoTokenizer.from_pretrained('microsoft/phi-3-mini-4k-instruct')\n",
        "print(tokenizer_phi3.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4012459",
      "metadata": {
        "id": "e4012459"
      },
      "source": [
        "##### Importing Models\n",
        "\n",
        "###### Custom (Full) Model File\n",
        "\n",
        "```python\n",
        "modelfile = \"\"\"\n",
        "FROM ./phi3-full-model\n",
        "TEMPLATE \"{{ if .System }}<|system|>\n",
        "{{ .System }}<|end|>\n",
        "{{ end }}{{ if .Prompt }}<|user|>\n",
        "{{ .Prompt }}<|end|>\n",
        "{{ end }}<|assistant|>\n",
        "{{ .Response }}<|end|>\"\n",
        "PARAMETER stop <|end|>\n",
        "PARAMETER stop <|user|>\n",
        "PARAMETER stop <|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "with open('phi3-full-modelfile', 'w') as f:\n",
        "    f.write(modelfile)\n",
        "```\n",
        "\n",
        "```\n",
        "!ollama create our_own_phi3 -f phi3-full-modelfile\n",
        "```\n",
        "\n",
        "```\n",
        "!ollama list\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a52507",
      "metadata": {
        "id": "62a52507"
      },
      "source": [
        "###### Custom Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ec65a7",
      "metadata": {
        "id": "80ec65a7"
      },
      "outputs": [],
      "source": [
        "adapterfile = \"\"\"\n",
        "FROM phi3:mini\n",
        "ADAPTER ./adapter.gguf\n",
        "TEMPLATE \"{{ if .System }}<|system|>\n",
        "{{ .System }}<|end|>\n",
        "{{ end }}{{ if .Prompt }}<|user|>\n",
        "{{ .Prompt }}<|end|>\n",
        "{{ end }}<|assistant|>\n",
        "{{ .Response }}<|end|>\"\n",
        "PARAMETER stop <|end|>\n",
        "PARAMETER stop <|user|>\n",
        "PARAMETER stop <|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "with open('phi3-adapter-file', 'w') as f:\n",
        "    f.write(adapterfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b5fb88",
      "metadata": {
        "id": "62b5fb88"
      },
      "outputs": [],
      "source": [
        "!ollama create our_own_phi3_adapted -f phi3-adapter-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336581b7",
      "metadata": {
        "id": "336581b7"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a7bf67",
      "metadata": {
        "id": "79a7bf67"
      },
      "source": [
        "##### Querying the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42618c42",
      "metadata": {
        "id": "42618c42"
      },
      "outputs": [],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea0b225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ea0b225",
        "outputId": "94c25ecd-0eb3-4f0e-abe6-ab417f748fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': 'yoda', 'created_at': '2024-11-19T16:33:49.665228456Z', 'response': 'In this one, the Force is strong. Hmm.', 'done': True, 'done_reason': 'stop', 'context': [32010, 29871, 13, 1576, 11004, 338, 4549, 297, 445, 697, 29991, 32007, 29871, 13, 32001, 29871, 13, 797, 445, 697, 29892, 278, 11004, 338, 4549, 29889, 28756, 29889], 'total_duration': 366255563, 'load_duration': 4767078, 'prompt_eval_count': 17, 'prompt_eval_duration': 15000000, 'eval_count': 12, 'eval_duration': 297000000}\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "prompt = \"The Force is strong in this one!\"\n",
        "response = ollama.generate(model='our_own_phi3_adapted',\n",
        "                           prompt=prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5189b804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5189b804",
        "outputId": "1320b25c-50b7-4918-e948-ca6b9b1862e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In this one, the Force is strong. Hmm.\n"
          ]
        }
      ],
      "source": [
        "print(response['response'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e35608",
      "metadata": {
        "id": "62e35608",
        "outputId": "998fee08-d51d-456a-d251-59e9b9a4fcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "The Force is strong in this one!<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "{'model': 'yoda', 'created_at': '2024-11-19T16:36:07.217876442Z', 'response': 'In this one, the Force is strong. Yes, hrrrm.', 'done': True, 'done_reason': 'stop', 'total_duration': 419360472, 'load_duration': 4926595, 'prompt_eval_count': 17, 'prompt_eval_duration': 21000000, 'eval_count': 16, 'eval_duration': 392000000}\n"
          ]
        }
      ],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "formatted = tokenizer_phi3.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(formatted)\n",
        "\n",
        "response = ollama.generate(model='our_own_phi3_adapted',\n",
        "                           prompt=formatted,\n",
        "                           raw=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07fb076",
      "metadata": {
        "id": "d07fb076"
      },
      "source": [
        "#### Llama.cpp\n",
        "\n",
        "Using the full Docker image, one that can be used to convert, quantize, and serve:\n",
        "\n",
        "```\n",
        "docker run -v \"/path/to/saved_model\":/model  \\\n",
        "           -p 8080:8000 \\\n",
        "           ghcr.io/ggerganov/llama.cpp:full \\\n",
        "           --server \\\n",
        "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
        "           --port 8000 \\\n",
        "           --host 0.0.0.0\n",
        "```\n",
        "\n",
        "1. `-v [local path]:[path inside container]`: It maps a folder on your computer to a folder inside the\n",
        "container, so effectively speaking, the container can \"see\" your local folder as if it were located inside the\n",
        "container itself.\n",
        "2. `-p [host port]:[container port]`: It forwards requests sent to a port on the host (e.g., 8080) to a port\n",
        "inside the container (e.g., 8000).\n",
        "3. `[docker image]`: We’re using llama.cpp’s Docker image, ghcr.io/ggerganov/llama.cpp:full.\n",
        "4. `--server`: This is the command we’re executing—it’s not a Docker command, but rather one that’s available\n",
        "within the specific image we’re utilizing.\n",
        "5. `-m /model/[quantized_qguf_file].qguf`: This is the model we’re serving.\n",
        "6. `--port [container port]`: This is the port inside the container used to serve the model. It should match\n",
        "the container port specified in the second argument.\n",
        "7. `--host [ip address]`: This is the local IP address used to serve the model.\n",
        "\n",
        "Using a smaller Docker image that’s specifically built for serving:\n",
        "\n",
        "```\n",
        "docker run -v \"path/to/saved_model\":/model \\\n",
        "           -p 8080:8000 \\\n",
        "           ghcr.io/ggerganov/llama.cpp:server \\\n",
        "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
        "           --port 8000 \\\n",
        "           --host 0.0.0.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79666d36",
      "metadata": {
        "id": "79666d36"
      },
      "source": [
        "##### Web Interface\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_ui.png?raw=True)\n",
        "<center>Figure 6.4 - Screenshot of llama.cpp’s web UI</center>\n",
        "\n",
        "If you click on the settings button at the top-right corner, you'll see plenty of parameters you can set, such as temperature:\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_settings.png?raw=True)\n",
        "<center>Figure 6.5 - Screenshot of llama.cpp’s settings</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be242056",
      "metadata": {
        "id": "be242056"
      },
      "source": [
        "##### REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ee1432",
      "metadata": {
        "id": "92ee1432"
      },
      "source": [
        "```python\n",
        "url = 'http://0.0.0.0:8080/completion'\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "data = {'prompt': 'There is bacon in this sandwich.',\n",
        "        'n_predict': 128}\n",
        "\n",
        "response = requests.post(url, json=data, headers=headers)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4e2819",
      "metadata": {
        "scrolled": true,
        "id": "aa4e2819"
      },
      "source": [
        "```python\n",
        "print(response.json()['content'])\n",
        "```\n",
        "\n",
        "```\n",
        " There is no bacon in this sandwich. This statement is a paradox because it contradicts itself, yet it seems to suggest that the sandwich has both bacon and no bacon at the same time.\n",
        "\n",
        "2. This statement is also a paradox, as it claims that it is a lie that it is lying. If the statement is true, then it is indeed a lie, making it false. But if it is false, then it is not a lie, making it true. This creates a circular reasoning that can't be resolved.\n",
        "\n",
        "3. This statement is a paradox\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb36d0e",
      "metadata": {
        "id": "fbb36d0e"
      },
      "source": [
        "### Thank You!\n",
        "\n",
        "If you have any suggestions, or if you find any errors, please don't hesitate to contact me through [GitHub](https://github.com/dvgodoy), [X](https://x.com/dvgodoy), [BlueSky](https://bsky.app/profile/dvgodoy.bsky.social), or [LinkedIn](https://www.linkedin.com/in/dvgodoy/).\n",
        "\n",
        "If you'd like to receive notifications about new book releases, updates, freebies, and discounts, follow me at:\n",
        "\n",
        "<center><a href=\"https://danielgodoy.gumroad.com/subscribe\">https://danielgodoy.gumroad.com/subscribe</a></center>\n",
        "\n",
        "I'm looking forward to hearing back from you!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}