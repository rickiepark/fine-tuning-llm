{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48e02e8",
   "metadata": {},
   "source": [
    "## Chapter 6: Deploying It Locally\n",
    "\n",
    "### Spoilers\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Load adapters and merge them to the base model for faster inference\n",
    "- Query the model to generate responses or completions\n",
    "- Convert the fine-tuned model to the GGUF file format used by llama.cpp\n",
    "- Use Ollama and llama.cpp to serve the model through web interfaces and REST APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ef1a5",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "For better reproducibility during training, use the pinned versions below, the same versions used in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c353825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes had to be bumped to 0.45.2 to avoid errors in Colab env\n",
    "!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65f6de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kODUm5BmEQhI",
    "outputId": "5a17ca8b-855a-4e55-b7a2-ab2af98d8906"
   },
   "outputs": [],
   "source": [
    "# If you're running on Colab\n",
    "#!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on runpod.io's Jupyter Template\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6a8fd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd09a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from dataclasses import asdict\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM, get_model_status, \\\n",
    "    get_layer_status, prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on Colab, you need to download the helper functions' Python file\n",
    "!wget https://raw.githubusercontent.com/dvgodoy/FineTuningLLMs/refs/heads/main/helper_functions.py\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Converts dataset from prompt/completion format (not supported anymore)\n",
    "# to the conversational format\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fecf6",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "\n",
    "We convert our fine-tuned models and adapters to the GGUF format and then quantize them so they are small enough to run on consumer-grade hardware (without GPUs). Next, we configure and import these models and adapters into Ollama or llama.cpp so we can serve them. That way, we can query them directly in a web interface or using a REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d2fca",
   "metadata": {},
   "source": [
    "### The Road So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Chapter 2\n",
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\n",
    "  \"facebook/opt-350m\", device_map='cuda:0', quantization_config=nf4_config\n",
    ")\n",
    "# From Chapter 3\n",
    "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model_q4, config)\n",
    "\n",
    "# From Chapter 4\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = modify_tokenizer(tokenizer)\n",
    "tokenizer = add_template(tokenizer)\n",
    "\n",
    "peft_model = modify_model(peft_model, tokenizer)\n",
    "\n",
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "\n",
    "# **IMPORTANT UPDATE**: unfortunately, in more recent versions of the `trl` library, \n",
    "# the \"instruction\" format is not properly supported anymore, thus leading to the chat\n",
    "# template not being applied to the dataset. In order to avoid this issue, we can\n",
    "# convert the dataset to the \"conversational\" format.\n",
    "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])\n",
    "\n",
    "# From Chapter 5\n",
    "min_effective_batch_size = 8\n",
    "lr = 3e-4\n",
    "max_seq_length = 64\n",
    "collator_fn = None\n",
    "packing = (collator_fn is None)\n",
    "steps = 20\n",
    "num_train_epochs = 10\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir='./future_name_on_the_hub',\n",
    "    # Dataset\n",
    "    packing=packing,\n",
    "    max_seq_length=max_seq_length,\n",
    "    # Gradients / Memory\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=min_effective_batch_size,\n",
    "    auto_find_batch_size=True,\n",
    "    # Training\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=lr,\n",
    "    # Env and Logging\n",
    "    report_to='tensorboard',\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=steps,\n",
    "    save_strategy='steps',\n",
    "    save_steps=steps\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator_fn,\n",
    "    args=sft_config\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('yoda-adapter') # trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc5796",
   "metadata": {},
   "source": [
    "### Loading Models and Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c331d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='facebook/opt-350m', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=8, target_modules={'q_proj', 'v_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
    "config = PeftConfig.from_pretrained(repo_or_folder)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95fe9d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map='auto')\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1c3098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (yoda): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (yoda): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (yoda): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (yoda): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(base_model, repo_or_folder, adapter_name='yoda')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83cda575",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.merge_adapter(['yoda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf03e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_or_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f41073",
   "metadata": {},
   "source": [
    "```python\n",
    "if tokenizer_exists:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        trust_remote_code=kwargs.get(\"trust_remote_code\", False)\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ffc55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50268, 512, padding_idx=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
    "model_resized = AutoPeftModelForCausalLM.from_pretrained(repo_or_folder, device_map='auto')\n",
    "model_resized.base_model.model.model.decoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411bc34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>module_type</th>\n",
       "      <th>enabled</th>\n",
       "      <th>active_adapters</th>\n",
       "      <th>merged_adapters</th>\n",
       "      <th>requires_grad</th>\n",
       "      <th>available_adapters</th>\n",
       "      <th>devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.model.decoder.layers.0.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.model.decoder.layers.0.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.model.decoder.layers.1.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.model.decoder.layers.1.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.model.decoder.layers.2.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.model.decoder.layers.2.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.model.decoder.layers.3.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.model.decoder.layers.3.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model.model.decoder.layers.4.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model.model.decoder.layers.4.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model.model.decoder.layers.5.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model.model.decoder.layers.5.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model.model.decoder.layers.6.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model.model.decoder.layers.6.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model.model.decoder.layers.7.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model.model.decoder.layers.7.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model.model.decoder.layers.8.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model.model.decoder.layers.8.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model.model.decoder.layers.9.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model.model.decoder.layers.9.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>model.model.decoder.layers.10.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>model.model.decoder.layers.10.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model.model.decoder.layers.11.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>model.model.decoder.layers.11.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model.model.decoder.layers.12.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>model.model.decoder.layers.12.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>model.model.decoder.layers.13.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>model.model.decoder.layers.13.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>model.model.decoder.layers.14.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model.model.decoder.layers.14.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>model.model.decoder.layers.15.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>model.model.decoder.layers.15.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>model.model.decoder.layers.16.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>model.model.decoder.layers.16.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>model.model.decoder.layers.17.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model.model.decoder.layers.17.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model.model.decoder.layers.18.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>model.model.decoder.layers.18.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>model.model.decoder.layers.19.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model.model.decoder.layers.19.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model.model.decoder.layers.20.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>model.model.decoder.layers.20.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>model.model.decoder.layers.21.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>model.model.decoder.layers.21.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>model.model.decoder.layers.22.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>model.model.decoder.layers.22.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>model.model.decoder.layers.23.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>model.model.decoder.layers.23.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  module_type  enabled  \\\n",
       "0    model.model.decoder.layers.0.self_attn.v_proj  lora.Linear     True   \n",
       "1    model.model.decoder.layers.0.self_attn.q_proj  lora.Linear     True   \n",
       "2    model.model.decoder.layers.1.self_attn.v_proj  lora.Linear     True   \n",
       "3    model.model.decoder.layers.1.self_attn.q_proj  lora.Linear     True   \n",
       "4    model.model.decoder.layers.2.self_attn.v_proj  lora.Linear     True   \n",
       "5    model.model.decoder.layers.2.self_attn.q_proj  lora.Linear     True   \n",
       "6    model.model.decoder.layers.3.self_attn.v_proj  lora.Linear     True   \n",
       "7    model.model.decoder.layers.3.self_attn.q_proj  lora.Linear     True   \n",
       "8    model.model.decoder.layers.4.self_attn.v_proj  lora.Linear     True   \n",
       "9    model.model.decoder.layers.4.self_attn.q_proj  lora.Linear     True   \n",
       "10   model.model.decoder.layers.5.self_attn.v_proj  lora.Linear     True   \n",
       "11   model.model.decoder.layers.5.self_attn.q_proj  lora.Linear     True   \n",
       "12   model.model.decoder.layers.6.self_attn.v_proj  lora.Linear     True   \n",
       "13   model.model.decoder.layers.6.self_attn.q_proj  lora.Linear     True   \n",
       "14   model.model.decoder.layers.7.self_attn.v_proj  lora.Linear     True   \n",
       "15   model.model.decoder.layers.7.self_attn.q_proj  lora.Linear     True   \n",
       "16   model.model.decoder.layers.8.self_attn.v_proj  lora.Linear     True   \n",
       "17   model.model.decoder.layers.8.self_attn.q_proj  lora.Linear     True   \n",
       "18   model.model.decoder.layers.9.self_attn.v_proj  lora.Linear     True   \n",
       "19   model.model.decoder.layers.9.self_attn.q_proj  lora.Linear     True   \n",
       "20  model.model.decoder.layers.10.self_attn.v_proj  lora.Linear     True   \n",
       "21  model.model.decoder.layers.10.self_attn.q_proj  lora.Linear     True   \n",
       "22  model.model.decoder.layers.11.self_attn.v_proj  lora.Linear     True   \n",
       "23  model.model.decoder.layers.11.self_attn.q_proj  lora.Linear     True   \n",
       "24  model.model.decoder.layers.12.self_attn.v_proj  lora.Linear     True   \n",
       "25  model.model.decoder.layers.12.self_attn.q_proj  lora.Linear     True   \n",
       "26  model.model.decoder.layers.13.self_attn.v_proj  lora.Linear     True   \n",
       "27  model.model.decoder.layers.13.self_attn.q_proj  lora.Linear     True   \n",
       "28  model.model.decoder.layers.14.self_attn.v_proj  lora.Linear     True   \n",
       "29  model.model.decoder.layers.14.self_attn.q_proj  lora.Linear     True   \n",
       "30  model.model.decoder.layers.15.self_attn.v_proj  lora.Linear     True   \n",
       "31  model.model.decoder.layers.15.self_attn.q_proj  lora.Linear     True   \n",
       "32  model.model.decoder.layers.16.self_attn.v_proj  lora.Linear     True   \n",
       "33  model.model.decoder.layers.16.self_attn.q_proj  lora.Linear     True   \n",
       "34  model.model.decoder.layers.17.self_attn.v_proj  lora.Linear     True   \n",
       "35  model.model.decoder.layers.17.self_attn.q_proj  lora.Linear     True   \n",
       "36  model.model.decoder.layers.18.self_attn.v_proj  lora.Linear     True   \n",
       "37  model.model.decoder.layers.18.self_attn.q_proj  lora.Linear     True   \n",
       "38  model.model.decoder.layers.19.self_attn.v_proj  lora.Linear     True   \n",
       "39  model.model.decoder.layers.19.self_attn.q_proj  lora.Linear     True   \n",
       "40  model.model.decoder.layers.20.self_attn.v_proj  lora.Linear     True   \n",
       "41  model.model.decoder.layers.20.self_attn.q_proj  lora.Linear     True   \n",
       "42  model.model.decoder.layers.21.self_attn.v_proj  lora.Linear     True   \n",
       "43  model.model.decoder.layers.21.self_attn.q_proj  lora.Linear     True   \n",
       "44  model.model.decoder.layers.22.self_attn.v_proj  lora.Linear     True   \n",
       "45  model.model.decoder.layers.22.self_attn.q_proj  lora.Linear     True   \n",
       "46  model.model.decoder.layers.23.self_attn.v_proj  lora.Linear     True   \n",
       "47  model.model.decoder.layers.23.self_attn.q_proj  lora.Linear     True   \n",
       "\n",
       "   active_adapters merged_adapters    requires_grad available_adapters  \\\n",
       "0           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "1           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "2           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "3           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "4           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "5           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "6           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "7           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "8           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "9           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "10          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "11          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "12          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "13          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "14          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "15          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "16          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "17          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "18          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "19          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "20          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "21          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "22          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "23          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "24          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "25          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "26          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "27          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "28          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "29          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "30          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "31          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "32          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "33          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "34          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "35          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "36          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "37          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "38          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "39          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "40          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "41          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "42          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "43          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "44          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "45          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "46          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "47          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "\n",
       "               devices  \n",
       "0   {'yoda': ['cuda']}  \n",
       "1   {'yoda': ['cuda']}  \n",
       "2   {'yoda': ['cuda']}  \n",
       "3   {'yoda': ['cuda']}  \n",
       "4   {'yoda': ['cuda']}  \n",
       "5   {'yoda': ['cuda']}  \n",
       "6   {'yoda': ['cuda']}  \n",
       "7   {'yoda': ['cuda']}  \n",
       "8   {'yoda': ['cuda']}  \n",
       "9   {'yoda': ['cuda']}  \n",
       "10  {'yoda': ['cuda']}  \n",
       "11  {'yoda': ['cuda']}  \n",
       "12  {'yoda': ['cuda']}  \n",
       "13  {'yoda': ['cuda']}  \n",
       "14  {'yoda': ['cuda']}  \n",
       "15  {'yoda': ['cuda']}  \n",
       "16  {'yoda': ['cuda']}  \n",
       "17  {'yoda': ['cuda']}  \n",
       "18  {'yoda': ['cuda']}  \n",
       "19  {'yoda': ['cuda']}  \n",
       "20  {'yoda': ['cuda']}  \n",
       "21  {'yoda': ['cuda']}  \n",
       "22  {'yoda': ['cuda']}  \n",
       "23  {'yoda': ['cuda']}  \n",
       "24  {'yoda': ['cuda']}  \n",
       "25  {'yoda': ['cuda']}  \n",
       "26  {'yoda': ['cuda']}  \n",
       "27  {'yoda': ['cuda']}  \n",
       "28  {'yoda': ['cuda']}  \n",
       "29  {'yoda': ['cuda']}  \n",
       "30  {'yoda': ['cuda']}  \n",
       "31  {'yoda': ['cuda']}  \n",
       "32  {'yoda': ['cuda']}  \n",
       "33  {'yoda': ['cuda']}  \n",
       "34  {'yoda': ['cuda']}  \n",
       "35  {'yoda': ['cuda']}  \n",
       "36  {'yoda': ['cuda']}  \n",
       "37  {'yoda': ['cuda']}  \n",
       "38  {'yoda': ['cuda']}  \n",
       "39  {'yoda': ['cuda']}  \n",
       "40  {'yoda': ['cuda']}  \n",
       "41  {'yoda': ['cuda']}  \n",
       "42  {'yoda': ['cuda']}  \n",
       "43  {'yoda': ['cuda']}  \n",
       "44  {'yoda': ['cuda']}  \n",
       "45  {'yoda': ['cuda']}  \n",
       "46  {'yoda': ['cuda']}  \n",
       "47  {'yoda': ['cuda']}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(asdict(layer) for layer in get_layer_status(model))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba40822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TunerModelStatus(base_model_type='OPTForCausalLM', adapter_model_type='LoraModel', peft_types={'yoda': 'LORA'}, trainable_params=0, total_params=331982848, num_adapter_layers=48, enabled=True, active_adapters=['yoda'], merged_adapters=['yoda'], requires_grad={'yoda': False}, available_adapters=['yoda'], devices={'yoda': ['cuda']})\n"
     ]
    }
   ],
   "source": [
    "print(get_model_status(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74c72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb1c59",
   "metadata": {},
   "source": [
    "### Querying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c663f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample, \n",
    "                                           tokenize=False, \n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41066fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "There is bacon in this sandwich.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = gen_prompt(tokenizer, 'There is bacon in this sandwich.')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97bb4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, \n",
    "             max_new_tokens=64, \n",
    "             skip_special_tokens=False, \n",
    "             response_only=False):\n",
    "    # Tokenizes the formatted prompt\n",
    "    tokenized_input = tokenizer(prompt, \n",
    "                                add_special_tokens=False, \n",
    "                                return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    # Generates the response/completion\n",
    "    generation_output = model.generate(**tokenized_input,\n",
    "                                       eos_token_id=tokenizer.eos_token_id,\n",
    "                                       max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # If required, removes the tokens belonging to the prompt\n",
    "    if response_only:\n",
    "        input_length = tokenized_input['input_ids'].shape[1]\n",
    "        generation_output = generation_output[:, input_length:]\n",
    "    \n",
    "    # Decodes the tokens back into text\n",
    "    output = tokenizer.batch_decode(generation_output, \n",
    "                                    skip_special_tokens=skip_special_tokens)[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b4b5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "There is bacon in this sandwich.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "In this sandwich, bacon there is.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer,prompt, skip_special_tokens=False, response_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "452a830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this sandwich, bacon there is.\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer,prompt, skip_special_tokens=True, response_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5b919d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences  = ['There is bacon in this sandwich.', 'Add some cheddar to it.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "17acc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(model, tokenizer, sentences, \n",
    "             max_new_tokens=64, \n",
    "             skip_special_tokens=False, \n",
    "             response_only=False):\n",
    "    \n",
    "    # Converts prompts into conversational format\n",
    "    converted_samples = [[{\"role\": \"user\", \"content\": sentence}] \n",
    "                         for sentence in sentences]\n",
    "\n",
    "    # Applies the chat template to format the prompts\n",
    "    prompts = tokenizer.apply_chat_template(converted_samples, \n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    \n",
    "    # Forces padding to the left for batch generation\n",
    "    tokenizer.padding_side = 'left'    \n",
    "    # Tokenizes the formatted prompts with padding\n",
    "    tokenized_inputs = tokenizer(prompts, \n",
    "                                 padding=True,\n",
    "                                 add_special_tokens=False,\n",
    "                                 return_tensors='pt').to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    # Generates the responses/completions\n",
    "    generation_output = model.generate(**tokenized_inputs, \n",
    "                                       eos_token_id=tokenizer.eos_token_id,\n",
    "                                       pad_token_id=tokenizer.pad_token_id,\n",
    "                                       max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # If required, removes the tokens belonging to the prompts\n",
    "    if response_only:\n",
    "        input_length = tokenized_inputs['input_ids'].shape[1]\n",
    "        generation_output = generation_output[:, input_length:]\n",
    "\n",
    "    # Decodes the tokens back into text\n",
    "    output = tokenizer.batch_decode(generation_output, \n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    if isinstance(sentences, str):\n",
    "        output = output[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ee75635c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this sandwich, bacon there is.', 'To it, add some cheddar, you must.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_generate(model, tokenizer, sentences, skip_special_tokens=True, response_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd75791",
   "metadata": {},
   "source": [
    "### Llama.cpp\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp.png?raw=True)\n",
    "\n",
    "<center>Figure 6.1 - Screenshot of llama.cpp’s GitHub Repo</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9754e",
   "metadata": {},
   "source": [
    "#### Converting Adapters\n",
    "\n",
    "In order to convert an adapter to the GGUF format, we need to do the following:\n",
    "\n",
    "- save the adapter to a local folder, either by calling the `save_model()` method after training, as we did in the last chapter, or by downloading it from the Hugging Face Hub (see the aside for details)\n",
    "- clone the llama.cpp repository from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbc969",
   "metadata": {},
   "source": [
    "- install the `gguf-py` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c870a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama.cpp/gguf-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccc7e0",
   "metadata": {},
   "source": [
    "***\n",
    "**Downloading Models from the Hub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1493a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d37117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"dvgodoy/phi3-mini-yoda-adapter\", local_dir='./phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb35f0",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "- run the `convert_lora_to_gguf.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./llama.cpp/convert_lora_to_gguf.py \\\n",
    "        ./phi3-mini-yoda-adapter \\\n",
    "        --outfile adapter.gguf \\\n",
    "        --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13bdca4",
   "metadata": {},
   "source": [
    "- the `outtype` may be one of the following choices: `f32`, `f16`, `bf16`, `q8_0`, or `auto`, which defaults to the highest-fidelity 16-bit float type depending on the first loaded tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669cfb0",
   "metadata": {},
   "source": [
    "#### Converting Full Models\n",
    "\n",
    "##### Using \"GGUF My Repo\"\n",
    "\n",
    "https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/gguf_my_repo.png?raw=True)\n",
    "\n",
    "<center>Figure 6.2 - Screenshot of \"GGUF My Repo\" space on Hugging Face</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa6074",
   "metadata": {},
   "source": [
    "##### Using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5aa020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgXjjGTk0OlW",
    "outputId": "e528cf43-1231-4c89-e7bb-3537d94bb048"
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16340ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "referenced_widgets": [
      "a563aa80f18f47f6be23140a8b1d6321",
      "b26bcb5313994843803893f376a7b7f3",
      "f72819964a464ebe96f701b88a0a63e1",
      "915f45d3647c43d3a5c3188c3dedab43",
      "777a6b6e00594a46865fd0a30a4b0aad",
      "326bbbd315de4dfba9d2986a5562a7ee",
      "7e7e5180887f44bb91893f252bfddba5",
      "80e836132f2947a4ab6d2b1c54549d42",
      "f88e0d01d32645ac97a866f1de248809",
      "def8fb4b8f3e4332b51eb6e359a82e1b",
      "1e2d1e15714948718c31d1e98aceb991",
      "ad13665088224ecd94ac5c96c44f7ba9",
      "903c2ebf2bd34db4adfcd44acc85ce3f",
      "a988b36ac81d44b6926fef9f89c2c902",
      "70aec4858c9f444dbaffc6643d055855",
      "ae7e659b97e74cb69114ed36320bd981",
      "5caa93ed1aa247a78546afddf4609518",
      "0453a514857743ea943a8e8b4860db12",
      "fcba1c34591f4e8e96695c7743cdcf45",
      "4431fa6834424ff79f18ed717778595c",
      "981291ff17cb4894964765d210135ab7",
      "004d14cc1cfe48f4a1b38d26cf8e1671",
      "1971a8422e344ff29d78cde6f12a1f0f",
      "c469890d6f424d179ed569b51d8d383f",
      "ec977660658143e682d37fc859ab46f0",
      "964556c37b354cdc973dd68c31f17ad7",
      "fd38ad5820d7419ca94bb9629817dd96",
      "71496598257c43bfbc2a796cb1f77340",
      "c5b0099e8bcf41f1a6bfd7949b7c01ca",
      "09e13fa1eada427ba47b8e7fa65376de",
      "6247d611a4b045daa6450f77809f29d5",
      "4a95d8ce73b641458522850a8fb62c9f",
      "742a4781a5fd4d28921862ed3ae4e7c7",
      "e6f21713f86f494fb337ffb123e2cb23",
      "3d5b2d1e727046e7b88513af01d4b348",
      "33f54950c1924b8db92982d520ae44ae",
      "c841a43d16a34fdd9eb0aabcc52c2cf9",
      "4f1f6c172ce8497ba344c6b564f5246f",
      "5aa4fb34a96d4774a7ef9908fbff6a28",
      "e7fe2cce7f78413eaf5bebc7bad2812b",
      "8ac31b4eaef34439bff239a20528349b",
      "d595758bddc84b388002d61a76c1ae41",
      "cf383f735e6a4afea5c16971d8283703",
      "4131c75920bd45e98bb3aef2fa8e6ef8",
      "2649a8e58ef34352827e6ba6417c281e",
      "9e06d8361a3f4454bd3190c4f77dc545",
      "55ed71e27e5f4cc3899e58ec65cb6138",
      "2d7f681ebb874a9f8cbc89bee5d53dd1",
      "da19650d00b64c848ea6f2c7a16cbeeb",
      "b5728bd3d1b24fa68aa8cff4cc157754",
      "1af99dcf1fe24813b7cb882b2e9b0c93",
      "dc21b474bee9469282ef0e49d596b542",
      "d56020df55f0480688f42c827421fc99",
      "2357b02e1e89476caf8d07b2821d042d",
      "edd849a039304d928f4b5aa1f903ab23",
      "0d6f2a18e4e04fabb12c714211fd83d9",
      "16405ad2084a4467a57430a5320fe03a",
      "8fda54f9d9764a688e76bc068b6ebf12",
      "94f5520fe4544e02afecf6741bf4ff65",
      "fcd053cf97194325b1c2aa1f159127eb",
      "370b558c601b41c58a490319fd16ea9b",
      "dadfa984a18b46d0844a970c179a949d",
      "d0a5a2a8db7e4df6b1ff66102330c63a",
      "af3226a9e4c04522be4bd5913943e08b",
      "b41d0bd86ab043e2a67637e4168676c3",
      "4b628f5e1df5423083cd0a3615fdc4f3",
      "c22bc86dd8db419d9030285aae80f0e1",
      "be97a316a8eb49ef917da1819469f98e",
      "6506a33bdd0c4a109f8071acd3862379",
      "8c523da1549748768906d291fd8a25d0",
      "7020c845736147e88c84ff181e8c2783",
      "72af7f5769864a4d866adb28ad32ee52",
      "9bc0004de47b40908e55c60c1a247d41",
      "f300b0beb26c4cf7b72ffb1afe2abfaf",
      "9714af1f43c74842966ea66f0eb7d148",
      "e281408357a641d093832e7a47f88c0d",
      "399e9a78fc1e49219ec92bf3a96c586a",
      "2c216f58aa8a4cc2b0549c4ebaaa723f",
      "480833262f174d5c8e722af3ce3cc914",
      "60738bb916c84a9fa89a759d95079e6a",
      "61729533300643f0966e5f0f96ca77c1",
      "a75f305873044be1aef8d057adc4e70b",
      "d124c0dff6a041f1a0bcdaf7630e1b22",
      "eeff8ffceda84a0c88a3dd210d1c0769",
      "89d4d1b4b2ea49e283a18b706f56c9db",
      "93245b0e9acd4569abb623b67b66a782",
      "abaa8cbdb9fb41c9a96f4a2f5947be66",
      "f38cf3346fcb44268a502d9a7318f403"
     ]
    },
    "id": "rtDK4cA70Qrk",
    "outputId": "875afd99-75e7-4d66-fe94-5758080ffba1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained('dvgodoy/phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55572d7",
   "metadata": {},
   "source": [
    "```\n",
    "==((====))==  Unsloth 2024.10.0: Fast Mistral patching. Transformers = 4.44.2.\n",
    "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
    "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
    "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
    " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "\n",
    "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]\n",
    "\n",
    "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]\n",
    "\n",
    "tokenizer_config.json:   0%|          | 0.00/3.34k [00:00<?, ?B/s]\n",
    "\n",
    "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
    "\n",
    "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]\n",
    "\n",
    "special_tokens_map.json:   0%|          | 0.00/458 [00:00<?, ?B/s]\n",
    "\n",
    "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
    "\n",
    "adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]\n",
    "\n",
    "Unsloth 2024.10.0 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0539a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcQOCRp84n41",
    "outputId": "f05c8ce8-b47a-4c8f-ef05-d0000c1ac587"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3fa7d",
   "metadata": {},
   "source": [
    "```\n",
    "PeftModelForCausalLM(\n",
    "  (base_model): LoraModel(\n",
    "    (model): MistralForCausalLM(\n",
    "      (model): MistralModel(\n",
    "        (embed_tokens): Embedding(32064, 3072, padding_idx=32009)\n",
    "        (layers): ModuleList(\n",
    "          (0-31): 32 x MistralDecoderLayer(\n",
    "            (self_attn): MistralAttention(\n",
    "              (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "              (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "              (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "              (o_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.05, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): MistralMLP(\n",
    "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
    "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
    "              (down_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.05, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (act_fn): SiLU()\n",
    "            )\n",
    "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
    "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
    "          )\n",
    "        )\n",
    "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
    "      )\n",
    "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfe3eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0u0rTr811KR",
    "outputId": "730f51b2-6e7c-44b3-dbac-d99ffb4acbda"
   },
   "outputs": [],
   "source": [
    "# This command may fail for several reasons, as it depends on the environment\n",
    "# and the stability of llama.cpp (which is installed during its execution)\n",
    "\n",
    "# Removing the llama.cpp folder we cloned above, so Unsloth can install it on its own\n",
    "!rm -rf llama.cpp/\n",
    "\n",
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ce724",
   "metadata": {},
   "source": [
    " ```\n",
    "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
    "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
    "To force `safe_serialization`, set it to `None` instead.\n",
    "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
    "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
    "Unsloth: Will remove a cached repo with size 2.3G\n",
    "\n",
    "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
    "Unsloth: Will use up to 5.02 out of 12.67 RAM for saving.\n",
    "Unsloth: Saving tokenizer... Done.\n",
    "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
    "Unsloth: Saving gguf_model/pytorch_model-00001-of-00002.bin...\n",
    "Unsloth: Saving gguf_model/pytorch_model-00002-of-00002.bin...\n",
    "Done.\n",
    "\n",
    "Unsloth: Converting mistral model. Can use fast conversion = True.\n",
    "\n",
    "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
    "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
    "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
    "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
    " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
    "\n",
    "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
    "\n",
    "Unsloth: Extending gguf_model/tokenizer.model with added_tokens.json.\n",
    "Originally tokenizer.model is of size (32000).\n",
    "But we need to extend to sentencepiece vocab size (32011).\n",
    "\n",
    "Unsloth: [1] Converting model at gguf_model into f16 GGUF format.\n",
    "The output location will be /content/gguf_model/unsloth.F16.gguf\n",
    "This will take 3 minutes...\n",
    "INFO:hf-to-gguf:Loading model: gguf_model\n",
    "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
    "INFO:hf-to-gguf:Exporting model...\n",
    "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
    "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
    "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 32064}\n",
    "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
    "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
    "...\n",
    "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
    "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
    "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
    "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {3072, 32064}\n",
    "INFO:hf-to-gguf:Set meta model\n",
    "INFO:hf-to-gguf:Set model parameters\n",
    "INFO:hf-to-gguf:gguf: context length = 4096\n",
    "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
    "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
    "INFO:hf-to-gguf:gguf: head count = 32\n",
    "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
    "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
    "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
    "INFO:hf-to-gguf:gguf: file type = 1\n",
    "INFO:hf-to-gguf:Set model tokenizer\n",
    "INFO:gguf.vocab:Setting special token type bos to 1\n",
    "INFO:gguf.vocab:Setting special token type eos to 32000\n",
    "INFO:gguf.vocab:Setting special token type unk to 0\n",
    "INFO:gguf.vocab:Setting special token type pad to 32009\n",
    "INFO:gguf.vocab:Setting add_bos_token to False\n",
    "INFO:gguf.vocab:Setting add_eos_token to False\n",
    "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
    "' + message['content'] + '<|end|>\n",
    "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
    "' + message['content'] + '<|end|>\n",
    "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
    "' + message['content'] + '<|end|>\n",
    "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
    "' }}{% else %}{{ eos_token }}{% endif %}\n",
    "INFO:hf-to-gguf:Set model quantization version\n",
    "INFO:gguf.gguf_writer:Writing the following files:\n",
    "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.F16.gguf: n_tensors = 291, total_size = 7.6G\n",
    "Writing: 100%|██████████| 7.64G/7.64G [01:56<00:00, 65.5Mbyte/s]\n",
    "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.F16.gguf\n",
    "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.F16.gguf\n",
    "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
    "main: build = 3934 (3752217e)\n",
    "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
    "main: quantizing '/content/gguf_model/unsloth.F16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
    "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from /content/gguf_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
    "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
    "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
    "llama_model_loader: - kv   1:                               general.type str              = model\n",
    "llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 4k Instruct Bnb 4bit\n",
    "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
    "llama_model_loader: - kv   4:                           general.finetune str              = 4k-instruct-bnb-4bit\n",
    "llama_model_loader: - kv   5:                           general.basename str              = phi-3\n",
    "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
    "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
    "llama_model_loader: - kv   8:                       llama.context_length u32              = 4096\n",
    "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
    "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
    "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
    "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
    "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
    "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
    "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 96\n",
    "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 96\n",
    "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
    "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32064\n",
    "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 96\n",
    "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
    "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
    "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
    "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
    "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
    "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
    "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
    "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 32000\n",
    "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
    "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32009\n",
    "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = false\n",
    "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
    "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
    "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
    "llama_model_loader: - type  f32:   65 tensors\n",
    "llama_model_loader: - type  f16:  226 tensors\n",
    "[   1/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
    "[   2/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
    "...\n",
    "[ 290/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
    "[ 291/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
    "llama_model_quantize_internal: model size  =  7288.51 MB\n",
    "llama_model_quantize_internal: quant size  =  2210.78 MB\n",
    "\n",
    "main: quantize time = 426187.37 ms\n",
    "main:    total time = 426187.37 ms\n",
    "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ddbf5",
   "metadata": {},
   "source": [
    "##### Using Docker Images\n",
    "\n",
    "To convert the model, we need to run the command below:\n",
    "\n",
    "```\n",
    "docker run --rm \n",
    "           -v \"/path/to/saved_model\":/repo \n",
    "           ghcr.io/ggerganov/llama.cpp:full \n",
    "           --convert \"/repo\" \n",
    "           --outtype f32 \n",
    "           --outfile /repo/gguf-model-f32.gguf\n",
    "```\n",
    "\n",
    "1. `--rm`: It automatically removes the container from execution after it finishes running, which can be very\n",
    "useful in cases such as ours, where we’re only interested in running a script once.\n",
    "2. `-v [local path]:[path inside container]`: It maps a folder on your computer to a folder inside the\n",
    "container. This allows the container to \"see\" your local folder as if it were located inside the container\n",
    "itself.\n",
    "3. `[docker image]`: We’re using llama.cpp’s Docker image, ghcr.io/ggerganov/llama.cpp:full\n",
    "4. `--convert [path inside container]`: This is the command we’re executing—it isn’t a Docker command,\n",
    "but rather a command that’s available in the particular image we’re using.\n",
    "5. `--outtype [GGUF type]`: This is an argument of the --convert command that specifies the data type of the\n",
    "resulting GGUF file.\n",
    "6. `--outfile [GGUF filename]`: This is yet another argument of the --convert command. It specifies the\n",
    "name of the GGUF file (note that it points to a path inside the container—/repo—which was mapped to a\n",
    "folder on your local computer, so in the end, the file is generated directly in your local folder).\n",
    "\n",
    "To quantize the converted model, we need to run the following command:\n",
    "\n",
    "```\n",
    "docker run --rm\n",
    "           -v \"/path/to/saved_model\":/repo \n",
    "           ghcr.io/ggerganov/llama.cpp:full \n",
    "           --quantize \"/repo/gguf-model-f32.gguf\" \n",
    "           \"/repo/gguf-model-Q4_K_M.gguf\" \n",
    "           \"Q4_K_M\"\n",
    "```\n",
    "\n",
    "7. `--quantize [GGUF filename]`: This is the new command we’re executing, it is a command available in this\n",
    "particular image only, and it should specify which GGUF file is to be quantized (usually the outfile from\n",
    "the convert command).\n",
    "8. `[quantized GGFUF filename]`: the name of the quantized file after the script finishes; make sure to point to\n",
    "the mapped folder so you can access it directly in your local folder as well\n",
    "9. `[quantization type]`: For a full list of quantization types, please check the [documentation](https://github.com/ggerganov/llama.cpp/blob/main/examples/quantize/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d1a2b",
   "metadata": {},
   "source": [
    "##### Building llama.cpp\n",
    "\n",
    "```python\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!pip install llama.cpp/gguf-py\n",
    "!pip install -r llama.cpp/requirements.txt\n",
    "```\n",
    "\n",
    "```python\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py /path/to/saved_model --outtype f16\n",
    "```\n",
    "\n",
    "```python\n",
    "!cd llama.cpp && make clean && make\n",
    "```\n",
    "\n",
    "```python\n",
    "!./llama.cpp/quantize \n",
    "    ./path/to/saved_model/ggml-model-f16.gguf \n",
    "    ./path/to/saved_model/ggml-model-q4_0.gguf \n",
    "    q4_0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120bbcd",
   "metadata": {},
   "source": [
    "### Serving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76ca5a",
   "metadata": {},
   "source": [
    "#### Ollama\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/ollama.png?raw=True)\n",
    "<center>Figure 6.3 - Screenshot of Ollama’s page</center>\n",
    "\n",
    "```\n",
    "ollama run phi3:mini\n",
    "```\n",
    "\n",
    "##### Installing Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c9062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhTyt1_OXJ5G",
    "outputId": "f35a6026-1626-431b-a6bd-90906bd162af"
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574e2e2",
   "metadata": {},
   "source": [
    "```\n",
    ">>> Installing ollama to /usr/local/bin...\n",
    ">>> Creating ollama user...\n",
    ">>> Adding ollama user to video group...\n",
    ">>> Adding current user to ollama group...\n",
    ">>> Creating ollama systemd service...\n",
    "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
    ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
    ">>> Install complete. Run \"ollama\" from the command line.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9f0b5",
   "metadata": {},
   "source": [
    "##### Running Ollama in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853f2ed6",
   "metadata": {
    "id": "-uBI17s9evuF"
   },
   "outputs": [],
   "source": [
    "# Adapter from https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import threading\n",
    "\n",
    "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
    "# Set environment variable for NVIDIA library\n",
    "# Set environment variables for CUDA\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
    "\n",
    "async def run_process(cmd):\n",
    "    print('>>> starting', *cmd)\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    # define an async pipe function\n",
    "    async def pipe(lines):\n",
    "        async for line in lines:\n",
    "            print(line.decode().strip())\n",
    "\n",
    "        await asyncio.gather(\n",
    "            pipe(process.stdout),\n",
    "            pipe(process.stderr),\n",
    "        )\n",
    "\n",
    "    # call it\n",
    "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
    "    \n",
    "async def start_ollama_serve():\n",
    "    await run_process(['ollama', 'serve'])\n",
    "\n",
    "def run_async_in_thread(loop, coro):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(coro)\n",
    "    loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df9604",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfhjeQnkezPd",
    "outputId": "fdd7a4ee-2c73-4aaa-81e8-4b00beafb1d0"
   },
   "outputs": [],
   "source": [
    "# Create a new event loop that will run in a new thread\n",
    "new_loop = asyncio.new_event_loop()\n",
    "\n",
    "# Start ollama serve in a separate thread so the cell won't block execution\n",
    "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13ef1c",
   "metadata": {},
   "source": [
    "##### Model Files\n",
    "\n",
    "| Instruction |\tDescription |\n",
    "|---|---|\n",
    "|FROM (required) | Defines the base model to use. |\n",
    "|PARAMETER | Sets the parameters for how Ollama will run the model. |\n",
    "|TEMPLATE | The full prompt template to be sent to the model. |\n",
    "|SYSTEM | Specifies the system message that will be set in the template. |\n",
    "|ADAPTER | Defines the (Q)LoRA adapters to apply to the model. |\n",
    "|LICENSE | Specifies the legal license. |\n",
    "|MESSAGE | Specify message history. |\n",
    "\n",
    "```\n",
    "ollama show --modelfile phi3:mini\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "# Modelfile generated by \"ollama show\"\n",
    "# To build a new Modelfile based on this, replace FROM with:\n",
    "# FROM phi3:mini\n",
    "\n",
    "FROM /usr/share/ollama/.ollama/models/blobs/sha256-633fc...\n",
    "TEMPLATE \"{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\n",
    "PARAMETER stop <|end|>\n",
    "PARAMETER stop <|user|>\n",
    "PARAMETER stop <|assistant|>\n",
    "LICENSE \"\"\"Microsoft.\n",
    "Copyright (c) Microsoft Corporation.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03abae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_phi3 = AutoTokenizer.from_pretrained('microsoft/phi-3-mini-4k-instruct')\n",
    "print(tokenizer_phi3.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4012459",
   "metadata": {},
   "source": [
    "##### Importing Models\n",
    "\n",
    "###### Custom (Full) Model File\n",
    "\n",
    "```python\n",
    "modelfile = \"\"\"\n",
    "FROM ./phi3-full-model\n",
    "TEMPLATE \"{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\n",
    "PARAMETER stop <|end|>\n",
    "PARAMETER stop <|user|>\n",
    "PARAMETER stop <|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "with open('phi3-full-modelfile', 'w') as f:\n",
    "    f.write(modelfile)\n",
    "```\n",
    "\n",
    "```\n",
    "!ollama create our_own_phi3 -f phi3-full-modelfile\n",
    "```\n",
    "\n",
    "```\n",
    "!ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a52507",
   "metadata": {},
   "source": [
    "###### Custom Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapterfile = \"\"\"\n",
    "FROM phi3:mini\n",
    "ADAPTER ./adapter.gguf\n",
    "TEMPLATE \"{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\n",
    "PARAMETER stop <|end|>\n",
    "PARAMETER stop <|user|>\n",
    "PARAMETER stop <|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "with open('phi3-adapter-file', 'w') as f:\n",
    "    f.write(adapterfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama create our_own_phi3_adapted -f phi3-adapter-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336581b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7bf67",
   "metadata": {},
   "source": [
    "##### Querying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42618c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ea0b225",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA28kOO5Omg2",
    "outputId": "94c25ecd-0eb3-4f0e-abe6-ab417f748fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'yoda', 'created_at': '2024-11-19T16:33:49.665228456Z', 'response': 'In this one, the Force is strong. Hmm.', 'done': True, 'done_reason': 'stop', 'context': [32010, 29871, 13, 1576, 11004, 338, 4549, 297, 445, 697, 29991, 32007, 29871, 13, 32001, 29871, 13, 797, 445, 697, 29892, 278, 11004, 338, 4549, 29889, 28756, 29889], 'total_duration': 366255563, 'load_duration': 4767078, 'prompt_eval_count': 17, 'prompt_eval_duration': 15000000, 'eval_count': 12, 'eval_duration': 297000000}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "prompt = \"The Force is strong in this one!\"\n",
    "response = ollama.generate(model='our_own_phi3_adapted', \n",
    "                           prompt=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5189b804",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PoB49QR5QNLq",
    "outputId": "1320b25c-50b7-4918-e948-ca6b9b1862e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this one, the Force is strong. Hmm.\n"
     ]
    }
   ],
   "source": [
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62e35608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The Force is strong in this one!<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "{'model': 'yoda', 'created_at': '2024-11-19T16:36:07.217876442Z', 'response': 'In this one, the Force is strong. Yes, hrrrm.', 'done': True, 'done_reason': 'stop', 'total_duration': 419360472, 'load_duration': 4926595, 'prompt_eval_count': 17, 'prompt_eval_duration': 21000000, 'eval_count': 16, 'eval_duration': 392000000}\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': prompt}]\n",
    "formatted = tokenizer_phi3.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(formatted)\n",
    "\n",
    "response = ollama.generate(model='our_own_phi3_adapted',\n",
    "                           prompt=formatted, \n",
    "                           raw=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07fb076",
   "metadata": {},
   "source": [
    "#### Llama.cpp\n",
    "\n",
    "Using the full Docker image, one that can be used to convert, quantize, and serve:\n",
    "\n",
    "```\n",
    "docker run -v \"/path/to/saved_model\":/model  \\\n",
    "           -p 8080:8000 \\\n",
    "           ghcr.io/ggerganov/llama.cpp:full \\\n",
    "           --server \\\n",
    "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
    "           --port 8000 \\\n",
    "           --host 0.0.0.0\n",
    "```\n",
    "\n",
    "1. `-v [local path]:[path inside container]`: It maps a folder on your computer to a folder inside the\n",
    "container, so effectively speaking, the container can \"see\" your local folder as if it were located inside the\n",
    "container itself.\n",
    "2. `-p [host port]:[container port]`: It forwards requests sent to a port on the host (e.g., 8080) to a port\n",
    "inside the container (e.g., 8000).\n",
    "3. `[docker image]`: We’re using llama.cpp’s Docker image, ghcr.io/ggerganov/llama.cpp:full.\n",
    "4. `--server`: This is the command we’re executing—it’s not a Docker command, but rather one that’s available\n",
    "within the specific image we’re utilizing.\n",
    "5. `-m /model/[quantized_qguf_file].qguf`: This is the model we’re serving.\n",
    "6. `--port [container port]`: This is the port inside the container used to serve the model. It should match\n",
    "the container port specified in the second argument.\n",
    "7. `--host [ip address]`: This is the local IP address used to serve the model.\n",
    "\n",
    "Using a smaller Docker image that’s specifically built for serving:\n",
    "\n",
    "```\n",
    "docker run -v \"path/to/saved_model\":/model \\\n",
    "           -p 8080:8000 \\\n",
    "           ghcr.io/ggerganov/llama.cpp:server \\\n",
    "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
    "           --port 8000 \\\n",
    "           --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79666d36",
   "metadata": {},
   "source": [
    "##### Web Interface\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_ui.png?raw=True)\n",
    "<center>Figure 6.4 - Screenshot of llama.cpp’s web UI</center>\n",
    "\n",
    "If you click on the settings button at the top-right corner, you'll see plenty of parameters you can set, such as temperature:\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_settings.png?raw=True)\n",
    "<center>Figure 6.5 - Screenshot of llama.cpp’s settings</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be242056",
   "metadata": {},
   "source": [
    "##### REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee1432",
   "metadata": {},
   "source": [
    "```python\n",
    "url = 'http://0.0.0.0:8080/completion'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "data = {'prompt': 'There is bacon in this sandwich.',\n",
    "        'n_predict': 128}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e2819",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "print(response.json()['content'])\n",
    "```\n",
    "\n",
    "```\n",
    " There is no bacon in this sandwich. This statement is a paradox because it contradicts itself, yet it seems to suggest that the sandwich has both bacon and no bacon at the same time.\n",
    "\n",
    "2. This statement is also a paradox, as it claims that it is a lie that it is lying. If the statement is true, then it is indeed a lie, making it false. But if it is false, then it is not a lie, making it true. This creates a circular reasoning that can't be resolved.\n",
    "\n",
    "3. This statement is a paradox\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb36d0e",
   "metadata": {},
   "source": [
    "### Thank You!\n",
    "\n",
    "If you have any suggestions, or if you find any errors, please don't hesitate to contact me through [GitHub](https://github.com/dvgodoy), [X](https://x.com/dvgodoy), [BlueSky](https://bsky.app/profile/dvgodoy.bsky.social), or [LinkedIn](https://www.linkedin.com/in/dvgodoy/).\n",
    "\n",
    "If you'd like to receive notifications about new book releases, updates, freebies, and discounts, follow me at:\n",
    "\n",
    "<center><a href=\"https://danielgodoy.gumroad.com/subscribe\">https://danielgodoy.gumroad.com/subscribe</a></center>\n",
    "\n",
    "I'm looking forward to hearing back from you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
