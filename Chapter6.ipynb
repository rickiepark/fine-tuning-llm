{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48e02e8",
      "metadata": {
        "id": "f48e02e8"
      },
      "source": [
        "## 6장 로컬에 배포하기\n",
        "\n",
        "### 스포일러\n",
        "이 장에서는 다음과 같은 내용을 배웁니다.\n",
        "\n",
        "- 빠른 추론을 위해 어댑터를 로드하여 베이스 모델과 병합합니다.\n",
        "- 모델을 사용해 응답 또는 완성을 생성합니다.\n",
        "- 미세 튜닝된 모델을 llama.cpp에서 사용하는 GGUF 파일 포맷으로 변환합니다.\n",
        "- Ollama와 llama.cpp를 사용하고 웹 인터페이스와 REST API를 통해 모델을 서빙(serving)합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245ef1a5",
      "metadata": {
        "id": "245ef1a5"
      },
      "source": [
        "### 패키지 설치\n",
        "\n",
        "훈련 재현성을 위해 이 책에서 사용하는 다음 버전과 동일 버전을 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c353825",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c353825",
        "outputId": "a30cc849-a365-49e3-a038-b418e01a4a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.55.2\n",
            "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.17.0\n",
            "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting accelerate==1.10.0\n",
            "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting trl==0.21.0\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes==0.47.0\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface-hub==0.34.4 in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: safetensors==0.6.2 in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (3.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (0.21.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (1.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (3.2.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.12.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.0) (3.0.2)\n",
            "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, bitsandbytes, accelerate, trl, peft\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.4\n",
            "    Uninstalling transformers-4.55.4:\n",
            "      Successfully uninstalled transformers-4.55.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.1\n",
            "    Uninstalling accelerate-1.10.1:\n",
            "      Successfully uninstalled accelerate-1.10.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "Successfully installed accelerate-1.10.0 bitsandbytes-0.47.0 peft-0.17.0 transformers-4.55.2 trl-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.56.1 peft==0.17.0 accelerate==1.10.0 trl==0.23.1 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba6a8fd",
      "metadata": {
        "id": "aba6a8fd"
      },
      "source": [
        "### 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd09a846",
      "metadata": {
        "id": "bd09a846"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import asdict\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM, get_model_status, \\\n",
        "    get_layer_status, prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1228fd66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1228fd66",
        "outputId": "ac619d07-3d06-4431-f991-364555b33a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-02 02:50:58--  https://raw.githubusercontent.com/rickiepark/fine-tuning-llm/refs/heads/main/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6492 (6.3K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]   6.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-02 02:50:59 (71.1 MB/s) - ‘helper_functions.py’ saved [6492/6492]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 깃허브에서 helper_functions.py 파일을 다운로드합니다.\n",
        "!wget https://raw.githubusercontent.com/rickiepark/fine-tuning-llm/refs/heads/main/helper_functions.py\n",
        "\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810fecf6",
      "metadata": {
        "id": "810fecf6"
      },
      "source": [
        "### 목표\n",
        "\n",
        "(GPU가 없는) 개인용 하드웨어에서 실행할 수 있도록 미세 튜닝된 모델과 어댑터를 GGUF 포맷으로 바꾼 다음 양자화합니다. 그다음 이런 모델과 어댑터를 서빙하기 위해 Ollama나 llama.cpp로 로드합니다. 이렇게 하면 웹 인터페이스나 REST API를 사용해 모델에게 직접 쿼리(query)를 보낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d4d2fca",
      "metadata": {
        "id": "5d4d2fca"
      },
      "source": [
        "### 준비 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afd7b7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679,
          "referenced_widgets": [
            "147645e2ef0041b2b1dd2e157968e4f1",
            "8513ec7b85fc4dc79ac9d57fa96caab7",
            "11925ff8ee1240c39af677820d7a35a9",
            "c35ade80cdf140218fad1ad1fbbeb51a",
            "4e8a641e88ed4bc086ab221aa5be0444",
            "44f180de3fb94dfeb078a4f531513b8f",
            "b6737307bc9840b9aa54faa37ece4f66",
            "4962fc66392d45b197efba758b29584d",
            "b667064fd82d48e9abed113e43f0c503",
            "1596790729bc4696ad85f9dfbeb4282d",
            "c3f5ea30fcc4413f9d15e782334b6360",
            "25998cd7c3694e218c1f2983ffd6c6f5",
            "1f37f6f7a50f479684b27b81a2824e03",
            "e8d10876867144d383c3aa9a7a173ec1",
            "df78632e971a46779b5ec27639b3c6de",
            "9f495f3948ad4aa69e27aa739e32203d",
            "6aea440a608949478e2a4cad014faea8",
            "36f4957d1d6840fc82abc60940922a1c",
            "3ae14b674ec74e7e94391fbac0335926",
            "58785ae436464a21b3e43763b39e79db",
            "6939ab20471341078675c0c3e77abab8",
            "fb078d9072a0413793a3ada303554201",
            "926567d74bdb4fb88530faffa96a782e",
            "f8275750d7fb45c78b18168a6f810091",
            "ba7df7698150440bb970cd7f512df1e6",
            "6ea361d60bdd452fa424edc827df3cb5",
            "3271383becbb4c619168361e754bbf05",
            "7fe4a210fc614c00a34263440aadb8b7",
            "2df6c8e2512d4fcfba97e2bdeb50459b",
            "94786b867e5b458a907a5b99f1ab8253",
            "bfa4caaa8e2d4a8d8054a06664d94561",
            "072cd063f3064340b62439620c897ccc",
            "4e91cb48d06e4cb5affddc30172b029a"
          ]
        },
        "id": "3afd7b7e",
        "outputId": "8f5faf41-0bc7-4a62-fb9c-64f7b56adb0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "147645e2ef0041b2b1dd2e157968e4f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25998cd7c3694e218c1f2983ffd6c6f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "926567d74bdb4fb88530faffa96a782e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Packing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 03:23, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.362300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.807500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.729500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.687800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.656900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.638000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2장\n",
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "model_q4 = AutoModelForCausalLM.from_pretrained(\n",
        "  \"facebook/opt-350m\", device_map='cuda:0', quantization_config=nf4_config\n",
        ")\n",
        "# 3장\n",
        "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "peft_model = get_peft_model(model_q4, config)\n",
        "\n",
        "# 4장\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = modify_tokenizer(tokenizer)\n",
        "tokenizer = add_template(tokenizer)\n",
        "\n",
        "peft_model = modify_model(peft_model, tokenizer)\n",
        "\n",
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "# 프롬프트/완성 쌍을 대화 메시지로 변환합니다.\n",
        "dataset = dataset.map(format_dataset)\n",
        "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
        "\n",
        "# 5장\n",
        "min_effective_batch_size = 8\n",
        "lr = 3e-4\n",
        "max_seq_length = 64\n",
        "collator_fn = None\n",
        "packing = (collator_fn is None)\n",
        "steps = 20\n",
        "num_train_epochs = 10\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir='./future_name_on_the_hub',\n",
        "    # 데이터셋\n",
        "    packing=packing,\n",
        "    packing_strategy='wrapped',\n",
        "    max_length=max_seq_length,\n",
        "    # 그레이디언트 / 메모리\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "    gradient_accumulation_steps=2,\n",
        "    per_device_train_batch_size=min_effective_batch_size,\n",
        "    auto_find_batch_size=True,\n",
        "    # 훈련\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=lr,\n",
        "    # 환경 및 로깅\n",
        "    report_to='tensorboard',\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=steps,\n",
        "    save_strategy='steps',\n",
        "    save_steps=steps,\n",
        "    bf16=supported\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_mode,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collator_fn,\n",
        "    args=sft_config\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model('yoda-adapter') # trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ecc5796",
      "metadata": {
        "id": "6ecc5796"
      },
      "source": [
        "### 모델과 어댑터를 로드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c331d06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "02185a0fb9274f64921695b86a0343d2",
            "556f560e661f48f893366c1ce7954c90",
            "f405f9135548441c85283e23aaa0cd64",
            "fe15c363ca74406c9cd6cfcf39a5e155",
            "b84aa6731f0b42efb0f60217c068121e",
            "a694baadde8e4f219ec45e8ba43a5176",
            "a1ef33a885d5425a9e8e583a49635b3c",
            "e83842f0bcad4bea929b944c84573137",
            "3d1039c2207b44a991a7b61ee9b74269",
            "eee6e2ff3dad446a97193da8539a9639",
            "5c2085f7f9ca4375b6c1a811ef37ce1c",
            "8db1150df2844a52a0f249ec3eeccad1",
            "c6c6e78a72d74293a5120243d4611dc7",
            "910232a3d74949738c52086c5f38377d",
            "8c7c1132b85f47b49a7c749dabc8d666",
            "a836045eacf14525872ce8b9aced074e",
            "687544f0d80b4b759151702924f0b829",
            "62c923933d9e4e8c9c391c7dcc95cc37",
            "34a282299258416eb2f94016899a4a1d",
            "1bee3eeda1be4b8983a8699c0cb15cfd",
            "3a3b58e75ec3485da0af2f8046b619ad",
            "47a391a558534d69b3b0391eefccb456",
            "60d8dfd449fe469e8916c0c415d24e3f",
            "5adcec75de6443a39470cadcb04d5a57",
            "0f1e4f9625e54ed488c1e5e849078f13",
            "81cd9083c3394f8dac6c031f973860bb",
            "6d5a15f397734376a41ae7dc8154e4fe",
            "1bac3ffc97a94f3dac4a4aa962aac535",
            "4fc8d58590da466aa201249edafd4b10",
            "ea1b47fb8fc74339a54c65eaad2173e5",
            "29993311bfaa48ecb38716ae95dddc38",
            "6d4edbef6ef9457a9b2c324824244722",
            "88d0850434e24e4fb8f9ef5eaae684f5",
            "bc7e949f6ce54b39afa16ffb3dfd89db",
            "b14e2eaae0934f198999467a6782ed5f",
            "6dbf9e76e3404c2da52abbfddaf9a571",
            "18e62cedb21a43639bc6a4c8a2fc4444",
            "1e9212fe3929413888422298f143b59e",
            "0f40d16d9f9345b2903b5fdd3c465e59",
            "a2f71d9fbebe4e848c27be64337f8846",
            "6f0cda7864c94f4698151b398fe8b80f",
            "cdb3c719985e4abebc6e722650f5a386",
            "9cfe596db4504829ae581ddff0435cc6",
            "7f315fa839524f92800d552331c5e7be",
            "5f3d5c8d52554bc9bce073360f619ace",
            "8ba2f55325294c5694ae2fabd6d7fb6d",
            "2cb4c75e5473447696ce537f0ef8ecc0",
            "b8454160f00c4a65b38608f545076767",
            "d9381e48567d45f9840413e216b9e170",
            "33041a6094d74f2fb29ed3110b2842b8",
            "578637fc6d2e4b96b6413643cffadc33",
            "705b648a17374854932042ca12b450af",
            "07e215e7fab84dd5a1a4195f9a6940c8",
            "af1a47a91c07486c935d6ea0d96671d2",
            "f6d897edf66f4fcab243482dadcddea9",
            "8fb29587f27d4916acdb4f05e9db2cf4",
            "356ca33745ce4bd188ab81110fc794be",
            "5329ba23dd7946428d53525fba917689",
            "88ea1fb708ff4aef876c0c79782a2766",
            "c0b136a2c828425fb828ace2d053b653",
            "a3e9b62266d746c796e6fd1c5880b9c1",
            "86d87f34916942d6a98d7f3158bb0f87",
            "76d392134f614e918c0b7f76e73f1951",
            "7f889168e9bc4bc78fd41fd435cfdf01",
            "5b3d5dd8b7384599a9c2646acb22f6ce",
            "53800838af7c40c5b43abac51bcb60f0",
            "03b8971eb3b74beb99c4b21ae588b833",
            "4823cf588ba44fd6b5173a4ab81f620e",
            "bcdf77bccf794ff295ba61ebffc1ec39",
            "f4f59278ddf945a0858d1db8effc7e97",
            "4d4cc37498874a559fd6bb00b46b828d",
            "de0f11ab240d4e93980da01a4fc93076",
            "def8d34f23ba407c9451676572811d5b",
            "9dc9ce3461fa4a2f87ce5ea55bde8c98",
            "cef495134c5049cf9b1ff9f1554457bf",
            "a677c996d2724f7f8d6241d2ae4a89c8",
            "a5c7cf22897446659cddeb7839ab8288",
            "a914884255784f77bb07cb7fe778942f",
            "61cfd8317c5d44ea969921c0a3384b78",
            "b5e0bd3a9f684afea2fc1cce255ef975",
            "ed1906bf67fb493b9835fa009e51e25d",
            "54ef26f5ba6f4e24b73b34d07ecf2129",
            "6e8a4613cb7e44ed8c6c7fae85882026",
            "e71e467aa2884585a96287ba1728ffbf",
            "3d09f3bcbc994aff898aba392182dd2c",
            "fdd515dda4e3424292a11ec70fc43b68",
            "3db2f8048e83409484acfc1e1c3ec1b6",
            "1918bffbd0374bfa89b0f361307b50f0"
          ]
        },
        "id": "2c331d06",
        "outputId": "7aacbf5f-3f16-4438-81ca-a3857ab12f68"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02185a0fb9274f64921695b86a0343d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8db1150df2844a52a0f249ec3eeccad1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60d8dfd449fe469e8916c0c415d24e3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc7e949f6ce54b39afa16ffb3dfd89db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f3d5c8d52554bc9bce073360f619ace",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fb29587f27d4916acdb4f05e9db2cf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/71.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03b8971eb3b74beb99c4b21ae588b833",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a914884255784f77bb07cb7fe778942f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/3.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): OPTForCausalLM(\n",
              "      (model): OPTModel(\n",
              "        (decoder): OPTDecoder(\n",
              "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
              "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
              "          (layers): ModuleList(\n",
              "            (0-23): 24 x OPTDecoderLayer(\n",
              "              (self_attn): OPTAttention(\n",
              "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (v_proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (yoda): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (yoda): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (q_proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (yoda): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (yoda): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              )\n",
              "              (activation_fn): ReLU()\n",
              "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(repo_or_folder,\n",
        "                                                 device_map='auto',\n",
        "                                                 adapter_name='yoda')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95fe9d58",
      "metadata": {
        "id": "95fe9d58"
      },
      "source": [
        "****\n",
        "**중요**: 현재는 토크나이저의 어휘사전 크기가 임베딩 층의 크기를 넘어서는 경우에만 임베딩 층의 크기가 변경됩니다. 따라서 책에서 언급한 오랜 문제가 해결되었습니다.\n",
        "\n",
        "이 문제 때문에 다음처럼 어댑터의 LoRA 설정에 따라 베이스 모델을 로드하고, PeftModel 클래스를 사용해 모델과 어댑터를 병합해야 했습니다.\n",
        "\n",
        "```python\n",
        "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
        "config = PeftConfig.from_pretrained(repo_or_folder)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "  config.base_model_name_or_path,\n",
        "  device_map='auto'\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "  base_model,\n",
        "  repo_or_folder,\n",
        "  adapter_name='yoda'\n",
        ")\n",
        "```\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83cda575",
      "metadata": {
        "id": "83cda575"
      },
      "outputs": [],
      "source": [
        "model.merge_adapter(['yoda'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf03e85",
      "metadata": {
        "id": "cbf03e85"
      },
      "outputs": [],
      "source": [
        "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_or_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411bc34f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "411bc34f",
        "outputId": "f007f72e-470f-429f-842c-8f65be771490"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"model.model.decoder.layers.13.self_attn.q_proj\",\n          \"model.model.decoder.layers.20.self_attn.v_proj\",\n          \"model.model.decoder.layers.13.self_attn.v_proj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"module_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"lora.Linear\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"enabled\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"active_adapters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"merged_adapters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"requires_grad\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"available_adapters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"devices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bd1a048d-191b-4077-85eb-90b6634a383e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>module_type</th>\n",
              "      <th>enabled</th>\n",
              "      <th>active_adapters</th>\n",
              "      <th>merged_adapters</th>\n",
              "      <th>requires_grad</th>\n",
              "      <th>available_adapters</th>\n",
              "      <th>devices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model.model.decoder.layers.0.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>model.model.decoder.layers.0.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>model.model.decoder.layers.1.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>model.model.decoder.layers.1.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>model.model.decoder.layers.2.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>model.model.decoder.layers.2.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>model.model.decoder.layers.3.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>model.model.decoder.layers.3.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>model.model.decoder.layers.4.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>model.model.decoder.layers.4.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>model.model.decoder.layers.5.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>model.model.decoder.layers.5.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>model.model.decoder.layers.6.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>model.model.decoder.layers.6.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>model.model.decoder.layers.7.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>model.model.decoder.layers.7.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>model.model.decoder.layers.8.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>model.model.decoder.layers.8.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>model.model.decoder.layers.9.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>model.model.decoder.layers.9.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>model.model.decoder.layers.10.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>model.model.decoder.layers.10.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>model.model.decoder.layers.11.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>model.model.decoder.layers.11.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>model.model.decoder.layers.12.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>model.model.decoder.layers.12.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>model.model.decoder.layers.13.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>model.model.decoder.layers.13.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>model.model.decoder.layers.14.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>model.model.decoder.layers.14.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>model.model.decoder.layers.15.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>model.model.decoder.layers.15.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>model.model.decoder.layers.16.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>model.model.decoder.layers.16.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>model.model.decoder.layers.17.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>model.model.decoder.layers.17.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>model.model.decoder.layers.18.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>model.model.decoder.layers.18.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>model.model.decoder.layers.19.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>model.model.decoder.layers.19.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>model.model.decoder.layers.20.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>model.model.decoder.layers.20.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>model.model.decoder.layers.21.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>model.model.decoder.layers.21.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>model.model.decoder.layers.22.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>model.model.decoder.layers.22.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>model.model.decoder.layers.23.self_attn.v_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>model.model.decoder.layers.23.self_attn.q_proj</td>\n",
              "      <td>lora.Linear</td>\n",
              "      <td>True</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': False}</td>\n",
              "      <td>[yoda]</td>\n",
              "      <td>{'yoda': ['cuda']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd1a048d-191b-4077-85eb-90b6634a383e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bd1a048d-191b-4077-85eb-90b6634a383e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bd1a048d-191b-4077-85eb-90b6634a383e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2cd97c16-5db7-42c8-be3b-721263686390\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2cd97c16-5db7-42c8-be3b-721263686390')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2cd97c16-5db7-42c8-be3b-721263686390 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_05319ffe-84cb-484e-8e19-0da4d79da661\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_05319ffe-84cb-484e-8e19-0da4d79da661 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              name  module_type  enabled  \\\n",
              "0    model.model.decoder.layers.0.self_attn.v_proj  lora.Linear     True   \n",
              "1    model.model.decoder.layers.0.self_attn.q_proj  lora.Linear     True   \n",
              "2    model.model.decoder.layers.1.self_attn.v_proj  lora.Linear     True   \n",
              "3    model.model.decoder.layers.1.self_attn.q_proj  lora.Linear     True   \n",
              "4    model.model.decoder.layers.2.self_attn.v_proj  lora.Linear     True   \n",
              "5    model.model.decoder.layers.2.self_attn.q_proj  lora.Linear     True   \n",
              "6    model.model.decoder.layers.3.self_attn.v_proj  lora.Linear     True   \n",
              "7    model.model.decoder.layers.3.self_attn.q_proj  lora.Linear     True   \n",
              "8    model.model.decoder.layers.4.self_attn.v_proj  lora.Linear     True   \n",
              "9    model.model.decoder.layers.4.self_attn.q_proj  lora.Linear     True   \n",
              "10   model.model.decoder.layers.5.self_attn.v_proj  lora.Linear     True   \n",
              "11   model.model.decoder.layers.5.self_attn.q_proj  lora.Linear     True   \n",
              "12   model.model.decoder.layers.6.self_attn.v_proj  lora.Linear     True   \n",
              "13   model.model.decoder.layers.6.self_attn.q_proj  lora.Linear     True   \n",
              "14   model.model.decoder.layers.7.self_attn.v_proj  lora.Linear     True   \n",
              "15   model.model.decoder.layers.7.self_attn.q_proj  lora.Linear     True   \n",
              "16   model.model.decoder.layers.8.self_attn.v_proj  lora.Linear     True   \n",
              "17   model.model.decoder.layers.8.self_attn.q_proj  lora.Linear     True   \n",
              "18   model.model.decoder.layers.9.self_attn.v_proj  lora.Linear     True   \n",
              "19   model.model.decoder.layers.9.self_attn.q_proj  lora.Linear     True   \n",
              "20  model.model.decoder.layers.10.self_attn.v_proj  lora.Linear     True   \n",
              "21  model.model.decoder.layers.10.self_attn.q_proj  lora.Linear     True   \n",
              "22  model.model.decoder.layers.11.self_attn.v_proj  lora.Linear     True   \n",
              "23  model.model.decoder.layers.11.self_attn.q_proj  lora.Linear     True   \n",
              "24  model.model.decoder.layers.12.self_attn.v_proj  lora.Linear     True   \n",
              "25  model.model.decoder.layers.12.self_attn.q_proj  lora.Linear     True   \n",
              "26  model.model.decoder.layers.13.self_attn.v_proj  lora.Linear     True   \n",
              "27  model.model.decoder.layers.13.self_attn.q_proj  lora.Linear     True   \n",
              "28  model.model.decoder.layers.14.self_attn.v_proj  lora.Linear     True   \n",
              "29  model.model.decoder.layers.14.self_attn.q_proj  lora.Linear     True   \n",
              "30  model.model.decoder.layers.15.self_attn.v_proj  lora.Linear     True   \n",
              "31  model.model.decoder.layers.15.self_attn.q_proj  lora.Linear     True   \n",
              "32  model.model.decoder.layers.16.self_attn.v_proj  lora.Linear     True   \n",
              "33  model.model.decoder.layers.16.self_attn.q_proj  lora.Linear     True   \n",
              "34  model.model.decoder.layers.17.self_attn.v_proj  lora.Linear     True   \n",
              "35  model.model.decoder.layers.17.self_attn.q_proj  lora.Linear     True   \n",
              "36  model.model.decoder.layers.18.self_attn.v_proj  lora.Linear     True   \n",
              "37  model.model.decoder.layers.18.self_attn.q_proj  lora.Linear     True   \n",
              "38  model.model.decoder.layers.19.self_attn.v_proj  lora.Linear     True   \n",
              "39  model.model.decoder.layers.19.self_attn.q_proj  lora.Linear     True   \n",
              "40  model.model.decoder.layers.20.self_attn.v_proj  lora.Linear     True   \n",
              "41  model.model.decoder.layers.20.self_attn.q_proj  lora.Linear     True   \n",
              "42  model.model.decoder.layers.21.self_attn.v_proj  lora.Linear     True   \n",
              "43  model.model.decoder.layers.21.self_attn.q_proj  lora.Linear     True   \n",
              "44  model.model.decoder.layers.22.self_attn.v_proj  lora.Linear     True   \n",
              "45  model.model.decoder.layers.22.self_attn.q_proj  lora.Linear     True   \n",
              "46  model.model.decoder.layers.23.self_attn.v_proj  lora.Linear     True   \n",
              "47  model.model.decoder.layers.23.self_attn.q_proj  lora.Linear     True   \n",
              "\n",
              "   active_adapters merged_adapters    requires_grad available_adapters  \\\n",
              "0           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "1           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "2           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "3           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "4           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "5           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "6           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "7           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "8           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "9           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "10          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "11          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "12          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "13          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "14          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "15          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "16          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "17          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "18          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "19          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "20          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "21          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "22          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "23          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "24          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "25          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "26          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "27          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "28          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "29          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "30          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "31          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "32          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "33          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "34          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "35          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "36          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "37          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "38          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "39          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "40          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "41          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "42          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "43          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "44          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "45          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "46          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "47          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
              "\n",
              "               devices  \n",
              "0   {'yoda': ['cuda']}  \n",
              "1   {'yoda': ['cuda']}  \n",
              "2   {'yoda': ['cuda']}  \n",
              "3   {'yoda': ['cuda']}  \n",
              "4   {'yoda': ['cuda']}  \n",
              "5   {'yoda': ['cuda']}  \n",
              "6   {'yoda': ['cuda']}  \n",
              "7   {'yoda': ['cuda']}  \n",
              "8   {'yoda': ['cuda']}  \n",
              "9   {'yoda': ['cuda']}  \n",
              "10  {'yoda': ['cuda']}  \n",
              "11  {'yoda': ['cuda']}  \n",
              "12  {'yoda': ['cuda']}  \n",
              "13  {'yoda': ['cuda']}  \n",
              "14  {'yoda': ['cuda']}  \n",
              "15  {'yoda': ['cuda']}  \n",
              "16  {'yoda': ['cuda']}  \n",
              "17  {'yoda': ['cuda']}  \n",
              "18  {'yoda': ['cuda']}  \n",
              "19  {'yoda': ['cuda']}  \n",
              "20  {'yoda': ['cuda']}  \n",
              "21  {'yoda': ['cuda']}  \n",
              "22  {'yoda': ['cuda']}  \n",
              "23  {'yoda': ['cuda']}  \n",
              "24  {'yoda': ['cuda']}  \n",
              "25  {'yoda': ['cuda']}  \n",
              "26  {'yoda': ['cuda']}  \n",
              "27  {'yoda': ['cuda']}  \n",
              "28  {'yoda': ['cuda']}  \n",
              "29  {'yoda': ['cuda']}  \n",
              "30  {'yoda': ['cuda']}  \n",
              "31  {'yoda': ['cuda']}  \n",
              "32  {'yoda': ['cuda']}  \n",
              "33  {'yoda': ['cuda']}  \n",
              "34  {'yoda': ['cuda']}  \n",
              "35  {'yoda': ['cuda']}  \n",
              "36  {'yoda': ['cuda']}  \n",
              "37  {'yoda': ['cuda']}  \n",
              "38  {'yoda': ['cuda']}  \n",
              "39  {'yoda': ['cuda']}  \n",
              "40  {'yoda': ['cuda']}  \n",
              "41  {'yoda': ['cuda']}  \n",
              "42  {'yoda': ['cuda']}  \n",
              "43  {'yoda': ['cuda']}  \n",
              "44  {'yoda': ['cuda']}  \n",
              "45  {'yoda': ['cuda']}  \n",
              "46  {'yoda': ['cuda']}  \n",
              "47  {'yoda': ['cuda']}  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(asdict(layer) for layer in get_layer_status(model))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba40822b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba40822b",
        "outputId": "ed43adc3-da77-4bea-c921-aecc8bc93c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TunerModelStatus(base_model_type='OPTForCausalLM', adapter_model_type='LoraModel', peft_types={'yoda': 'LORA'}, trainable_params=0, total_params=331982848, num_adapter_layers=48, enabled=True, active_adapters=['yoda'], merged_adapters=['yoda'], requires_grad={'yoda': False}, available_adapters=['yoda'], devices={'yoda': ['cuda']})\n"
          ]
        }
      ],
      "source": [
        "print(get_model_status(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74c72fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a74c72fb",
        "outputId": "6b7a13f6-a29b-4477-e44d-cbab3b001d8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fb1c59",
      "metadata": {
        "id": "66fb1c59"
      },
      "source": [
        "### 모델에 쿼리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c663f21f",
      "metadata": {
        "id": "c663f21f"
      },
      "outputs": [],
      "source": [
        "def gen_prompt(tokenizer, sentence):\n",
        "    converted_sample = [\n",
        "        {\"role\": \"user\", \"content\": sentence},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
        "                                           tokenize=False,\n",
        "                                           add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41066fe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41066fe7",
        "outputId": "1bbe7725-118b-4b7f-f756-280ab22aa042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "There is bacon in this sandwich.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = gen_prompt(tokenizer, 'There is bacon in this sandwich.')\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bb4da4",
      "metadata": {
        "id": "97bb4da4"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt,\n",
        "             max_new_tokens=64,\n",
        "             skip_special_tokens=False,\n",
        "             response_only=False):\n",
        "    # 포맷팅된 프롬프트를 토큰화합니다.\n",
        "    tokenized_input = tokenizer(prompt,\n",
        "                                add_special_tokens=False,\n",
        "                                return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    # 혼합 정밀도를 사용해 훈련하는 경우 autocast 컨택스트를 사용합니다.\n",
        "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
        "        if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
        "    with ctx:\n",
        "        generation_output = model.generate(**tokenized_input,\n",
        "                                           eos_token_id=tokenizer.eos_token_id,\n",
        "                                           max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # 필요한 경우 프롬프트에 속한 토큰을 제외합니다.\n",
        "    if response_only:\n",
        "        input_length = tokenized_input['input_ids'].shape[1]\n",
        "        generation_output = generation_output[:, input_length:]\n",
        "\n",
        "    # 토큰을 다시 텍스트로 디코딩합니다.\n",
        "    output = tokenizer.batch_decode(generation_output,\n",
        "                                    skip_special_tokens=skip_special_tokens)[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4b5b77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b4b5b77",
        "outputId": "71e26ed1-f297-481c-eeea-c44029e94d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "There is bacon in this sandwich.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "In this sandwich, bacon there is.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, tokenizer,prompt, skip_special_tokens=False, response_only=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452a830e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "452a830e",
        "outputId": "7b86fb44-026f-44e4-cbfb-a5f574c86306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In this sandwich, bacon there is.\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, tokenizer,prompt, skip_special_tokens=True, response_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b919d66",
      "metadata": {
        "id": "5b919d66"
      },
      "outputs": [],
      "source": [
        "sentences  = ['There is bacon in this sandwich.', 'Add some cheddar to it.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17acc29f",
      "metadata": {
        "id": "17acc29f"
      },
      "outputs": [],
      "source": [
        "def batch_generate(model, tokenizer, sentences,\n",
        "             max_new_tokens=64,\n",
        "             skip_special_tokens=False,\n",
        "             response_only=False):\n",
        "\n",
        "    # 프롬프트를 대화 포맷으로 변경합니다.\n",
        "    converted_samples = [[{\"role\": \"user\", \"content\": sentence}]\n",
        "                         for sentence in sentences]\n",
        "\n",
        "    # 프롬프트를 포맷팅하기 위해 채팅 템플릿을 적용합니다.\n",
        "    prompts = tokenizer.apply_chat_template(converted_samples,\n",
        "                                            tokenize=False,\n",
        "                                            add_generation_prompt=True)\n",
        "\n",
        "    # 배치 생성을 위해 왼쪽 패딩으로 설정\n",
        "    tokenizer.padding_side = 'left'\n",
        "    # 패딩을 포함해 포맷팅된 프롬프트를 토큰화합니다.\n",
        "    tokenized_inputs = tokenizer(prompts,\n",
        "                                 padding=True,\n",
        "                                 add_special_tokens=False,\n",
        "                                 return_tensors='pt').to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    # 혼합 정밀도를 사용해 훈련하는 경우 autocast 컨택스트를 사용합니다.\n",
        "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
        "        if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
        "    with ctx:\n",
        "        generation_output = model.generate(**tokenized_inputs,\n",
        "                                           eos_token_id=tokenizer.eos_token_id,\n",
        "                                           pad_token_id=tokenizer.pad_token_id,\n",
        "                                           max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # 필요한 경우 프롬프트에 해당하는 토큰 제외\n",
        "    if response_only:\n",
        "        input_length = tokenized_inputs['input_ids'].shape[1]\n",
        "        generation_output = generation_output[:, input_length:]\n",
        "\n",
        "    # 토큰을 다시 텍스트로 디코딩합니다.\n",
        "    output = tokenizer.batch_decode(generation_output,\n",
        "                                    skip_special_tokens=skip_special_tokens)\n",
        "    if isinstance(sentences, str):\n",
        "        output = output[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee75635c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee75635c",
        "outputId": "393c78a1-a139-4a43-db5f-dee4a7975696"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In this sandwich, bacon there is.', 'To it, add some cheddar, you must.']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_generate(model, tokenizer, sentences, skip_special_tokens=True, response_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dd75791",
      "metadata": {
        "id": "7dd75791"
      },
      "source": [
        "### Llama.cpp\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp.png?raw=True)\n",
        "\n",
        "<center>그림 6.1 - llama.cpp의 깃허브 저장소 스크린샷</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d9754e",
      "metadata": {
        "id": "07d9754e"
      },
      "source": [
        "#### 어댑터 변환\n",
        "\n",
        "어댑터를 GGUF 포맷으로 변환하려면 다음 단계를 따릅니다.\n",
        "\n",
        "- 이전 장에서 했던 것처럼 훈련 후 `save_model()` 메서드를 호출하거나 허깅 페이스 허브에서 어댑터를 다운로드(자세한 내용은 사이드바 참조)하여 로컬 폴더에 저장합니다.\n",
        "- 깃허브에서 llama.cpp 저장소를 복제(clone)합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197d2fad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "197d2fad",
        "outputId": "eb45419c-dc36-404c-d769-c057a0902d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 60558, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 60558 (delta 1), reused 2 (delta 1), pack-reused 60545 (from 1)\u001b[K\n",
            "Receiving objects: 100% (60558/60558), 150.73 MiB | 14.97 MiB/s, done.\n",
            "Resolving deltas: 100% (43981/43981), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fdbc969",
      "metadata": {
        "id": "0fdbc969"
      },
      "source": [
        "- `gguf-py` 패키지와 `mistral-common` 패키지를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c870a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52c870a1",
        "outputId": "9462940c-cbb5-4197-99d1-2b825c8cb979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./llama.cpp/gguf-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from gguf==0.17.1) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf==0.17.1) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf==0.17.1) (4.67.1)\n",
            "Building wheels for collected packages: gguf\n",
            "  Building wheel for gguf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gguf: filename=gguf-0.17.1-py3-none-any.whl size=104600 sha256=8944201e1ee9bbf870fa3a510789aeb3ada09ee2c91d1565a23a69a3358fc46d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/b7/15/8e6796fb0734c2c4fa1234732c782043ead15465c2e6f75560\n",
            "Successfully built gguf\n",
            "Installing collected packages: gguf\n",
            "Successfully installed gguf-0.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip install llama.cpp/gguf-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "epUWapSPQbzI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epUWapSPQbzI",
        "outputId": "c198f6ab-6e35-4fcd-f528-9ef9aa5f72f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mistral-common\n",
            "  Downloading mistral_common-1.8.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral-common\n",
            "Successfully installed mistral-common-1.8.4 pycountry-24.6.1 pydantic-extra-types-2.10.5\n"
          ]
        }
      ],
      "source": [
        "!pip install mistral-common"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ccc7e0",
      "metadata": {
        "id": "f9ccc7e0"
      },
      "source": [
        "***\n",
        "**허깅 페이스 허브에서 모델 다운로드하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1493a2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "e8c4df90e2524abd976b7d28d7fb4ada",
            "2b1a57cc6bd14c44aad54bccd5a7ccde",
            "dc1c88397b98420ea8cbb49e7340aa2f",
            "ed70c82010b044778a276470caea849b",
            "920eac0cd12142ba9de612140fc752b7",
            "431956d6cd724090bc127c7cb8fdb090",
            "519c9d754d7a4f16826219ad678a8b2b",
            "a53161e2e9fe445f9a5e1609fa4c097d",
            "f670e9fdf70d4d4895d745dc9786e77c",
            "317ca3ad812e466a90ae57b8db35a8d5",
            "c2535082da354d50a88e19010e4f8b7d",
            "24260c4ccddf433185316c8ee8a5a8d9",
            "684c0b75fbf946f884e0546a4b71c042",
            "a663a01e310f4675822222687bd95c72",
            "d2213b9b41244136a747203a529b5750",
            "cc42df50a2e243f887266c253bcd917f",
            "f594392b14204e118e1abe4ca0634d55"
          ]
        },
        "id": "f1493a2f",
        "outputId": "e2a93481-b12c-49e2-9615-07ee444b19ad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8c4df90e2524abd976b7d28d7fb4ada",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d37117",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "4ea51a407fa24aa08f446526cf100cf5",
            "a61772e3064840dd9501881bc097075b",
            "ba010a109c634042bfc04d6a070509c1",
            "99095c0570784d12aa4cb79b834ccd56",
            "da882c2c5193423780203a476fac566b",
            "c40f78566c6549e6930b6d4e10dc27a9",
            "ccd8c2e6337e40d5b1a92dfd77b3fef9",
            "018286e1776543299b722adc9886b1f4",
            "6557e89dcb234f9793894f90d4a8049c",
            "6e0c608b2c074d7394e3089d2c6d4a0a",
            "c60c63b682fe44508ac0f3dd0e79812e",
            "56535ddaf9fd439b9149be55066a8576",
            "d52982266ee64043807f6c47a697e281",
            "67825d157b9442d1aa621ab0df610d38",
            "45c38e5a43a94df2bd15c3af122ecfd3",
            "7ef915832b3a48c4ae8afeb3f82d9b47",
            "d2d24a1b47dd4907a84a36513e5ffc5c",
            "a5638808909c4330a3a680c1a07b68ec",
            "e448bbc8243c4879a2d07bf7e049bcc4",
            "ebc15efe96504b37815a8fd501c5f09b",
            "31b58e37db964673b3aadec05f468bf5",
            "1791b33e4a714f9ebf0b30116de43192",
            "249219763bba4f18802a01b8641fca5c",
            "da179543b2ca49f28cc55d5a7e3a81c8",
            "56474a1878d74068b9b65f954c7c5a2f",
            "ddfc62a0be1043bb8cca113c15a27d59",
            "6248976fa53245b6a022a34864680237",
            "e2b73500ae9f431c8639604ed0a04645",
            "2a632a2e8ad243ea94b1c89f99f0e0b7",
            "672eae2f43ff4ef390afef6695e8a792",
            "0c4403a31ef9456bb5c8bf6a0613a266",
            "15682684c4034fadb75391a2f72e9ec7",
            "b12798c85d1a45d5902efea50ac3e636",
            "fac1ea3dfc914dc886244f1c65f15ac8",
            "3415f55c96f3404cb068f4896e1be705",
            "42c5b21c1f0243df9bb754fc01ad697f",
            "9241a2e357994e379ab106bdc03071db",
            "8c94e33ba8f4411689c89193ec76c8a8",
            "9c009d2b78d34929828622447f9a78fe",
            "a95b512197d94b7faa7d45ba8fac4949",
            "4c9d9adce66f463fb76e0b3f86c9f2d5",
            "eca34bb3874a4248a86b1a4f96fcda06",
            "4458b3f0764f452b8bfddea62799f2a3",
            "a3d4dc29258340a0a6a845277f8aedc2",
            "1f2ffdee24f64a319a2fb93f95987346",
            "0c2faf87e060441fa2e8072e2eec56b9",
            "9e312bb8f781423e933b0f9727638ac7",
            "d9e710fa1a3a415bafa37b5e3240c50a",
            "d12d40d5e8d94be39b8d557d542902af",
            "8d5be5dabd304c4f8836389a4d534ffc",
            "be91563721e94770a1d2d13550cf41d3",
            "cbbda3bae7c94b23991710e0d818fb47",
            "77dea37bfa1d4037b875cddfd2219861",
            "795ed0821e5240e091b758257983cf65",
            "3319ea8fcd944d41a3ca2e72307ef3b7",
            "b13d9a8865404566a209bde5cac5ca0b",
            "2cc903ca640847be8b8e875905b13d43",
            "832daa497a7246fda3853d4e410f09dc",
            "0e0a513f5dee42fca26663301e04c47c",
            "c03f6ae86f6b49d5bc6a4e9e5fa1e398",
            "8eb2df8d13e5412f8597db6791aaed45",
            "e4bc4953f73b48fa9ce7669e36291576",
            "393f321394684e50a2be06256f982d10",
            "7a2bba5a1e51401e8b1890c5cbc2edd8",
            "e2758477286b414db2de2c0edac0ee09",
            "24c26bdc6d28423288910e650a243fa4",
            "fd5c6c99eb6b4dd1bd9cb40fb8a1d665",
            "f7a70ed82d8c4023b03b963a81276b16",
            "8d393a99d42d43d985cb88eae9215c12",
            "4a9fa3a8eb0b47278e71ca1097d9e328",
            "c117397fcc4c4c9194eb17fe37677abf",
            "b3a2400b67f64db891b00519b2191194",
            "f5497421f97c4fc9ae7d50e2d8d62075",
            "f6f6b675135749c796c7c5a212cefee1",
            "25c0e374ed0d4b46b4b08813730b1e54",
            "5ca3ddc5acd24196b5e15383ffba42d7",
            "00aec8775ffb4426b7039205d908c97b",
            "b4d74fc354924539a4bc2f5a1b4152c1",
            "32a9090cb9204c648ddc94c3c4c739cf",
            "57b9d82ff32444e78c39c5c1a4ae86eb",
            "f1f4d9d7060f404889d7aa6a287f9dcc",
            "563c5f6013c849769ea06b144e5e61f1",
            "14799277e6624aad86f2509db02002dc",
            "c12ea64cc7fa46d78f2c483d500cd5cc",
            "e14820b5e8e246ac9d0e055e477683f3",
            "7af4160414cf43078b9bf064e5551c72",
            "0e029ce369274ca1a6068054209876b2",
            "0d1b6a51746244a6ab891800d58a81cd",
            "8ade8aa83c7a407798d9fd255899fc45",
            "9c7f1490f5e749cebaf892adc3626e57",
            "5ee2352a2e8b484896c4bff0c080a400",
            "6c2dd47606a845d09fa4a62716c5b3db",
            "aac3b0de37cd43379a21e274356f5192",
            "54198d70ac89433ba50ba530fc4b431e",
            "209902cc222c4b47b4ce36bf70703908",
            "79983874ad8743a0afb9a383b55f529b",
            "4f3d6fc7e45f4a589fbd3967b74fd873",
            "47ff52d7f626490387163642e2645b69",
            "87cff2156ea3461781ea48d8fcb4e74c",
            "012589e12c4b4f33bd9a8cc1333f3aa3",
            "d8807efaa9244c69912788f894ba9fab",
            "c7d2d9b35383473183e7e93c241068f3",
            "f4cefeb4d0e54b3b8323054b849b1225",
            "0fa44c48d5e545d2a3bf12c928c0c335",
            "b0dd00a5af7b4e7abf29cad89f575606",
            "545d0c52443141beb7dd7e8e27ed1350",
            "0a52569265fb4be8a29f1b36436ffe29",
            "014e18e98c984c3e98477d6c77c5b812",
            "2cab6f278b054d4aa6c281d1d300c623",
            "186a476eba7e42648fa54734420ef2e0",
            "c4bd2eecf9834d5c8f11ba1be98ba261",
            "1cf7ba7e646046a38d45437de69e425c",
            "ea07924783c44cd4a35fca02d9df9cf3",
            "d395f43e199745829910824e40ddc8ce",
            "c43aa9f9ec7a48179e157cc1a2ec4762",
            "e5a68b7074b54520aaec0a5232b6ddd9",
            "753b799bcae14aafbb744b9b424c9f23",
            "7736a9c20f2d414288dfdeeeca4f3831",
            "f0f3dc12fe4d46a0a80e28652e268710",
            "a7601a5298af4b22a974b170b589fe8f",
            "a83251983140400f99b74f887708a1a2"
          ]
        },
        "id": "f3d37117",
        "outputId": "dc383e3e-aee7-4fda-a846-c17fd29058ed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ea51a407fa24aa08f446526cf100cf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56535ddaf9fd439b9149be55066a8576",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "249219763bba4f18802a01b8641fca5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fac1ea3dfc914dc886244f1c65f15ac8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f2ffdee24f64a319a2fb93f95987346",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b13d9a8865404566a209bde5cac5ca0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd5c6c99eb6b4dd1bd9cb40fb8a1d665",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4d74fc354924539a4bc2f5a1b4152c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/696 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ade8aa83c7a407798d9fd255899fc45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "012589e12c4b4f33bd9a8cc1333f3aa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/4.99k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4bd2eecf9834d5c8f11ba1be98ba261",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/phi3-mini-yoda-adapter'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"dvgodoy/phi3-mini-yoda-adapter\", local_dir='./phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bccb35f0",
      "metadata": {
        "id": "bccb35f0"
      },
      "source": [
        "***\n",
        "\n",
        "- `convert_lora_to_gguf.py` 스크립트를 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a621c7be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a621c7be",
        "outputId": "7814b37e-c2a5-4a1b-b164-28e0922adbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:lora-to-gguf:Loading base model from Hugging Face: microsoft/Phi-3-mini-4k-instruct\n",
            "config.json: 100% 967/967 [00:00<00:00, 5.69MB/s]\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:lora-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.0.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.1.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.10.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.11.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.12.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.13.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.14.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.15.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.16.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.17.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.18.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.19.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.2.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.20.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.21.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.22.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.23.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.24.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.25.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.26.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.27.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.28.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.29.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.3.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.30.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.31.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.4.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.5.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.6.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.7.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.8.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8192, 8}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {8, 16384}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_b, torch.float32 --> F32, shape = {8, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_qkv.weight.lora_a, torch.float32 --> F32, shape = {3072, 8}\n",
            "INFO:hf-to-gguf:blk.9.attn_qkv.weight.lora_b, torch.float32 --> F32, shape = {8, 9216}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:adapter.gguf: n_tensors = 256, total_size = 50.3M\n",
            "Writing: 100% 50.3M/50.3M [00:00<00:00, 665Mbyte/s]\n",
            "INFO:lora-to-gguf:Model successfully exported to adapter.gguf\n"
          ]
        }
      ],
      "source": [
        "!python ./llama.cpp/convert_lora_to_gguf.py \\\n",
        "        ./phi3-mini-yoda-adapter \\\n",
        "        --outfile adapter.gguf \\\n",
        "        --outtype q8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13bdca4",
      "metadata": {
        "id": "f13bdca4"
      },
      "source": [
        "- `outtype`은 `f32`, `f16`, `bf16`, `q8_0`, `auto` 중 하나입니다. `auto`는 첫 번째로 로드된 텐서를 따라 가장 높은 정밀도의 16비트 부동 소수점 타입을 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f669cfb0",
      "metadata": {
        "id": "f669cfb0"
      },
      "source": [
        "#### 전체 모델 변환하기\n",
        "\n",
        "##### \"GGUF My Repo\" 사용하기\n",
        "\n",
        "https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/gguf_my_repo.png?raw=True)\n",
        "\n",
        "<center>그림 6.2 - 허깅 페이스의 \"GGUF My Repo\" 스페이스</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3fa6074",
      "metadata": {
        "id": "b3fa6074"
      },
      "source": [
        "##### Unsloth 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5aa020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6b5aa020",
        "outputId": "74a4564b-707c-4a0e-d7fd-ee55627ded94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.8.10-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting unsloth_zoo>=2025.8.9 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.8.9-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.47.0)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.55.2)\n",
            "Collecting datasets<4.0.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.10.0)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.21.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.34.4)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (0.21.4)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.9->unsloth) (0.10.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.8.9->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.9->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.8.9->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading protobuf-3.20.1-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.8.10-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.8.9-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.31-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shtab, protobuf, msgspec, tyro, xformers, datasets, cut_cross_entropy, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigtable 2.32.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-spanner 3.57.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.21.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-dataproc 5.21.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.25.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-secret-manager 2.24.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-trace 1.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-appengine-logging 1.6.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-speech 2.33.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.110.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-audit-log 0.3.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-logging 3.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.32.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cut_cross_entropy-25.1.1 datasets-3.6.0 msgspec-0.19.0 protobuf-3.20.1 shtab-1.7.2 tyro-0.9.31 unsloth-2025.8.10 unsloth_zoo-2025.8.9 xformers-0.0.32.post2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f1183b1252e8413a9a9f5003d65b8e4e",
              "pip_warning": {
                "packages": [
                  "datasets",
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xformers in /usr/local/lib/python3.12/dist-packages (0.0.32.post2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.21.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth protobuf==3.20.1\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16340ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "f9ae2b7a517f479f96350d56f04d2c90",
            "a0cfd8a5ca6a4cdcad4b3e3dc9d9bfb5",
            "4747ac4e52374d49a9b78c622c9c1d18",
            "4a7e73ea16e346eba8a4c945308056b0",
            "1c176c20f5094e22915474001aa25701",
            "aca0e545ff384edfa2fc8360007c398c",
            "e1e5af5d3161403c8e0d18b997239fc6",
            "5f3fb174bc064f4996216061f1ef72bf",
            "417facba98e04b94a3621eab9d62429e",
            "6e51ad89518a42c1ae65c74e03ffd551",
            "cb8aa93c8584498f8f0cc03932b3a99f",
            "be9d590722f247bba2e678fcf779e298",
            "453a1894c04742578d11eaea4c3afb01",
            "1e70eaeab9c248ba9f11e92522bb07ba",
            "ac5c748efc7a4cd9ab7bccfc0ade21ab",
            "41fff7de6ae34cd791ef33b404a07a2d",
            "d74ca809e318449584d95a7027f06759",
            "c8e165caddc34c26951639c593e9e0ac",
            "4ddeca5544614b088b0d47758aaa3d15",
            "1bb0a2c29ed044418988e32997a2779b",
            "212efff1b0ec440387fbd157513fbe39",
            "4faa978410854af1812b5e266a97aabd",
            "64dc2b3b68bb4a33a58e1aeef9c5fe82",
            "c10e18a920a7436dad482e47f1bdefac",
            "f004666ed0674e76948f8597c085bf0f",
            "f283b23cac1241799f761dc5f3c7aff3",
            "9307807036ce415ca4179d6f7940af8b",
            "61e48ff376544a9394aa9bfdd1eba3ae",
            "571198cb9eae4490bd696ed89db94602",
            "3d72dd3220b241cbb2e8b57a5a240060",
            "5abd390d20c44881b84d1e616081abdc",
            "f0b18f084de4424086437a9ad7f82c65",
            "34fad0ac7a984caaa98e93c682f3a28b",
            "d44e85c5328b473d90b2f800e86788a0",
            "07ee39d03a6a4a8e9f2174878103c349",
            "4d6b958dc8c4425589cf4cc89503f534",
            "1c7ee98614ee44a49f9ad5b7b3ee3fbb",
            "d50c7907303d44ce8cd0cbc5efbbbaa9",
            "a796c85bfb41429ea897e06c999a28b6",
            "cd053e1e33f24258953e38ad7938b7a1",
            "23aae1c44ca2472e929cf4e2064697db",
            "6ec7e49888824cba9e3c6d7f7768e8ad",
            "f7aa9c57764a4e68ba21039b8099a587",
            "bc16bc3056134d8f814f73065fecfccf",
            "9e7b31287f1d417786cc7fcedc1dd3e1",
            "f4d278349b6f4033b76675cf0b24eb66",
            "bdecdea0d8b54faeb168e0328b1ce33f",
            "5d63ffdbff67451c96d9f4906979f0df",
            "44d1e50acb09498798ec774fbecd2dfa",
            "5b5150990fda4156b1df72dbe71cf384",
            "ac20510029fd461ea78e96472950150b",
            "23a823be1fa84556a49653c272f9c398",
            "4c79bb8db3a34a1c99babe817152afc2",
            "c9f7eda98c264dceaf3186e9dbaf929c",
            "85d4a1e1255046cd921dc166f79638cf",
            "7c286aa7bc294386b0d6c0af5295ab18",
            "692f4d616f11477380d698dfec9d8831",
            "786348debc5e46c88e7b157b3aec8e5d",
            "44745a53d2584ef3994b4db06f35b7f1",
            "35b8b3dac8244b31aadd1854b320bfd5",
            "8d4f6ecd3ced49199454d8b488b54ede",
            "5dd36e935c2e4ee5b8a838e46b8fdd3f",
            "de13775ce4d14a2c9843bd464aabaf8f",
            "9fcb925a616142f9905bc47e7ea290c7",
            "1427b4b889114a5d8c279ebe80cb8ddb",
            "c57c6f940fab4731a9f25a1a2cb7dde0",
            "36ce3806a9154159886e284e1a1d8d0f",
            "f0682f8eb00344108f64ded5da3e7794",
            "7ac378f138db446f967cf587d1fa60c2",
            "d1e4d21d02b24bfe8203b4d0b7c8cb57",
            "6d667458c7a04d469d97b9130350498a",
            "7f1d001b1d7f4d138b343bbd986fc010",
            "d18dd16e20934948849ea26fe0b4b7b5",
            "cb9394b92e9a4208a2dde2d0cc4252b7",
            "be457e42759741d6b5f49a87b91001e2",
            "3afb44df108248348912a5d9a329f99a",
            "10196b57167e4adab1d0a8af8f77d840",
            "4f72ff80ce9b4acb9edbbe220041705a",
            "4f12569c426d409ba1fabd5f7a16423f",
            "85ebaee81b174e0db5d07bee70b94af1",
            "4097cb4d507742bdbfd4b974a8691e85",
            "f658407f90744046ad2c84d0a721806a",
            "8dc818e0a09544de991aa0d7f0f74652",
            "c786792a6e1641c698559e602a108646",
            "610a51b8378b4d22b3be0a33eddc7b63",
            "d4dd9d938b5145f4be125736cbed4f80",
            "4e41bb7ec91e48feb0fde275b33845c7",
            "05cc9b802b01488f944dbe53009cbe61"
          ]
        },
        "id": "d16340ec",
        "outputId": "f5f8e17c-2b9a-42de-8e59-3a72c80955f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.8.10: Fast Mistral patching. Transformers: 4.55.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9ae2b7a517f479f96350d56f04d2c90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be9d590722f247bba2e678fcf779e298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64dc2b3b68bb4a33a58e1aeef9c5fe82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d44e85c5328b473d90b2f800e86788a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e7b31287f1d417786cc7fcedc1dd3e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c286aa7bc294386b0d6c0af5295ab18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/458 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36ce3806a9154159886e284e1a1d8d0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f72ff80ce9b4acb9edbbe220041705a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.8.10 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained('dvgodoy/phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55572d7",
      "metadata": {
        "id": "f55572d7"
      },
      "source": [
        "```\n",
        "==((====))==  Unsloth 2024.10.0: Fast Mistral patching. Transformers = 4.44.2.\n",
        "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
        "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
        "\n",
        "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]\n",
        "\n",
        "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]\n",
        "\n",
        "tokenizer_config.json:   0%|          | 0.00/3.34k [00:00<?, ?B/s]\n",
        "\n",
        "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
        "\n",
        "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]\n",
        "\n",
        "special_tokens_map.json:   0%|          | 0.00/458 [00:00<?, ?B/s]\n",
        "\n",
        "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
        "\n",
        "adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]\n",
        "\n",
        "Unsloth 2024.10.0 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf0539a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf0539a",
        "outputId": "2793aad0-406e-4250-9891-60c2dccfecde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32064, 3072, padding_idx=32009)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "              (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "              (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f3fa7d",
      "metadata": {
        "id": "c3f3fa7d"
      },
      "source": [
        "```\n",
        "PeftModelForCausalLM(\n",
        "  (base_model): LoraModel(\n",
        "    (model): MistralForCausalLM(\n",
        "      (model): MistralModel(\n",
        "        (embed_tokens): Embedding(32064, 3072, padding_idx=32009)\n",
        "        (layers): ModuleList(\n",
        "          (0-31): 32 x MistralDecoderLayer(\n",
        "            (self_attn): MistralAttention(\n",
        "              (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "              (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "              (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "              (o_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Dropout(p=0.05, inplace=False)\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (rotary_emb): LlamaRotaryEmbedding()\n",
        "            )\n",
        "            (mlp): MistralMLP(\n",
        "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
        "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
        "              (down_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Dropout(p=0.05, inplace=False)\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
        "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
        "      )\n",
        "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
        "    )\n",
        "  )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdcfe3eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdcfe3eb",
        "outputId": "b0f013fe-8cde-4fae-deb8-b357f8c3ded8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 2.3G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.38 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:02<00:00, 15.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving gguf_model/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving gguf_model/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting mistral model. Can use fast conversion = True.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at gguf_model into f16 GGUF format.\n",
            "The output location will be /content/gguf_model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Extending gguf_model/tokenizer.model with added_tokens.json.\n",
            "Originally tokenizer.model is of size (32000).\n",
            "But we need to extend to sentencepiece vocab size (32011).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:hf-to-gguf:Loading model: gguf_model\n",
            "INFO:hf-to-gguf:Model architecture: MistralForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 32064}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {3072, 32064}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 4096\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
            "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 32000\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 32009\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.F16.gguf: n_tensors = 291, total_size = 7.6G\n",
            "Writing: 100%|██████████| 7.64G/7.64G [01:47<00:00, 71.2Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 6351 (5d804a49)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/gguf_model/unsloth.F16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from /content/gguf_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gguf_Model\n",
            "llama_model_loader: - kv   3:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 3.8B\n",
            "llama_model_loader: - kv   5:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [\"unsloth\", \"llama.cpp\"]\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 96\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 96\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32009\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "llama_model_quantize_impl: model size  =  7288.51 MB\n",
            "llama_model_quantize_impl: quant size  =  2210.78 MB\n",
            "\n",
            "main: quantize time = 448781.48 ms\n",
            "main:    total time = 448781.48 ms\n",
            "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# 이 명령은 여러 이유로 실패할 수 있으며 환경과 (실행 도중 설치된) llama.cpp의 안정성에 따라 다릅니다.\n",
        "\n",
        "# Unsloth가 직접 설치하도록 클론한 llama.cpp 폴더를 삭제합니다.\n",
        "!rm -rf llama.cpp/\n",
        "\n",
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927ce724",
      "metadata": {
        "id": "927ce724"
      },
      "source": [
        " ```\n",
        "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
        "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
        "To force `safe_serialization`, set it to `None` instead.\n",
        "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
        "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
        "Unsloth: Will remove a cached repo with size 2.3G\n",
        "\n",
        "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
        "Unsloth: Will use up to 5.02 out of 12.67 RAM for saving.\n",
        "Unsloth: Saving tokenizer... Done.\n",
        "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
        "Unsloth: Saving gguf_model/pytorch_model-00001-of-00002.bin...\n",
        "Unsloth: Saving gguf_model/pytorch_model-00002-of-00002.bin...\n",
        "Done.\n",
        "\n",
        "Unsloth: Converting mistral model. Can use fast conversion = True.\n",
        "\n",
        "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
        "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
        "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
        "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
        " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
        "\n",
        "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
        "\n",
        "Unsloth: Extending gguf_model/tokenizer.model with added_tokens.json.\n",
        "Originally tokenizer.model is of size (32000).\n",
        "But we need to extend to sentencepiece vocab size (32011).\n",
        "\n",
        "Unsloth: [1] Converting model at gguf_model into f16 GGUF format.\n",
        "The output location will be /content/gguf_model/unsloth.F16.gguf\n",
        "This will take 3 minutes...\n",
        "INFO:hf-to-gguf:Loading model: gguf_model\n",
        "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
        "INFO:hf-to-gguf:Exporting model...\n",
        "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
        "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
        "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 32064}\n",
        "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
        "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
        "...\n",
        "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
        "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
        "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
        "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {3072, 32064}\n",
        "INFO:hf-to-gguf:Set meta model\n",
        "INFO:hf-to-gguf:Set model parameters\n",
        "INFO:hf-to-gguf:gguf: context length = 4096\n",
        "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
        "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
        "INFO:hf-to-gguf:gguf: head count = 32\n",
        "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
        "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
        "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
        "INFO:hf-to-gguf:gguf: file type = 1\n",
        "INFO:hf-to-gguf:Set model tokenizer\n",
        "INFO:gguf.vocab:Setting special token type bos to 1\n",
        "INFO:gguf.vocab:Setting special token type eos to 32000\n",
        "INFO:gguf.vocab:Setting special token type unk to 0\n",
        "INFO:gguf.vocab:Setting special token type pad to 32009\n",
        "INFO:gguf.vocab:Setting add_bos_token to False\n",
        "INFO:gguf.vocab:Setting add_eos_token to False\n",
        "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
        "' + message['content'] + '<|end|>\n",
        "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
        "' + message['content'] + '<|end|>\n",
        "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
        "' + message['content'] + '<|end|>\n",
        "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
        "' }}{% else %}{{ eos_token }}{% endif %}\n",
        "INFO:hf-to-gguf:Set model quantization version\n",
        "INFO:gguf.gguf_writer:Writing the following files:\n",
        "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.F16.gguf: n_tensors = 291, total_size = 7.6G\n",
        "Writing: 100%|██████████| 7.64G/7.64G [01:56<00:00, 65.5Mbyte/s]\n",
        "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.F16.gguf\n",
        "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.F16.gguf\n",
        "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
        "main: build = 3934 (3752217e)\n",
        "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
        "main: quantizing '/content/gguf_model/unsloth.F16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
        "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from /content/gguf_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
        "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
        "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
        "llama_model_loader: - kv   1:                               general.type str              = model\n",
        "llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 4k Instruct Bnb 4bit\n",
        "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
        "llama_model_loader: - kv   4:                           general.finetune str              = 4k-instruct-bnb-4bit\n",
        "llama_model_loader: - kv   5:                           general.basename str              = phi-3\n",
        "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
        "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
        "llama_model_loader: - kv   8:                       llama.context_length u32              = 4096\n",
        "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
        "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
        "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
        "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
        "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
        "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
        "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 96\n",
        "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 96\n",
        "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
        "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32064\n",
        "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 96\n",
        "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
        "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
        "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
        "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
        "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
        "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
        "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
        "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 32000\n",
        "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
        "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32009\n",
        "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = false\n",
        "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
        "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
        "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
        "llama_model_loader: - type  f32:   65 tensors\n",
        "llama_model_loader: - type  f16:  226 tensors\n",
        "[   1/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
        "[   2/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
        "...\n",
        "[ 290/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
        "[ 291/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
        "llama_model_quantize_internal: model size  =  7288.51 MB\n",
        "llama_model_quantize_internal: quant size  =  2210.78 MB\n",
        "\n",
        "main: quantize time = 426187.37 ms\n",
        "main:    total time = 426187.37 ms\n",
        "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94ddbf5",
      "metadata": {
        "id": "b94ddbf5"
      },
      "source": [
        "##### 도커 이미지 사용하기\n",
        "\n",
        "모델을 변환하려면 다음 명령을 실행합니다.\n",
        "\n",
        "```\n",
        "docker run --rm\n",
        "           -v \"/path/to/saved_model\":/repo\n",
        "           ghcr.io/ggerganov/llama.cpp:full\n",
        "           --convert \"/repo\"\n",
        "           --outtype f32\n",
        "           --outfile /repo/gguf-model-f32.gguf\n",
        "```\n",
        "\n",
        "1. `--rm`: 실행이 완료된 후 컨테이너를 자동으로 삭제합니다. 이 예제와 같이 스크립트를 한 번 실행하는 경우에 유용합니다.\n",
        "2. `-v [local path]:[path inside container]`: 컴퓨터에 있는 폴더를 컨테이너 내부 폴더에 매핑합니다. 이를 통해 컨테이너는 내부에 있는 폴더처럼 로컬 폴더를 참조할 수 있습니다.\n",
        "3. `[docker image]`: llama.cpp의 도커 이미지 ghcr.io/ggerganov/llama.cpp:full를 사용합니다.\n",
        "4. `--convert [path inside container]`: 이것이 실행할 명령입니다. 이는 도커 명령이 아니라 여기서 사용하는 특정 이미지에 있는 명령입니다.\n",
        "5. `--outtype [GGUF type]`: --convert 명령의 매개변수로 GGUF 파일의 데이터 타입을 지정합니다.\n",
        "6. `--outfile [GGUF filename]`: --convert 명령의 또 다른 매개변수입니다. GGUF 파일의 이름을 지정합니다(컨테이너 내부 경로 /repo를 지정했습니다. 이 경로는 로컬 컴퓨터에 있는 폴더에 매핑되어 있으므로 로컬 폴더에 파일이 생성됩니다).\n",
        "\n",
        "변환된 모델을 양자화하려면 다음 명령을 실행해야 합니다.\n",
        "\n",
        "```\n",
        "docker run --rm\n",
        "           -v \"/path/to/saved_model\":/repo\n",
        "           ghcr.io/ggerganov/llama.cpp:full\n",
        "           --quantize \"/repo/gguf-model-f32.gguf\"\n",
        "           \"/repo/gguf-model-Q4_K_M.gguf\"\n",
        "           \"Q4_K_M\"\n",
        "```\n",
        "\n",
        "7. `--quantize [GGUF filename]`: 실행할 새로운 명령으로 이 특정 이미지에서만 있는 명령입니다. 어떤 GGUF 파일을 양자화할지 지정합니다(일반적으로 convert 명령의 outfile).\n",
        "8. `[quantized GGFUF filename]`: 스크립트가 실행 완료된 후 양자화된 파일 이름. 로컬 폴더에 접근할 수 있는 매핑된 폴더를 지정하세요.\n",
        "9. `[quantization type]`: 전체 양자화 타입 목록은 [온라인 문서](https://github.com/ggerganov/llama.cpp/blob/main/examples/quantize/README.md)를 참고하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94d1a2b",
      "metadata": {
        "id": "f94d1a2b"
      },
      "source": [
        "##### llama.cpp 빌드하기\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!pip install llama.cpp/gguf-py\n",
        "!pip install -r llama.cpp/requirements.txt\n",
        "```\n",
        "\n",
        "```python\n",
        "!python ./llama.cpp/convert_hf_to_gguf.py /path/to/saved_model --outtype f16\n",
        "```\n",
        "\n",
        "```python\n",
        "!cd llama.cpp && make clean && make\n",
        "```\n",
        "\n",
        "```python\n",
        "!./llama.cpp/quantize\n",
        "    ./path/to/saved_model/ggml-model-f16.gguf\n",
        "    ./path/to/saved_model/ggml-model-q4_0.gguf\n",
        "    q4_0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e120bbcd",
      "metadata": {
        "id": "e120bbcd"
      },
      "source": [
        "### 모델 서빙하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f76ca5a",
      "metadata": {
        "id": "0f76ca5a"
      },
      "source": [
        "#### Ollama\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/ollama.png?raw=True)\n",
        "<center>Figure 6.3 - Screenshot of Ollama’s page</center>\n",
        "\n",
        "```\n",
        "ollama run phi3:mini\n",
        "```\n",
        "\n",
        "##### Ollama 설치하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49c9062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49c9062",
        "outputId": "c47b5e69-85ba-4b6e-f44e-62ef61a05b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.9.6 sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1574e2e2",
      "metadata": {
        "id": "1574e2e2"
      },
      "source": [
        "```\n",
        ">>> Installing ollama to /usr/local/bin...\n",
        ">>> Creating ollama user...\n",
        ">>> Adding ollama user to video group...\n",
        ">>> Adding current user to ollama group...\n",
        ">>> Creating ollama systemd service...\n",
        "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
        ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
        ">>> Install complete. Run \"ollama\" from the command line.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc9f0b5",
      "metadata": {
        "id": "dcc9f0b5"
      },
      "source": [
        "##### 코랩에서 Ollama 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853f2ed6",
      "metadata": {
        "id": "853f2ed6"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab를 참고함\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import threading\n",
        "\n",
        "# 노트: 실행 중인 백엔드에 따라 이 설정을 하고 cuda를 활성화해야 할 수 있습니다.\n",
        "# NVIDIA 라이브러리를 위해 환경 변수를 설정합니다.\n",
        "# CUDA를 위해 환경 변수를 설정합니다.\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# LD_LIBRARY_PATH에 /usr/lib64-nvidia와 CUDA lib 디렉토리가 포함되도록 설정합니다.\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # 비동기 함수를 정의합니다.\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # 호출\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75df9604",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75df9604",
        "outputId": "f02f546e-2c38-4eae-d281-c22302422e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting ollama serve\n"
          ]
        }
      ],
      "source": [
        "# 새 스레드에서 실행될 이벤트 루프를 만듭니다.\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# 셀 때문에 실행이 중지되지 않도록 별도의 스레드에서 ollama serve를 시작합니다.\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee13ef1c",
      "metadata": {
        "id": "ee13ef1c"
      },
      "source": [
        "##### 모델 파일\n",
        "\n",
        "| 명령 |\t설명 |\n",
        "|---|---|\n",
        "|FROM (필수) | 사용할 베이스 모델을 정의합니다. |\n",
        "|PARAMETER | Ollama가 모델을 어떻게 실행할지에 대한 매개변수를 설정합니다. |\n",
        "|TEMPLATE | 모델에 전달할 완전한 프롬프트 템플릿 |\n",
        "|SYSTEM | 템플릿에 포함할 시스템 메시지를 지정합니다. |\n",
        "|ADAPTER | 모델에 적용할 (Q)LoRA 어댑터를 정의합니다. |\n",
        "|LICENSE | 법적 라이센스를 지정합니다. |\n",
        "|MESSAGE | 메시지 기록을 지정합니다. |\n",
        "\n",
        "```\n",
        "ollama show --modelfile phi3:mini\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# Modelfile generated by \"ollama show\"\n",
        "# To build a new Modelfile based on this, replace FROM with:\n",
        "# FROM phi3:mini\n",
        "\n",
        "FROM /usr/share/ollama/.ollama/models/blobs/sha256-633fc...\n",
        "TEMPLATE \"{{ if .System }}<|system|>\n",
        "{{ .System }}<|end|>\n",
        "{{ end }}{{ if .Prompt }}<|user|>\n",
        "{{ .Prompt }}<|end|>\n",
        "{{ end }}<|assistant|>\n",
        "{{ .Response }}<|end|>\"\n",
        "PARAMETER stop <|end|>\n",
        "PARAMETER stop <|user|>\n",
        "PARAMETER stop <|assistant|>\n",
        "LICENSE \"\"\"Microsoft.\n",
        "Copyright (c) Microsoft Corporation.\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03abae0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "d9bd247f86304c87992bcfcc73e0850c",
            "036a9f2d12c24b3ab73c714e81424ff1",
            "0ebd062f75ac415690b4f63753e63e92",
            "73817049800f4d55ba5dd59e95cbfc2b",
            "87c24f8193fe447e8b27d13d763c2d84",
            "acd14c1d44ff488ebfc55690f096f453",
            "872166b5dafa4d889b012733bee9cef3",
            "3019c5a339ae47b485098440295b4700",
            "4ad6d9b6dc6344269eeb6e8b42f71f71",
            "5db1a98dd4f64b9db2dadea428c54fd9",
            "56cb456a16884d3cb4f47d70185d0215",
            "647c4f0960a742e4a1db5201184f70cf",
            "7efedc977f8043b2956709d580aaef68",
            "8aa0a0414ff6480e87141c7c06d25189",
            "25633961fca442429821e3885efb17ff",
            "b13c5fd3438d4ae3a57cfed19921caa1",
            "2c3d26cf24ca41ea804ccdf7f3416cc5",
            "437f3b5a86f24b7a91dcac1a8f1e5694",
            "81f08cbfe22b4c1ea663217006aac4de",
            "e0b57c8113184e2f9826f12b466c17c2",
            "8f1b76da625b403f85d7a41ea4e90d08",
            "94704a3b4ae744a7a3aeca0209ef7c00",
            "915686a01a474003aab7213f942a6937",
            "4c61697daf4f416e9afa946643eeca00",
            "bfbe6ff5732d4473936871aa4fa98961",
            "50e79f10e21a48a2b8ebf3be734d5256",
            "bef5f6c0517f413a8409c9a0e1469ae3",
            "32b7b2c3304b405ca7919a477a372621",
            "1a5b65266cdb4504bffae90ed060928f",
            "e89def30d8de4b5ba4eb5994bd73dfa2",
            "4896788a646e470e80d97c2f78072094",
            "87528227b8b04c59bb383cf2d2d1a0d3",
            "e8dff7c03e574fd585119568a52b6435",
            "7b1f93de1f814297aa444a0a1cc77fc8",
            "bda0a5a6d7db42afbd70de3c47873fb3",
            "bb56d7a5c3e847d49e678c47e7b579f1",
            "231f017461774072843467cb83f63954",
            "00ecdcbf6b6f4705bdf106e65fa6828f",
            "69564e629a4a43028d21d41a9723feb0",
            "85f2b1f8ae9e4505a8aa797b183527be",
            "a5332878cf0c4a80989a39f6f17a3666",
            "f66bc8076f3f42ffb1c40ec4d93ad952",
            "6564f20e81cc4f4a922127fb215f3333",
            "a028acab9e924174b3af15aa36cb9cd5",
            "4c8f892f88c242c5b1fb37ee128624d4",
            "f9fd2c3a583d40e1aa93f9243a56e956",
            "59efaa49fb964abda787174ffc252ffd",
            "92b4532c9a4744b8a22ab383243a7c46",
            "c8e279c77f2146cba5d592c87bc94c35",
            "2ebded28842c4857be4c5275ee805aaa",
            "e32ec53669b140b181ca8ed8819ab302",
            "8cd2f60f12ef4012935204e4830399e9",
            "71e4d38719cc4908b0fe15d545153036",
            "e5b8140fd6684ceb9732113112f9c659",
            "dd2029f60e594f9793b5e5a028413651"
          ]
        },
        "id": "c03abae0",
        "outputId": "d39582d9-7f7b-4b91-eb64-5b3112bfdf0c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9bd247f86304c87992bcfcc73e0850c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "647c4f0960a742e4a1db5201184f70cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "915686a01a474003aab7213f942a6937",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b1f93de1f814297aa444a0a1cc77fc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c8f892f88c242c5b1fb37ee128624d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer_phi3 = AutoTokenizer.from_pretrained('microsoft/phi-3-mini-4k-instruct')\n",
        "print(tokenizer_phi3.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4012459",
      "metadata": {
        "id": "e4012459"
      },
      "source": [
        "##### 모델 임포트하기\n",
        "\n",
        "###### 사용자 정의 (전체) 모델 파일\n",
        "\n",
        "```python\n",
        "modelfile = \"\"\"\n",
        "FROM ./phi3-full-model\n",
        "TEMPLATE \"{{ if .System }}<|system|>\n",
        "{{ .System }}<|end|>\n",
        "{{ end }}{{ if .Prompt }}<|user|>\n",
        "{{ .Prompt }}<|end|>\n",
        "{{ end }}<|assistant|>\n",
        "{{ .Response }}<|end|>\"\n",
        "PARAMETER stop <|end|>\n",
        "PARAMETER stop <|user|>\n",
        "PARAMETER stop <|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "with open('phi3-full-modelfile', 'w') as f:\n",
        "    f.write(modelfile)\n",
        "```\n",
        "\n",
        "```\n",
        "!ollama create our_own_phi3 -f phi3-full-modelfile\n",
        "```\n",
        "\n",
        "```\n",
        "!ollama list\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a52507",
      "metadata": {
        "id": "62a52507"
      },
      "source": [
        "###### 사용자 정의 어댑터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ec65a7",
      "metadata": {
        "id": "80ec65a7"
      },
      "outputs": [],
      "source": [
        "adapterfile = \"\"\"\n",
        "FROM phi3:mini\n",
        "ADAPTER ./adapter.gguf\n",
        "TEMPLATE \"{{ if .System }}<|system|>\n",
        "{{ .System }}<|end|>\n",
        "{{ end }}{{ if .Prompt }}<|user|>\n",
        "{{ .Prompt }}<|end|>\n",
        "{{ end }}<|assistant|>\n",
        "{{ .Response }}<|end|>\"\n",
        "PARAMETER stop <|end|>\n",
        "PARAMETER stop <|user|>\n",
        "PARAMETER stop <|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "with open('phi3-adapter-file', 'w') as f:\n",
        "    f.write(adapterfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b5fb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62b5fb88",
        "outputId": "6cfa2811-9686-4a3f-fc9e-1454b3005709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GIN] 2025/09/02 - 03:25:17 | 200 |     123.538µs |       127.0.0.1 | HEAD     \"/\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l[GIN] 2025/09/02 - 03:25:18 | 201 |  420.460532ms |       127.0.0.1 | POST     \"/api/blobs/sha256:1e0d1652754702c76c0dce6c1f9ad17ca57783c5587ba428116f7666175e5405\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-09-02T03:25:19.691Z level=INFO source=download.go:177 msg=\"downloading 633fc5be925f in 16 136 MB part(s)\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-09-02T03:25:43.331Z level=INFO source=download.go:177 msg=\"downloading fa8235e5b48f in 1 1.1 KB part(s)\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-09-02T03:25:44.931Z level=INFO source=download.go:177 msg=\"downloading 542b217f179c in 1 148 B part(s)\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-09-02T03:25:46.499Z level=INFO source=download.go:177 msg=\"downloading 8dde1baf1db0 in 1 78 B part(s)\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-09-02T03:25:48.078Z level=INFO source=download.go:177 msg=\"downloading 23291dc44752 in 1 483 B part(s)\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l[GIN] 2025/09/02 - 03:25:58 | 200 | 39.748413286s |       127.0.0.1 | POST     \"/api/create\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!ollama create our_own_phi3_adapted -f phi3-adapter-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336581b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "336581b7",
        "outputId": "6e86e09a-c595-4763-c928-c45bdb11ad58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GIN] 2025/09/02 - 03:25:58 | 200 |      37.413µs |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2025/09/02 - 03:25:58 | 200 |     826.923µs |       127.0.0.1 | GET      \"/api/tags\"\n",
            "NAME                           ID              SIZE      MODIFIED               \n",
            "our_own_phi3_adapted:latest    4aa0c981ee99    2.2 GB    Less than a second ago    \n",
            "phi3:mini                      4f2222927938    2.2 GB    Less than a second ago    \n"
          ]
        }
      ],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a7bf67",
      "metadata": {
        "id": "79a7bf67"
      },
      "source": [
        "##### 모델에 쿼리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42618c42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42618c42",
        "outputId": "f1a34554-fa28-4f4a-c3cb-3618c4d19800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea0b225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ea0b225",
        "outputId": "28f79c38-a678-478b-c8af-5ea83ead85c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time=2025-09-02T03:26:05.882Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf gpu=GPU-8caa9397-9fac-1744-bf08-28392bfdf945 parallel=2 available=13180731392 required=\"6.1 GiB\"\n",
            "time=2025-09-02T03:26:06.146Z level=INFO source=server.go:135 msg=\"system memory\" total=\"12.7 GiB\" free=\"6.7 GiB\" free_swap=\"0 B\"\n",
            "time=2025-09-02T03:26:06.147Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[12.3 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.1 GiB\" memory.required.partial=\"6.1 GiB\" memory.required.kv=\"3.0 GiB\" memory.required.allocations=\"[6.1 GiB]\" memory.weights.total=\"2.0 GiB\" memory.weights.repeating=\"1.9 GiB\" memory.weights.nonrepeating=\"77.1 MiB\" memory.graph.full=\"512.0 MiB\" memory.graph.partial=\"512.0 MiB\"\n",
            "llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /root/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Phi-3\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
            "llama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
            "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q4_0:  129 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 2.03 GiB (4.55 BPW)\n",
            "load: special tokens cache size = 14\n",
            "load: token to piece cache size = 0.1685 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 1\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi 3 Mini 128k Instruct\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "time=2025-09-02T03:26:06.376Z level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /root/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --lora /root/.ollama/models/blobs/sha256-1e0d1652754702c76c0dce6c1f9ad17ca57783c5587ba428116f7666175e5405 --threads 1 --parallel 2 --port 43177\"\n",
            "time=2025-09-02T03:26:06.397Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\n",
            "time=2025-09-02T03:26:06.397Z level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-09-02T03:26:06.397Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
            "time=2025-09-02T03:26:06.442Z level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
            "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
            "time=2025-09-02T03:26:07.324Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
            "time=2025-09-02T03:26:07.348Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:43177\"\n",
            "time=2025-09-02T03:26:07.402Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 12570 MiB free\n",
            "llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /root/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Phi-3\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
            "llama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
            "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q4_0:  129 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 2.03 GiB (4.55 BPW)\n",
            "load: special tokens cache size = 14\n",
            "load: token to piece cache size = 0.1685 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 262144\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi 3 Mini 128k Instruct\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors:    CUDA_Host model buffer size =    52.84 MiB\n",
            "load_tensors:        CUDA0 model buffer size =  2021.82 MiB\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 2\n",
            "llama_context: n_ctx         = 8192\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 1024\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context:  CUDA_Host  output buffer size =     0.27 MiB\n",
            "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  3072.00 MiB\n",
            "llama_kv_cache_unified: KV self size  = 3072.00 MiB, K (f16): 1536.00 MiB, V (f16): 1536.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   564.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    22.01 MiB\n",
            "llama_context: graph nodes  = 1350\n",
            "llama_context: graph splits = 2\n",
            "llama_adapter_lora_init_impl: loading lora adapter from '/root/.ollama/models/blobs/sha256-1e0d1652754702c76c0dce6c1f9ad17ca57783c5587ba428116f7666175e5405' ...\n",
            "llama_adapter_lora_init_impl:      CUDA0 LoRA buffer size =    48.00 MiB\n",
            "llama_adapter_lora_init_impl: loaded 256 tensors from lora file\n",
            "time=2025-09-02T03:26:13.711Z level=INFO source=server.go:637 msg=\"llama runner started in 7.31 seconds\"\n",
            "[GIN] 2025/09/02 - 03:26:15 | 200 |  9.819489152s |       127.0.0.1 | POST     \"/api/generate\"\n",
            "model='our_own_phi3_adapted' created_at='2025-09-02T03:26:15.242913467Z' done=True done_reason='stop' total_duration=9819110416 load_duration=8287550431 prompt_eval_count=17 prompt_eval_duration=1093952453 eval_count=16 eval_duration=436172941 response='In this one, the Force is strong. Yes, hrrrm.' thinking=None context=[32010, 29871, 13, 1576, 11004, 338, 4549, 297, 445, 697, 29991, 32007, 29871, 13, 32001, 29871, 13, 797, 445, 697, 29892, 278, 11004, 338, 4549, 29889, 3869, 29892, 298, 21478, 1758, 29889]\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "prompt = \"The Force is strong in this one!\"\n",
        "response = ollama.generate(model='our_own_phi3_adapted',\n",
        "                           prompt=prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5189b804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5189b804",
        "outputId": "8f1c119f-314b-4522-a0e0-edb7e086d196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In this one, the Force is strong. Yes, hrrrm.\n"
          ]
        }
      ],
      "source": [
        "print(response['response'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e35608",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62e35608",
        "outputId": "734f297a-0013-46d2-e88f-a0875ab7cf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "The Force is strong in this one!<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "[GIN] 2025/09/02 - 03:26:15 | 200 |   394.70269ms |       127.0.0.1 | POST     \"/api/generate\"\n",
            "model='our_own_phi3_adapted' created_at='2025-09-02T03:26:15.668060038Z' done=True done_reason='stop' total_duration=394641382 load_duration=78638877 prompt_eval_count=17 prompt_eval_duration=14965866 eval_count=16 eval_duration=300399596 response='In this one, the Force is strong. Yes, hrrrm.' thinking=None context=None\n"
          ]
        }
      ],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "formatted = tokenizer_phi3.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(formatted)\n",
        "\n",
        "response = ollama.generate(model='our_own_phi3_adapted',\n",
        "                           prompt=formatted,\n",
        "                           raw=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07fb076",
      "metadata": {
        "id": "d07fb076"
      },
      "source": [
        "#### Llama.cpp\n",
        "\n",
        "변환, 양자화, 서빙에 사용할 수 있는 완전한 도커 이미지를 사용할 수 있습니다.\n",
        "\n",
        "```\n",
        "docker run -v \"/path/to/saved_model\":/model  \\\n",
        "           -p 8080:8000 \\\n",
        "           ghcr.io/ggerganov/llama.cpp:full \\\n",
        "           --server \\\n",
        "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
        "           --port 8000 \\\n",
        "           --host 0.0.0.0\n",
        "```\n",
        "\n",
        "1. `-v [local path]:[path inside container]`: 컴퓨터에 있는 폴더를 컨테이너 내부 폴더에 매핑합니다. 컨테이너가 로컬 폴더를 컨테이너 내부에 있는 것처럼 접근할 수 있습니다.\n",
        "2. `-p [host port]:[container port]`: 호스트 포트(예를 들면, 8080)로 전달된 요청을 컨테이너 내부 포트(예를 들면, 8000)으로 전달합니다.\n",
        "3. `[docker image]`: 사용할 llama.cpp의 도커 이미지. 여기서는 ghcr.io/ggerganov/llama.cpp:full\n",
        "4. `--server`: 실행할 명령. 도커 명령이 아니라 이 특정 이미지에서 제공하는 명령입니다.\n",
        "5. `-m /model/[quantized_qguf_file].qguf`: 서빙할 모델\n",
        "6. `--port [container port]`: 모델을 서빙하기 위해 사용할 컨테이너 내부 포트. 두 번째 인자에 지정된 container port와 같아야 합니다.\n",
        "7. `--host [ip address]`: 모델 서빙에 사용할 로컬 IP 주소\n",
        "\n",
        "도커를 사용해 변환하거나 양자화하는데 관심이 없다면 특별히 서빙을 위해 만들어진 작은 도커 이미지를 선택할 수 있습니다.\n",
        "\n",
        "```\n",
        "docker run -v \"path/to/saved_model\":/model \\\n",
        "           -p 8080:8000 \\\n",
        "           ghcr.io/ggerganov/llama.cpp:server \\\n",
        "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
        "           --port 8000 \\\n",
        "           --host 0.0.0.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79666d36",
      "metadata": {
        "id": "79666d36"
      },
      "source": [
        "##### 웹 인터페이스\u001f\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_ui.png?raw=True)\n",
        "<center>그림 6.4 llama.cpp 웹 UI</center>\n",
        "\n",
        "오른쪽 위에 있는 설정 버튼은 온도를 포함하여 다양한 파라미터를 제공합니다.\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_settings.png?raw=True)\n",
        "<center>그림 6.5 llama.cpp 설정</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be242056",
      "metadata": {
        "id": "be242056"
      },
      "source": [
        "##### REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ee1432",
      "metadata": {
        "id": "92ee1432"
      },
      "source": [
        "```python\n",
        "url = 'http://0.0.0.0:8080/completion'\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "data = {'prompt': 'There is bacon in this sandwich.',\n",
        "        'n_predict': 128}\n",
        "\n",
        "response = requests.post(url, json=data, headers=headers)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4e2819",
      "metadata": {
        "id": "aa4e2819",
        "scrolled": true
      },
      "source": [
        "```python\n",
        "print(response.json()['content'])\n",
        "```\n",
        "\n",
        "```\n",
        " There is no bacon in this sandwich. This statement is a paradox because it contradicts itself, yet it seems to suggest that the sandwich has both bacon and no bacon at the same time.\n",
        "\n",
        "2. This statement is also a paradox, as it claims that it is a lie that it is lying. If the statement is true, then it is indeed a lie, making it false. But if it is false, then it is not a lie, making it true. This creates a circular reasoning that can't be resolved.\n",
        "\n",
        "3. This statement is a paradox\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb36d0e",
      "metadata": {
        "id": "fbb36d0e"
      },
      "source": [
        "### 감사합니다!\n",
        "\n",
        "제안 사항이나 오류를 발견하면 주저하지 말고 [깃허브](https://github.com/dvgodoy), [X](https://x.com/dvgodoy), [BlueSky](https://bsky.app/profile/dvgodoy.bsky.social), 또는 [링크드인](https://www.linkedin.com/in/dvgodoy/)으로 연락주세요.\n",
        "\n",
        "새로운 책 출시, 업데이트, 할인에 대한 알림을 받고 싶다면 검로드(Gumroad)에서 저를 팔로우하세요.\n",
        "\n",
        "<center><a href=\"https://danielgodoy.gumroad.com/subscribe\">https://danielgodoy.gumroad.com/subscribe</a></center>\n",
        "\n",
        "여러분의 의견을 기다리겠습니다!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}