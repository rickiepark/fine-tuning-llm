{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d578ad67",
   "metadata": {
    "id": "5dd522b0"
   },
   "source": [
    "## Chapter 3: Low-Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090ae39",
   "metadata": {
    "id": "bdd3396a"
   },
   "source": [
    "### Spoilers\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Understand what a low-rank adapter is and why itâ€™s useful\n",
    "- Prepare the quantized model for training\n",
    "- Use `peft` to create and attach adapters to a base model\n",
    "- Discuss configuration options for targeting layers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b9edf",
   "metadata": {
    "id": "001b25f1"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9b772",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acc2f40c",
    "outputId": "95ec9531-5d6a-4437-a0ea-d54f1b0f7983"
   },
   "outputs": [],
   "source": [
    "# If you're running on Colab\n",
    "!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bf629",
   "metadata": {
    "id": "ca2f769f"
   },
   "outputs": [],
   "source": [
    "# If you're running on runpod.io's Jupyter Template\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5859",
   "metadata": {
    "id": "1d2630dc"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093a0aa",
   "metadata": {
    "id": "556455b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import matrix_rank\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573c625",
   "metadata": {
    "id": "8836cf4f"
   },
   "source": [
    "### The Goal\n",
    "\n",
    "We attach adapters to the huge linear layers in an LLM to drastically reduce the number of trainable parameters. We can easily shrink the number of trainable parameters down to less than 1% of their original number. By reducing both computation (fewer gradients to compute) and memory footprint (fewer parameters tracked by the optimizer), we achieve significant efficiency gains. Keep in mind, however, that low-rank adapters are unlikely to match the performance of full-model tuning, and their effectiveness may vary depending on the base model and the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d220e57",
   "metadata": {
    "id": "31ecd6d8"
   },
   "source": [
    "### Pre-Reqs\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/matmul.png?raw=True)\n",
    "<center>Figure 3.1 - Matrix multiplication</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ad170",
   "metadata": {
    "id": "461eaab6"
   },
   "source": [
    "### Low-Rank Adaptation in a Nutshell\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/two_matrices.png?raw=True)\n",
    "<center>Figure 3.2 - Multiplying two low-rank matrices</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59057a29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e05c5b83",
    "outputId": "2633033f-11fd-4f9b-ddf8-b2aac4f2cc34"
   },
   "outputs": [],
   "source": [
    "base_layer = nn.Linear(1024, 1024, bias=False)\n",
    "base_layer.weight.shape, base_layer.weight.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bcba3",
   "metadata": {
    "id": "55651493"
   },
   "source": [
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/lowrank_matrices.png?raw=True)\n",
    "<center>Figure 3.3 - Frozen weights and low-rank matrices</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cb90b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0305a2fc",
    "outputId": "79f3f623-3194-4d87-8f8b-5006f05544b7"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "r = 8\n",
    "layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
    "layer_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
    "layer_A, layer_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f95a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e716c5f3",
    "outputId": "dd308b37-4b4b-441e-e290-f2f23196b7d8"
   },
   "outputs": [],
   "source": [
    "layer_A.weight.numel(), layer_B.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10aee54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da2e401c",
    "outputId": "d7f7a5c6-5ce9-4081-e245-3e6a9dba5e2a"
   },
   "outputs": [],
   "source": [
    "composite = layer_B.weight @ layer_A.weight\n",
    "composite.shape, composite.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43acb37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b25358bc",
    "outputId": "62f80f93-d058-4afa-8757-8d5d32efbe92"
   },
   "outputs": [],
   "source": [
    "matrix_rank(composite.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63efc8",
   "metadata": {
    "id": "952840a7"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = X @ (W + B @ A)^T\n",
    "$$\n",
    "<center>Equation 3.1 - Adding the resulting product to the weights</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9e8e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ea66ae2",
    "outputId": "2107be40-6940-4bdc-e06d-003fa4d63e8d"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(19)\n",
    "batch = torch.randn(1, 1024)\n",
    "\n",
    "batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753fad4",
   "metadata": {
    "id": "4575839d"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n",
    "$$\n",
    "<center>Equation 3.2 - Using two forward passes</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/forward.png?raw=True)\n",
    "<center>Figure 3.4 - Using two forward passes</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13473a5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8e9b9c3",
    "outputId": "900dca7e-4313-4056-9ebc-19fc846f86c3"
   },
   "outputs": [],
   "source": [
    "regular_output = batch @ base_layer.weight.data.T\n",
    "additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n",
    "regular_output, additional_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894d27d",
   "metadata": {
    "id": "547f73e0"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n",
    "$$\n",
    "<center>Equation 3.3 - Chaining the adapterâ€™s forward passes</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf4baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4e9cd6e",
    "outputId": "1f6f2148-45cc-4c56-a9cb-35447d5c3c9d"
   },
   "outputs": [],
   "source": [
    "out_A = (batch @ layer_A.weight.T)\n",
    "additional_output = out_A @ layer_B.weight.T\n",
    "additional_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fe68e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "267a2e77",
    "outputId": "8568a39b-f9a8-459e-cc1e-bcde1809d7db"
   },
   "outputs": [],
   "source": [
    "regular_output = base_layer(batch)\n",
    "out_A = layer_A(batch)\n",
    "additional_output = layer_B(out_A)\n",
    "output = regular_output + additional_output\n",
    "regular_output, additional_output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c6072",
   "metadata": {
    "id": "9ea74713"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n",
    "$$\n",
    "<center>Equation 3.4 - LoRAâ€™s alpha</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2*r\n",
    "output = regular_output + (alpha / r) * additional_output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab264b",
   "metadata": {
    "id": "55483043"
   },
   "source": [
    "### The Road So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e5d2e-8b15-43e2-879d-e4481f7fa7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='cuda:0',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d8255",
   "metadata": {
    "id": "85a9072b"
   },
   "source": [
    "### Parameter Types and Gradients\n",
    "\n",
    "****\n",
    "**Summary of \"Parameter Types and Gradients\"**\n",
    "- quantization only freezes the linear layers that have been quantized\n",
    "- after quantization, a model can be prepared using the `prepare_model_for_kbit_training()` function\n",
    "  - it freezes **all** layers\n",
    "  - it casts every non-quantized 16-bit layer to FP32 to improve training\n",
    "  - it enables gradient checkpointing\n",
    "- you'll be able to unfreeze layers of your choice later on using the LoRA configuration\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ca55c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49a9abe7",
    "outputId": "503bdb8d-ce79-463e-d439-97a63d644207",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainable_parms(model):\n",
    "    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n",
    "    return parms\n",
    "\n",
    "trainable_parms(model_q4.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c94df",
   "metadata": {
    "id": "2132acb9"
   },
   "source": [
    "#### `prepare_model_for_kbit_training()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8093b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3455ebde",
    "outputId": "fcc291e6-2b5b-4e24-c939-52c1252e9d3d"
   },
   "outputs": [],
   "source": [
    "prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "                                        use_gradient_checkpointing=True,\n",
    "                                        gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709936d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dbad9bb",
    "outputId": "0a870055-9f21-4e7e-ff2b-337b70467636",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainable_parms(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277e665",
   "metadata": {
    "id": "9e8be25c"
   },
   "outputs": [],
   "source": [
    "def parms_of_dtype(model, dtype=torch.float32):\n",
    "    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n",
    "    return parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43586616",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "653fa73d",
    "outputId": "b00392b8-4aa3-4408-8b61-a2108d1468cf"
   },
   "outputs": [],
   "source": [
    "parms_of_dtype(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f83018",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025520ae",
    "outputId": "1b67511e-7205-4755-f4dc-9786ec9aeec3"
   },
   "outputs": [],
   "source": [
    "prepared_model.get_memory_footprint()/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b2537",
   "metadata": {
    "id": "00981764"
   },
   "source": [
    "### PEFT\n",
    "\n",
    "\"_ðŸ¤— PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware._\n",
    "\n",
    "_PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference._\"\n",
    "\n",
    "****\n",
    "**Summary of \"PEFT\"**\n",
    "- the basic configuration below should work well in many cases\n",
    "```python\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "- ranks of 8, 16, or 32 are typical, but using higher values shouldnâ€™t significantly impact the modelâ€™s memory footprint.\n",
    "- the scaling factor, `lora_alpha` is typically twice the rank.\n",
    "- if your model has `Conv1D` layers, add `fan_in_fan_out=True` to your configuration\n",
    "- if your model was recently released, you may need to specify the `target_modules` manually\n",
    "  - typically, use the names of the massive linear layers in the attention module.\n",
    "- by default, only the adapters are trainable\n",
    "  - if you'd like to train other layers, such as layer norms, add them to the `modules_to_save` argument\n",
    "  - if you're adding your own tokens to the tokenizer, you'll need to also train vocabulary-related layers such as embeddings and the model's head\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e78bf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be9679cd",
    "outputId": "ed7e7b1d-bfad-42e1-b138-7f65a6c7bf89"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig()\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3647c9d",
   "metadata": {
    "id": "13211b1e"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471b64b",
   "metadata": {
    "id": "1c79b1ea"
   },
   "source": [
    "### `target_modules`\n",
    "\n",
    "Since there are new models and architectures being released on a weekly basis, chances are that there is no preconfigured list of target layers in your currently installed version of the PEFT library. In this case, youâ€™ll be greeted with the following error:\n",
    "\n",
    "***\n",
    "`ValueError: Please specify `target_modules` in `peft_config``\n",
    "***\n",
    "\n",
    "Once you have the names, you can use yet another configuration argument: target_modules, which is either\n",
    "the name or a list of the names of the modules to which you want to apply the adapters.\n",
    "\n",
    "**Supported Models**\n",
    "\n",
    "If you'd like to check if a given model's architecture is already supported by the installed version of the `peft` package, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85ac9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a3bd853",
    "outputId": "5fdd3702-bb0b-44ab-d036-9d467d5dda3c"
   },
   "outputs": [],
   "source": [
    "from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a153a16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a729f4bb",
    "outputId": "47e2bf62-6da7-4721-c1eb-d16ff7e3904b"
   },
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055fc23",
   "metadata": {
    "id": "7b9a83df"
   },
   "source": [
    "#### The PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb5a74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35c51eff",
    "outputId": "3c071364-9926-4d64-ced4-fbcb89d587f7"
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ef650",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fe2884e",
    "outputId": "628a0563-4703-4afe-f5f3-10069e257767"
   },
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402db03b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc0992b0",
    "outputId": "c426867b-95d1-4da5-9674-bccac596ff16"
   },
   "outputs": [],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d82f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77fa2a75",
    "outputId": "d0ecd8aa-1ba6-4877-eb9c-f7963058c72e"
   },
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e1eec1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca0a102e",
    "outputId": "39f18d0d-cd69-4bdb-899b-0824aac76f48"
   },
   "outputs": [],
   "source": [
    "trainable_parms(peft_model.base_model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565b542",
   "metadata": {
    "id": "a9e8fefa"
   },
   "source": [
    "#### `modules_to_save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed675139",
   "metadata": {
    "id": "7eb95263"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca96ce",
   "metadata": {
    "id": "57fb1553"
   },
   "outputs": [],
   "source": [
    "# Since the model is modified in-place, we need to unload adapters\n",
    "# from previous configuration to avoid mixing them.\n",
    "# In a regular workflow, you'd load configuration only once and\n",
    "# this wouldn't be needed.\n",
    "_ = peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0c0be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9052a377",
    "outputId": "c5f09181-22cc-4f1b-cf4e-f958134c5424"
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6c873",
   "metadata": {
    "id": "a620eaf7"
   },
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414bbfa",
   "metadata": {
    "id": "13f7e0b2"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm', 'embed_tokens']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5434635",
   "metadata": {
    "id": "bc90eaa5"
   },
   "outputs": [],
   "source": [
    "# Since the model is modified in-place, we need to unload adapters\n",
    "# from previous configuration to avoid mixing them.\n",
    "# In a regular workflow, you'd load configuration only once and\n",
    "# this wouldn't be needed.\n",
    "_ = peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce91d50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80178211",
    "outputId": "50f78914-1b25-43a1-ed73-f7d1ca230cab"
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e995e3",
   "metadata": {
    "id": "04e7735c"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4922bb",
   "metadata": {
    "id": "e1d01087"
   },
   "outputs": [],
   "source": [
    "# Since the model is modified in-place, we need to unload adapters\n",
    "# from previous configuration to avoid mixing them.\n",
    "# In a regular workflow, you'd load configuration only once and\n",
    "# this wouldn't be needed.\n",
    "_ = peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f639794",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a09a856",
    "outputId": "dac76edf-d6cc-4989-884f-022de4883aa3"
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375dd51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f13bade5",
    "outputId": "b3594431-4b43-48e1-bcaa-0485a8e7df87"
   },
   "outputs": [],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.embed_tokens\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26dc0a9",
   "metadata": {
    "id": "9875a42d"
   },
   "source": [
    "#### Managing Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb913a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46154ae3",
    "outputId": "7e74c29f-7089-490e-f2cd-99904997c0b5"
   },
   "outputs": [],
   "source": [
    "peft_model.load_adapter('dvgodoy/opt-350m-lora-yoda', adapter_name='yoda')\n",
    "lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb7063",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cfaf80a",
    "outputId": "7b6ec260-4988-465c-ea76-759acc5ab81b"
   },
   "outputs": [],
   "source": [
    "peft_model.add_adapter(adapter_name='third', peft_config=config)\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d0c63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a9d787c",
    "outputId": "3bd55add-b059-4be1-85b5-5516f2089e4e"
   },
   "outputs": [],
   "source": [
    "peft_model.delete_adapter(adapter_name='third')\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d0bb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60106701",
    "outputId": "580075aa-669d-41e9-fa3f-c779cfc4caa3"
   },
   "outputs": [],
   "source": [
    "peft_model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed93286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e69e9f7c",
    "outputId": "ea949b66-f88f-41b0-83de-2dc3d83aa7b8"
   },
   "outputs": [],
   "source": [
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a6d06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "563a806c",
    "outputId": "a166dbb0-ccfe-49b5-dde2-d349290566ff"
   },
   "outputs": [],
   "source": [
    "peft_model.set_adapter('yoda')\n",
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29965d0",
   "metadata": {
    "id": "8c5f03c6"
   },
   "source": [
    "```python\n",
    "with peft_model.disable_adapter():\n",
    "    original_outputs = peft_model(inputs)\n",
    "\n",
    "original_outputs = peft_model.base_model(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1989ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8358ebf9",
    "outputId": "00aa69eb-f21e-4f5e-e9ec-8c054ebe06b8"
   },
   "outputs": [],
   "source": [
    "peft_model.merge_adapter(adapter_names=['yoda'])\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2018ce5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdea3313",
    "outputId": "ce884503-0035-4640-b9be-f9b3a84cc0c8"
   },
   "outputs": [],
   "source": [
    "peft_model.unload()\n",
    "peft_model.base_model.model.model.decoder.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab320816",
   "metadata": {
    "id": "dda37040"
   },
   "source": [
    "### Coming Up in \"Fine-Tuning LLMs\"\n",
    "\n",
    "Low-rank adapters saved the day by swooping in and enabling fast and cheap fine-tuning for LLMs. These humongous models, although powerful, are masters of a single tradeâ€”predicting the next tokenâ€”thus remaining limited by the structure of their inputs. A new kind of input must be developed to enable these creatures to chat. Learn more about the incredible tale of chat templates in the next chapter of \"Fine-Tuning LLMs.\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
