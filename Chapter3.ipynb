{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d578ad67",
      "metadata": {
        "id": "d578ad67"
      },
      "source": [
        "## 3장 LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용된 패키지\n",
        "\n",
        "* torch 2.9.0\n",
        "* transformers 5.2.0\n",
        "* datasets 4.0.0\n",
        "* bitsandbytes 0.49.1\n",
        "* peft 1.18.0"
      ],
      "metadata": {
        "id": "s0awpz0vEgLZ"
      },
      "id": "s0awpz0vEgLZ"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "378c787c-c30a-47bb-a3b8-b3140ec66629",
      "metadata": {
        "id": "378c787c-c30a-47bb-a3b8-b3140ec66629"
      },
      "outputs": [],
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "import os\n",
        "from transformers.utils import logging\n",
        "\n",
        "# tqdm 비활성화\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "logging.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9090ae39",
      "metadata": {
        "id": "9090ae39"
      },
      "source": [
        "### 스포일러\n",
        "이 장에서는 다음과 같은 내용을 배웁니다.\n",
        "\n",
        "- LoRA(Low-Rank Adaptation)가 무엇이고 왜 유용한지 이해합니다.\n",
        "- 훈련을 위해 양자화된 모델을 준비합니다.\n",
        "- `peft` 라이브러리를 사용해 어댑터(adapter)를 만들어 베이스 모델에 추가합니다.\n",
        "- 훈련 대상 층을 지정하기 위한 설정 옵션을 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b56b9edf",
      "metadata": {
        "id": "b56b9edf"
      },
      "source": [
        "### 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b6d9b772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6d9b772",
        "outputId": "911d00d2-5e40-48f8-f7fb-c6eff5b92e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.26.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 코랩에서 실행하는 경우\n",
        "!pip install bitsandbytes trl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7cf5859",
      "metadata": {
        "id": "e7cf5859"
      },
      "source": [
        "### 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c093a0aa",
      "metadata": {
        "id": "c093a0aa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from numpy.linalg import matrix_rank\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a573c625",
      "metadata": {
        "id": "a573c625"
      },
      "source": [
        "### 목표\n",
        "\n",
        "LLM에 있는 거대 선형 층에 어댑터를 추가하여 훈련 파라미터 개수를 극적으로 줄입니다. 훈련 파라미터 개수를 원래 파라미터 개수의 1% 미만으로 쉽게 줄일 수 있습니다. 계산(그레이디언트가 적음)과 메모리 사용량(옵티마이저가 추적할 파라미터가 적음)을 줄임으로써 효율성을 크게 높일 수 있습니다. 하지만 LoRA는 완전히 미세 튜닝한 모델의 성능에 도달하지 못할 가능성이 많으며 효과는 베이스 모델과 작업에 따라 다를 수 있음을 유념하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d220e57",
      "metadata": {
        "id": "1d220e57"
      },
      "source": [
        "### 준비 운동\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/matmul.png?raw=True)\n",
        "<center>그림 3.1 - 행렬 곱셈</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32ad170",
      "metadata": {
        "id": "f32ad170"
      },
      "source": [
        "### LoRA의 핵심\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/two_matrices.png?raw=True)\n",
        "<center>그림 3.2 - 두 개의 작은 행렬 곱하기</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "59057a29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59057a29",
        "outputId": "3e033936-97c1-4a14-8346-24edb4c5455c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 1024]), 1048576)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "base_layer = nn.Linear(1024, 1024, bias=False)\n",
        "base_layer.weight.shape, base_layer.weight.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7bcba3",
      "metadata": {
        "id": "5b7bcba3"
      },
      "source": [
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/lowrank_matrices.png?raw=True)\n",
        "<center>그림 3.3 동결된 가중치와 작은 행렬</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d28cb90b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d28cb90b",
        "outputId": "abe4e406-7703-43d7-ec34-85e2f71628bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Linear(in_features=1024, out_features=8, bias=False),\n",
              " Linear(in_features=8, out_features=1024, bias=False))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.manual_seed(11)\n",
        "r = 8\n",
        "layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
        "layer_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
        "layer_A, layer_B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb1f95a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb1f95a3",
        "outputId": "08164898-14b1-4481-cc38-30211ddfac72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8192, 8192)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "layer_A.weight.numel(), layer_B.weight.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d10aee54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d10aee54",
        "outputId": "26d7336a-fe1f-4e53-a333-db9a0ab153b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 1024]), 1048576)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "composite = layer_B.weight @ layer_A.weight\n",
        "composite.shape, composite.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d43acb37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d43acb37",
        "outputId": "ddc13257-3621-471e-d818-ab295cfe4823"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "matrix_rank(composite.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff63efc8",
      "metadata": {
        "id": "ff63efc8"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{output} = X @ (W + B @ A)^T\n",
        "$$\n",
        "<center>식 3.1 가중치에 곱셈 결과 행렬을 더하기</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c9c9e8e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9c9e8e4",
        "outputId": "8f7e446a-e0a8-4ae9-e508-be67339021f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9942, -0.8913, -0.6378,  ...,  0.3953, -0.8154,  1.1929]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "torch.manual_seed(19)\n",
        "batch = torch.randn(1, 1024)\n",
        "\n",
        "batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2753fad4",
      "metadata": {
        "id": "2753fad4"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n",
        "$$\n",
        "<center>식 3.2 두 개의 정방향 계산</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/forward.png?raw=True)\n",
        "<center>그림 3.4 두 개의 정방향 계산 사용하기</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13473a5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13473a5d",
        "outputId": "d1c532d1-82a8-4eec-ecb3-5157fefc99e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.9584, -0.9143, -0.1771,  ...,  0.6873, -1.0098,  0.6888]]),\n",
              " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
              "        grad_fn=<MmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "regular_output = batch @ base_layer.weight.data.T\n",
        "additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n",
        "regular_output, additional_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a894d27d",
      "metadata": {
        "id": "a894d27d"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n",
        "$$\n",
        "<center>식 3.3 어댑터의 정방향 계산을 분할하기</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e1bf4baa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bf4baa",
        "outputId": "c30ed315-2025-45ff-ea06-0ae89b7b9b0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "out_A = (batch @ layer_A.weight.T)\n",
        "additional_output = out_A @ layer_B.weight.T\n",
        "additional_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "853fe68e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853fe68e",
        "outputId": "46248ef7-8f3c-432f-bc17-8e6a046b234c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.9584, -0.9143, -0.1771,  ...,  0.6873, -1.0098,  0.6888]],\n",
              "        grad_fn=<MmBackward0>),\n",
              " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
              "        grad_fn=<MmBackward0>),\n",
              " tensor([[-0.9942, -0.8913, -0.6378,  ...,  0.3953, -0.8154,  1.1929]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "regular_output = base_layer(batch)\n",
        "out_A = layer_A(batch)\n",
        "additional_output = layer_B(out_A)\n",
        "output = regular_output + additional_output\n",
        "regular_output, additional_output, output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09c6072",
      "metadata": {
        "id": "f09c6072"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n",
        "$$\n",
        "<center>식 3.4 LoRA의 스케일링 인자</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2d5604dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d5604dc",
        "outputId": "9a976020-8d29-4cac-8bbf-57cd3f01ac24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.0299, -0.8683, -1.0985,  ...,  0.1032, -0.6210,  1.6970]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "alpha = 2*r\n",
        "output = regular_output + (alpha / r) * additional_output\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eab264b",
      "metadata": {
        "id": "2eab264b"
      },
      "source": [
        "### 준비 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9a4e5d2e-8b15-43e2-879d-e4481f7fa7cd",
      "metadata": {
        "id": "9a4e5d2e-8b15-43e2-879d-e4481f7fa7cd"
      },
      "outputs": [],
      "source": [
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "\n",
        "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
        "                                                device_map='cuda:0',\n",
        "                                                dtype=compute_dtype,\n",
        "                                                quantization_config=nf4_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54d8255",
      "metadata": {
        "id": "d54d8255"
      },
      "source": [
        "### 파라미터 타입과 그레이디언트\n",
        "\n",
        "****\n",
        "**\"파라미터 타입과 그레이디언트\" 요약**\n",
        "\n",
        "- 양자화는 양자화된 선형 층만 동결합니다.\n",
        "- 양자화 후에 `prepare_model_for_kbit_training()` 함수를 사용해 모델을 준비합니다.\n",
        "    - 모든 층을 동결합니다.\n",
        "    - 양자화되지 않은 모든 16비트 층을 FP32로 바꾸어 훈련을 안정화합니다.\n",
        "    - 그레이디언트 체크포인팅을 활성화합니다.\n",
        "- LoRA 설정을 사용해 나중에 선택적으로 층을 동결 해제할 수 있습니다.\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "210ca55c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "210ca55c",
        "outputId": "10a5f5ec-dabc-4639-9242-59a7fea4ecf4",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('decoder.embed_tokens.weight', torch.float32),\n",
              " ('decoder.embed_positions.weight', torch.float32),\n",
              " ('decoder.layers.0.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.0.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.0.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.0.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.1.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.1.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.1.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.1.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.2.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.2.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.2.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.2.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.3.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.3.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.3.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.3.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.4.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.4.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.4.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.4.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.5.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.5.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.5.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.5.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.6.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.6.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.6.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.6.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.7.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.7.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.7.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.7.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.8.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.8.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.8.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.8.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.9.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.9.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.9.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.9.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.10.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.10.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.10.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.10.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.11.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.11.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.11.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.11.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.12.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.12.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.12.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.12.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.13.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.13.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.13.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.13.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.14.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.14.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.14.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.14.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.15.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.15.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.15.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.15.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.16.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.16.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.16.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.16.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.17.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.17.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.17.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.17.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.18.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.18.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.18.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.18.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.19.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.19.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.19.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.19.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.20.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.20.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.20.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.20.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.21.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.21.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.21.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.21.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.22.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.22.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.22.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.22.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.23.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.23.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.23.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.23.final_layer_norm.bias', torch.float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def trainable_parms(model):\n",
        "    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n",
        "    return parms\n",
        "\n",
        "trainable_parms(model_q4.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13c94df",
      "metadata": {
        "id": "d13c94df"
      },
      "source": [
        "#### `prepare_model_for_kbit_training()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "77a8093b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77a8093b",
        "outputId": "0bb6352b-6ac4-4a40-a612-db06c19bff66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
              "      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "prepared_model = prepare_model_for_kbit_training(model_q4,\n",
        "                                        use_gradient_checkpointing=True,\n",
        "                                        gradient_checkpointing_kwargs={'use_reentrant': False})\n",
        "prepared_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "709936d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "709936d7",
        "outputId": "3c8645e2-92c5-4362-b12d-a1f4727694d1",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "trainable_parms(prepared_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4277e665",
      "metadata": {
        "id": "4277e665"
      },
      "outputs": [],
      "source": [
        "def parms_of_dtype(model, dtype=torch.float32):\n",
        "    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n",
        "    return parms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43586616",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43586616",
        "outputId": "5301a041-8aa4-4884-db9d-2fbef6a0bbbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.decoder.embed_tokens.weight',\n",
              " 'model.decoder.embed_positions.weight',\n",
              " 'model.decoder.layers.0.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.0.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.0.fc1.bias',\n",
              " 'model.decoder.layers.0.fc2.bias',\n",
              " 'model.decoder.layers.0.final_layer_norm.weight',\n",
              " 'model.decoder.layers.0.final_layer_norm.bias',\n",
              " 'model.decoder.layers.1.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.1.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.1.fc1.bias',\n",
              " 'model.decoder.layers.1.fc2.bias',\n",
              " 'model.decoder.layers.1.final_layer_norm.weight',\n",
              " 'model.decoder.layers.1.final_layer_norm.bias',\n",
              " 'model.decoder.layers.2.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.2.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.2.fc1.bias',\n",
              " 'model.decoder.layers.2.fc2.bias',\n",
              " 'model.decoder.layers.2.final_layer_norm.weight',\n",
              " 'model.decoder.layers.2.final_layer_norm.bias',\n",
              " 'model.decoder.layers.3.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.3.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.3.fc1.bias',\n",
              " 'model.decoder.layers.3.fc2.bias',\n",
              " 'model.decoder.layers.3.final_layer_norm.weight',\n",
              " 'model.decoder.layers.3.final_layer_norm.bias',\n",
              " 'model.decoder.layers.4.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.4.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.4.fc1.bias',\n",
              " 'model.decoder.layers.4.fc2.bias',\n",
              " 'model.decoder.layers.4.final_layer_norm.weight',\n",
              " 'model.decoder.layers.4.final_layer_norm.bias',\n",
              " 'model.decoder.layers.5.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.5.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.5.fc1.bias',\n",
              " 'model.decoder.layers.5.fc2.bias',\n",
              " 'model.decoder.layers.5.final_layer_norm.weight',\n",
              " 'model.decoder.layers.5.final_layer_norm.bias',\n",
              " 'model.decoder.layers.6.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.6.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.6.fc1.bias',\n",
              " 'model.decoder.layers.6.fc2.bias',\n",
              " 'model.decoder.layers.6.final_layer_norm.weight',\n",
              " 'model.decoder.layers.6.final_layer_norm.bias',\n",
              " 'model.decoder.layers.7.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.7.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.7.fc1.bias',\n",
              " 'model.decoder.layers.7.fc2.bias',\n",
              " 'model.decoder.layers.7.final_layer_norm.weight',\n",
              " 'model.decoder.layers.7.final_layer_norm.bias',\n",
              " 'model.decoder.layers.8.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.8.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.8.fc1.bias',\n",
              " 'model.decoder.layers.8.fc2.bias',\n",
              " 'model.decoder.layers.8.final_layer_norm.weight',\n",
              " 'model.decoder.layers.8.final_layer_norm.bias',\n",
              " 'model.decoder.layers.9.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.9.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.9.fc1.bias',\n",
              " 'model.decoder.layers.9.fc2.bias',\n",
              " 'model.decoder.layers.9.final_layer_norm.weight',\n",
              " 'model.decoder.layers.9.final_layer_norm.bias',\n",
              " 'model.decoder.layers.10.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.10.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.10.fc1.bias',\n",
              " 'model.decoder.layers.10.fc2.bias',\n",
              " 'model.decoder.layers.10.final_layer_norm.weight',\n",
              " 'model.decoder.layers.10.final_layer_norm.bias',\n",
              " 'model.decoder.layers.11.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.11.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.11.fc1.bias',\n",
              " 'model.decoder.layers.11.fc2.bias',\n",
              " 'model.decoder.layers.11.final_layer_norm.weight',\n",
              " 'model.decoder.layers.11.final_layer_norm.bias',\n",
              " 'model.decoder.layers.12.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.12.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.12.fc1.bias',\n",
              " 'model.decoder.layers.12.fc2.bias',\n",
              " 'model.decoder.layers.12.final_layer_norm.weight',\n",
              " 'model.decoder.layers.12.final_layer_norm.bias',\n",
              " 'model.decoder.layers.13.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.13.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.13.fc1.bias',\n",
              " 'model.decoder.layers.13.fc2.bias',\n",
              " 'model.decoder.layers.13.final_layer_norm.weight',\n",
              " 'model.decoder.layers.13.final_layer_norm.bias',\n",
              " 'model.decoder.layers.14.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.14.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.14.fc1.bias',\n",
              " 'model.decoder.layers.14.fc2.bias',\n",
              " 'model.decoder.layers.14.final_layer_norm.weight',\n",
              " 'model.decoder.layers.14.final_layer_norm.bias',\n",
              " 'model.decoder.layers.15.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.15.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.15.fc1.bias',\n",
              " 'model.decoder.layers.15.fc2.bias',\n",
              " 'model.decoder.layers.15.final_layer_norm.weight',\n",
              " 'model.decoder.layers.15.final_layer_norm.bias',\n",
              " 'model.decoder.layers.16.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.16.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.16.fc1.bias',\n",
              " 'model.decoder.layers.16.fc2.bias',\n",
              " 'model.decoder.layers.16.final_layer_norm.weight',\n",
              " 'model.decoder.layers.16.final_layer_norm.bias',\n",
              " 'model.decoder.layers.17.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.17.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.17.fc1.bias',\n",
              " 'model.decoder.layers.17.fc2.bias',\n",
              " 'model.decoder.layers.17.final_layer_norm.weight',\n",
              " 'model.decoder.layers.17.final_layer_norm.bias',\n",
              " 'model.decoder.layers.18.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.18.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.18.fc1.bias',\n",
              " 'model.decoder.layers.18.fc2.bias',\n",
              " 'model.decoder.layers.18.final_layer_norm.weight',\n",
              " 'model.decoder.layers.18.final_layer_norm.bias',\n",
              " 'model.decoder.layers.19.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.19.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.19.fc1.bias',\n",
              " 'model.decoder.layers.19.fc2.bias',\n",
              " 'model.decoder.layers.19.final_layer_norm.weight',\n",
              " 'model.decoder.layers.19.final_layer_norm.bias',\n",
              " 'model.decoder.layers.20.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.20.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.20.fc1.bias',\n",
              " 'model.decoder.layers.20.fc2.bias',\n",
              " 'model.decoder.layers.20.final_layer_norm.weight',\n",
              " 'model.decoder.layers.20.final_layer_norm.bias',\n",
              " 'model.decoder.layers.21.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.21.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.21.fc1.bias',\n",
              " 'model.decoder.layers.21.fc2.bias',\n",
              " 'model.decoder.layers.21.final_layer_norm.weight',\n",
              " 'model.decoder.layers.21.final_layer_norm.bias',\n",
              " 'model.decoder.layers.22.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.22.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.22.fc1.bias',\n",
              " 'model.decoder.layers.22.fc2.bias',\n",
              " 'model.decoder.layers.22.final_layer_norm.weight',\n",
              " 'model.decoder.layers.22.final_layer_norm.bias',\n",
              " 'model.decoder.layers.23.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.23.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.23.fc1.bias',\n",
              " 'model.decoder.layers.23.fc2.bias',\n",
              " 'model.decoder.layers.23.final_layer_norm.weight',\n",
              " 'model.decoder.layers.23.final_layer_norm.bias']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "parms_of_dtype(prepared_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "55f83018",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55f83018",
        "outputId": "48e58c5a-ea32-43db-cee8-7ef63b37e7ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264.15104"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prepared_model.get_memory_footprint()/1e6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1b2537",
      "metadata": {
        "id": "6f1b2537"
      },
      "source": [
        "### PEFT\n",
        "\n",
        "\"_🤗 PEFT(Parameter-Efficient Fine-Tuning)는 사전 훈련된 대규모 모델을 다양한 후속 애플리케이션에 효율적으로 적응시키기 위한 라이브러리입니다. 매우 많은 비용이 들기 때문에 모델 파라미터를 모두 미세 튜닝하지 않습니다. PEFT 방법은 모델에 추가된 적은 개수의 파라미터만 미세 튜닝합니다. 계산 비용과 저장 공간을 크게 줄이면서 완전히 미세 튜닝된 모델에 비견할만한 성능을 냅니다. 이를 통해 대규모 언어 모델을 개인용 하드웨어에서 훈련할 수 있습니다._\n",
        "\n",
        "_PEFT는 transformers, diffusers, accelerate 라이브러리와 통합되어 있어 대규모 모델을 쉽고 빠르게 로드하고, 훈련하고, 추론에 사용할 수 있습니다._\"\n",
        "\n",
        "****\n",
        "**\"PEFT\" 요약**\n",
        "- 다음 기본 설정이 대부분의 경우 잘 동작합니다.\n",
        "```python\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "peft_model = get_peft_model(model, config)\n",
        "```\n",
        "    - 일반적인 랭크 값은 8, 16, 32이지만 더 높은 값을 사용해도 모델의 메모리 사용량에 큰 영향을 미치지 않습니다.\n",
        "    - 스케일링 인자인 `lora_alpha`는 일반적으로 랭크의 두 배로 합니다.\n",
        "- 모델이 `Conv1D`를 사용한다면 설정에 `fan_in_fan_out=True`를 추가합니다.\n",
        "- 최근에 공개된 모델이라면 수동으로 `target_modules`를 지정해야 할 수 있습니다.\n",
        "  - 일반적으로 어텐션 모듈 안에 있는 대규모의 선형 층을 지정합니다.\n",
        "- 기본적으로 어댑터만 훈련 가능합니다.\n",
        "  - 층 정규화 같은 다른 층을 훈련하고 싶다면 `modules_to_save` 매개변수에 추가하세요.\n",
        "  - 토크나이저에 토큰을 추가하고 싶다면 임베딩과 모델 헤드와 같이 어휘사전에 관련된 층도 훈련해야 합니다.\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c3e78bf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3e78bf0",
        "outputId": "c539bd37-8b07-4da2-d4a7-98d6a27bc84c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules=None, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "lora_config = LoraConfig()\n",
        "lora_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e3647c9d",
      "metadata": {
        "id": "e3647c9d"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6471b64b",
      "metadata": {
        "id": "6471b64b"
      },
      "source": [
        "### `target_modules`\n",
        "\n",
        "매주 새로운 모델과 구조가 공개되고 있기 때문에 현재 설치된 PEFT 라이브러리의 사전 설정된 층 목록에 없을 수 있습니다. 이런 경우 다음과 같은 오류가 발생합니다:\n",
        "\n",
        "***\n",
        "`ValueError: Please specify `target_modules` in `peft_config``\n",
        "***\n",
        "\n",
        "이름이 확인되면 `target_modules` 매개변수를 사용하여 어댑터를 적용할 모듈 이름이나 목록을 지정할 수 있습니다.\n",
        "\n",
        "**지원 모델**\n",
        "\u001f\n",
        "사용하려는 모델 구조가 peft 패키지에서 지원하는지 확인하려면 다음 코드를 참고하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3d85ac9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d85ac9d",
        "outputId": "88118809-a156-492b-a5e4-f7037b1fc6fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['t5', 'mt5', 'bart', 'gpt2', 'bloom', 'blip-2', 'opt', 'gptj', 'gpt_neox', 'gpt_neo', 'bert', 'roberta', 'xlm-roberta', 'electra', 'deberta-v2', 'deberta', 'layoutlm', 'llama', 'llama4', 'chatglm', 'gpt_bigcode', 'mpt', 'RefinedWebModel', 'RefinedWeb', 'falcon', 'btlm', 'codegen', 'mistral', 'mixtral', 'stablelm', 'phi', 'gemma', 'gemma2', 'gemma3_text', 'qwen2', 'qwen3', 'rwkv', 'rwkv7'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9a153a16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a153a16",
        "outputId": "a6488a3f-3045-4ef1-e18d-4b800e9895f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['q_proj', 'v_proj', 'fc1', 'fc2']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4055fc23",
      "metadata": {
        "id": "4055fc23"
      },
      "source": [
        "#### PEFT 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "73bb5a74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73bb5a74",
        "outputId": "d5e93976-1b9c-4b30-f8e6-527bd790b29b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): OPTForCausalLM(\n",
              "      (model): OPTModel(\n",
              "        (decoder): OPTDecoder(\n",
              "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
              "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
              "          (layers): ModuleList(\n",
              "            (0-23): 24 x OPTDecoderLayer(\n",
              "              (self_attn): OPTAttention(\n",
              "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                (v_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (q_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              )\n",
              "              (activation_fn): ReLU()\n",
              "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n",
        "peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "aa9ef650",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9ef650",
        "outputId": "c0ad7ac2-0b40-41e4-9649-0577f062f38b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['q_proj', 'v_proj']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "402db03b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "402db03b",
        "outputId": "415524cc-bccc-4fc4-ce59-23a802a6346d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lora.Linear4bit(\n",
              "  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  (lora_dropout): ModuleDict(\n",
              "    (default): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (lora_A): ModuleDict(\n",
              "    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  )\n",
              "  (lora_B): ModuleDict(\n",
              "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "  )\n",
              "  (lora_embedding_A): ParameterDict()\n",
              "  (lora_embedding_B): ParameterDict()\n",
              "  (lora_magnitude_vector): ModuleDict()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n",
        "lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "424d82f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "424d82f5",
        "outputId": "1ce21dac-7387-4365-f00c-72f42c48767b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"
          ]
        }
      ],
      "source": [
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a8e1eec1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8e1eec1",
        "outputId": "7e88e8b6-5727-403b-e5e9-26c4be2f8549"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "trainable_parms(peft_model.base_model.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0565b542",
      "metadata": {
        "id": "0565b542"
      },
      "source": [
        "#### `modules_to_save`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ed675139",
      "metadata": {
        "id": "ed675139"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=['layer_norm']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0aca96ce",
      "metadata": {
        "id": "0aca96ce"
      },
      "outputs": [],
      "source": [
        "# `get_peft_model()` 함수는 입력된 모델을 수정하므로 다른 설정 객체로 반복해서 호출하면 설정이 꼬이게 됩니다.\n",
        "# 일반적인 워크플로에서는 설정을 한 번만 로드하므로 이런 코드가 필요하지 않습니다.\n",
        "_ = peft_model.unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "43f0c0be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43f0c0be",
        "outputId": "6ab9d558-3649-4ae4-e4db-5447859e28df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 884,736 || all params: 332,081,152 || trainable%: 0.2664\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b6c873",
      "metadata": {
        "id": "f3b6c873"
      },
      "source": [
        "#### 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9414bbfa",
      "metadata": {
        "id": "9414bbfa"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=['layer_norm', 'embed_tokens']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c5434635",
      "metadata": {
        "id": "c5434635"
      },
      "outputs": [],
      "source": [
        "# `get_peft_model()` 함수는 입력된 모델을 수정하므로 다른 설정 객체로 반복해서 호출하면 설정이 꼬이게 됩니다.\n",
        "# 일반적인 워크플로에서는 설정을 한 번만 로드하므로 이런 코드가 필요하지 않습니다.\n",
        "_ = peft_model.unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2ce91d50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ce91d50",
        "outputId": "4ef0746d-6ba2-4dfc-a532-6e17bccc18b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:1222: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 26,624,000 || all params: 357,820,416 || trainable%: 7.4406\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f5e995e3",
      "metadata": {
        "id": "f5e995e3"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "fc4922bb",
      "metadata": {
        "id": "fc4922bb"
      },
      "outputs": [],
      "source": [
        "# `get_peft_model()` 함수는 입력된 모델을 수정하므로 다른 설정 객체로 반복해서 호출하면 설정이 꼬이게 됩니다.\n",
        "# 일반적인 워크플로에서는 설정을 한 번만 로드하므로 이런 코드가 필요하지 않습니다.\n",
        "_ = peft_model.unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1f639794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f639794",
        "outputId": "c9f46f4f-a4c0-4ede-f342-1e1561bb3c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,192,704 || all params: 358,128,384 || trainable%: 0.3330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:916: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8375dd51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8375dd51",
        "outputId": "1b7072a4-dd41-4d38-8608-1f9e45030a39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lora.Embedding(\n",
              "  (base_layer): Embedding(50272, 512, padding_idx=1)\n",
              "  (lora_dropout): ModuleDict(\n",
              "    (default): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (lora_A): ModuleDict()\n",
              "  (lora_B): ModuleDict()\n",
              "  (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x50272 (cuda:0)])\n",
              "  (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x8 (cuda:0)])\n",
              "  (lora_magnitude_vector): ModuleDict()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "lin = peft_model.base_model.model.model.decoder.embed_tokens\n",
        "lin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b26dc0a9",
      "metadata": {
        "id": "b26dc0a9"
      },
      "source": [
        "#### 어댑터 관리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "55eb913a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55eb913a",
        "outputId": "f2759051-281b-4847-d8a2-02d71245db98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "peft_model.load_adapter('dvgodoy/opt-350m-lora-yoda', adapter_name='yoda')\n",
        "lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n",
        "lora_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "2ceb7063",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ceb7063",
        "outputId": "c580ca5d-cf68-4581-df84-667079e543c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:916: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (third): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "peft_model.add_adapter(adapter_name='third', peft_config=config)\n",
        "lora_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "689d0c63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "689d0c63",
        "outputId": "ce167a4d-5287-4800-ce09-3c0dc32d1bf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "peft_model.delete_adapter(adapter_name='third')\n",
        "lora_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f65d0bb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f65d0bb0",
        "outputId": "d16dd454-0070-4752-e9e7-fb2e1d7a74b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['default', 'yoda'])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "peft_model.peft_config.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "aed93286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aed93286",
        "outputId": "7b266f75-8b99-4a74-acef-807a8731213d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'default'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "peft_model.active_adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "8a4a6d06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8a4a6d06",
        "outputId": "69814f09-a9a1-405e-f15b-96d4f2a5f8d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yoda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "peft_model.set_adapter('yoda')\n",
        "peft_model.active_adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29965d0",
      "metadata": {
        "id": "c29965d0"
      },
      "source": [
        "```python\n",
        "with peft_model.disable_adapter():\n",
        "    original_outputs = peft_model(inputs)\n",
        "\n",
        "original_outputs = peft_model.base_model(inputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "da1989ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da1989ad",
        "outputId": "56819db9-1a61-4d4b-c7b3-7fd6fc081e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:569: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
            "```python\n",
            "from transformers import AutoModelForCausalLM\n",
            "\n",
            "# Load original tied model\n",
            "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
            "\n",
            "# Set the randomly initialized lm_head to the previously tied embeddings\n",
            "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
            "\n",
            "# Save the untied model\n",
            "untied_model_dir = \"dir/for/untied/model\"\n",
            "model.save_pretrained(untied_model_dir)\n",
            "model.config.save_pretrained(untied_model_dir)\n",
            "\n",
            "# Now use the original model but in untied format\n",
            "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
            "```\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "peft_model.merge_adapter(adapter_names=['yoda'])\n",
        "lora_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "c2018ce5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2018ce5",
        "outputId": "a0119096-1ecb-42ca-8128-a4c931d7855e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTAttention(\n",
              "  (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "peft_model.unload()\n",
        "peft_model.base_model.model.model.decoder.layers[0].self_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab320816",
      "metadata": {
        "id": "ab320816"
      },
      "source": [
        "### 다음 장에서는\n",
        "\n",
        "LoRA 어댑터는 빠르고 적은 비용으로 LLM을 미세 튜닝할 수 있습니다. 이런 거대 모델은 강력하지만 다음 토큰 예측이라는 단일 작업에 특화되어 있어 입력 구조에 한계가 있습니다. 이런 모델에게 대화 기능을 부여하려면 새로운 종류의 입력을 개발해야 합니다. 다음 장에서 채팅 템플릿에 관해 자세히 알아 보겠습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}