{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fefef8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578ad67",
   "metadata": {
    "id": "d578ad67"
   },
   "source": [
    "## Chapter 3: Low-Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c787c-c30a-47bb-a3b8-b3140ec66629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행 표시줄을 나타내지 않기 위해 (깃허브의 위젯 상태 오류)\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_PROGRESS_BAR\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090ae39",
   "metadata": {
    "id": "9090ae39"
   },
   "source": [
    "### Spoilers\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Understand what a low-rank adapter is and why it’s useful\n",
    "- Prepare the quantized model for training\n",
    "- Use `peft` to create and attach adapters to a base model\n",
    "- Discuss configuration options for targeting layers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b9edf",
   "metadata": {
    "id": "b56b9edf"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d9b772",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6d9b772",
    "outputId": "67c58665-989b-4060-b4df-65186899598e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.9.0)\n",
      "Requirement already satisfied: transformers>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.53.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl) (0.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# If you're running on Colab\n",
    "!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0bf629",
   "metadata": {
    "id": "dc0bf629"
   },
   "outputs": [],
   "source": [
    "# If you're running on runpod.io's Jupyter Template\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5859",
   "metadata": {
    "id": "e7cf5859"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c093a0aa",
   "metadata": {
    "id": "c093a0aa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import matrix_rank\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573c625",
   "metadata": {
    "id": "a573c625"
   },
   "source": [
    "### The Goal\n",
    "\n",
    "We attach adapters to the huge linear layers in an LLM to drastically reduce the number of trainable parameters. We can easily shrink the number of trainable parameters down to less than 1% of their original number. By reducing both computation (fewer gradients to compute) and memory footprint (fewer parameters tracked by the optimizer), we achieve significant efficiency gains. Keep in mind, however, that low-rank adapters are unlikely to match the performance of full-model tuning, and their effectiveness may vary depending on the base model and the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d220e57",
   "metadata": {
    "id": "1d220e57"
   },
   "source": [
    "### Pre-Reqs\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/matmul.png?raw=True)\n",
    "<center>Figure 3.1 - Matrix multiplication</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ad170",
   "metadata": {
    "id": "f32ad170"
   },
   "source": [
    "### Low-Rank Adaptation in a Nutshell\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/two_matrices.png?raw=True)\n",
    "<center>Figure 3.2 - Multiplying two low-rank matrices</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59057a29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59057a29",
    "outputId": "edd3edd9-8938-4aa4-98b1-973de0fb2f90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1024]), 1048576)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer = nn.Linear(1024, 1024, bias=False)\n",
    "base_layer.weight.shape, base_layer.weight.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bcba3",
   "metadata": {
    "id": "5b7bcba3"
   },
   "source": [
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/lowrank_matrices.png?raw=True)\n",
    "<center>Figure 3.3 - Frozen weights and low-rank matrices</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28cb90b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d28cb90b",
    "outputId": "f85430b6-0b6d-4c6c-88b8-1f63fcde43b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=1024, out_features=8, bias=False),\n",
       " Linear(in_features=8, out_features=1024, bias=False))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "r = 8\n",
    "layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
    "layer_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
    "layer_A, layer_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1f95a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb1f95a3",
    "outputId": "3a593d99-98ec-4aa0-e253-a79682cd7f20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 8192)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_A.weight.numel(), layer_B.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10aee54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d10aee54",
    "outputId": "f0ba5f43-fbf8-48d1-b29b-5f8a7338c3d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1024]), 1048576)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite = layer_B.weight @ layer_A.weight\n",
    "composite.shape, composite.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43acb37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d43acb37",
    "outputId": "13506247-4f0e-4e7d-9dcb-267e889c0a06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_rank(composite.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63efc8",
   "metadata": {
    "id": "ff63efc8"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = X @ (W + B @ A)^T\n",
    "$$\n",
    "<center>Equation 3.1 - Adding the resulting product to the weights</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c9e8e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9c9e8e4",
    "outputId": "9b018356-247a-4012-ae83-73b679511ecc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0278, -0.6941, -0.4225,  ...,  0.4753, -1.0289,  0.3819]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "batch = torch.randn(1, 1024)\n",
    "\n",
    "batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753fad4",
   "metadata": {
    "id": "2753fad4"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n",
    "$$\n",
    "<center>Equation 3.2 - Using two forward passes</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/forward.png?raw=True)\n",
    "<center>Figure 3.4 - Using two forward passes</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13473a5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13473a5d",
    "outputId": "d111299b-86cf-42ff-ce14-e5dac5391f68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0079, -0.7171,  0.0382,  ...,  0.7674, -1.2233, -0.1222]]),\n",
       " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_output = batch @ base_layer.weight.data.T\n",
    "additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n",
    "regular_output, additional_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894d27d",
   "metadata": {
    "id": "a894d27d"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n",
    "$$\n",
    "<center>Equation 3.3 - Chaining the adapter’s forward passes</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bf4baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1bf4baa",
    "outputId": "50babb5b-7586-4f60-cac4-0c9e13a8065c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_A = (batch @ layer_A.weight.T)\n",
    "additional_output = out_A @ layer_B.weight.T\n",
    "additional_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853fe68e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "853fe68e",
    "outputId": "b8298cba-354d-431c-af6d-ca59a7a4f0be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0079, -0.7171,  0.0382,  ...,  0.7674, -1.2233, -0.1222]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[-0.0278, -0.6941, -0.4225,  ...,  0.4753, -1.0289,  0.3819]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_output = base_layer(batch)\n",
    "out_A = layer_A(batch)\n",
    "additional_output = layer_B(out_A)\n",
    "output = regular_output + additional_output\n",
    "regular_output, additional_output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c6072",
   "metadata": {
    "id": "f09c6072"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n",
    "$$\n",
    "<center>Equation 3.4 - LoRA’s alpha</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5604dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d5604dc",
    "outputId": "ab1891e9-9b39-4669-d720-5a69e12cbf3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0635, -0.6711, -0.8832,  ...,  0.1833, -0.8345,  0.8860]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 2*r\n",
    "output = regular_output + (alpha / r) * additional_output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab264b",
   "metadata": {
    "id": "2eab264b"
   },
   "source": [
    "### The Road So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a4e5d2e-8b15-43e2-879d-e4481f7fa7cd",
   "metadata": {
    "id": "9a4e5d2e-8b15-43e2-879d-e4481f7fa7cd"
   },
   "outputs": [],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='cuda:0',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d8255",
   "metadata": {
    "id": "d54d8255"
   },
   "source": [
    "### Parameter Types and Gradients\n",
    "\n",
    "****\n",
    "**Summary of \"Parameter Types and Gradients\"**\n",
    "- quantization only freezes the linear layers that have been quantized\n",
    "- after quantization, a model can be prepared using the `prepare_model_for_kbit_training()` function\n",
    "  - it freezes **all** layers\n",
    "  - it casts every non-quantized 16-bit layer to FP32 to improve training\n",
    "  - it enables gradient checkpointing\n",
    "- you'll be able to unfreeze layers of your choice later on using the LoRA configuration\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "210ca55c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "210ca55c",
    "outputId": "213cddb2-18ed-4bd1-f180-aa305e88612d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decoder.embed_tokens.weight', torch.float32),\n",
       " ('decoder.embed_positions.weight', torch.float32),\n",
       " ('decoder.layers.0.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.0.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.0.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.0.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.1.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.1.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.1.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.1.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.2.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.2.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.2.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.2.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.3.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.3.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.3.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.3.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.4.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.4.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.4.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.4.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.5.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.5.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.5.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.5.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.6.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.6.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.6.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.6.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.7.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.7.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.7.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.7.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.8.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.8.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.8.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.8.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.9.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.9.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.9.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.9.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.10.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.10.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.10.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.10.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.11.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.11.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.11.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.11.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.12.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.12.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.12.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.12.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.13.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.13.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.13.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.13.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.14.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.14.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.14.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.14.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.15.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.15.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.15.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.15.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.16.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.16.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.16.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.16.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.17.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.17.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.17.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.17.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.18.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.18.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.18.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.18.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.19.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.19.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.19.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.19.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.20.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.20.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.20.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.20.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.21.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.21.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.21.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.21.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.22.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.22.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.22.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.22.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.23.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.23.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.23.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.23.final_layer_norm.bias', torch.float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainable_parms(model):\n",
    "    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n",
    "    return parms\n",
    "\n",
    "trainable_parms(model_q4.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c94df",
   "metadata": {
    "id": "d13c94df"
   },
   "source": [
    "#### `prepare_model_for_kbit_training()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a8093b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77a8093b",
    "outputId": "50697d37-e06f-4f9d-f2df-9a93916f4f4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "                                        use_gradient_checkpointing=True,\n",
    "                                        gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "709936d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "709936d7",
    "outputId": "1debb3f5-e218-48d7-dc0a-452e76213152",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_parms(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4277e665",
   "metadata": {
    "id": "4277e665"
   },
   "outputs": [],
   "source": [
    "def parms_of_dtype(model, dtype=torch.float32):\n",
    "    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n",
    "    return parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43586616",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43586616",
    "outputId": "146bc146-8847-4d08-9479-d6b0fc4b8aaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.decoder.embed_tokens.weight',\n",
       " 'model.decoder.embed_positions.weight',\n",
       " 'model.decoder.layers.0.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.0.fc1.bias',\n",
       " 'model.decoder.layers.0.fc2.bias',\n",
       " 'model.decoder.layers.0.final_layer_norm.weight',\n",
       " 'model.decoder.layers.0.final_layer_norm.bias',\n",
       " 'model.decoder.layers.1.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.1.fc1.bias',\n",
       " 'model.decoder.layers.1.fc2.bias',\n",
       " 'model.decoder.layers.1.final_layer_norm.weight',\n",
       " 'model.decoder.layers.1.final_layer_norm.bias',\n",
       " 'model.decoder.layers.2.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.2.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.2.fc1.bias',\n",
       " 'model.decoder.layers.2.fc2.bias',\n",
       " 'model.decoder.layers.2.final_layer_norm.weight',\n",
       " 'model.decoder.layers.2.final_layer_norm.bias',\n",
       " 'model.decoder.layers.3.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.3.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.3.fc1.bias',\n",
       " 'model.decoder.layers.3.fc2.bias',\n",
       " 'model.decoder.layers.3.final_layer_norm.weight',\n",
       " 'model.decoder.layers.3.final_layer_norm.bias',\n",
       " 'model.decoder.layers.4.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.4.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.4.fc1.bias',\n",
       " 'model.decoder.layers.4.fc2.bias',\n",
       " 'model.decoder.layers.4.final_layer_norm.weight',\n",
       " 'model.decoder.layers.4.final_layer_norm.bias',\n",
       " 'model.decoder.layers.5.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.5.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.5.fc1.bias',\n",
       " 'model.decoder.layers.5.fc2.bias',\n",
       " 'model.decoder.layers.5.final_layer_norm.weight',\n",
       " 'model.decoder.layers.5.final_layer_norm.bias',\n",
       " 'model.decoder.layers.6.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.6.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.6.fc1.bias',\n",
       " 'model.decoder.layers.6.fc2.bias',\n",
       " 'model.decoder.layers.6.final_layer_norm.weight',\n",
       " 'model.decoder.layers.6.final_layer_norm.bias',\n",
       " 'model.decoder.layers.7.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.7.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.7.fc1.bias',\n",
       " 'model.decoder.layers.7.fc2.bias',\n",
       " 'model.decoder.layers.7.final_layer_norm.weight',\n",
       " 'model.decoder.layers.7.final_layer_norm.bias',\n",
       " 'model.decoder.layers.8.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.8.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.8.fc1.bias',\n",
       " 'model.decoder.layers.8.fc2.bias',\n",
       " 'model.decoder.layers.8.final_layer_norm.weight',\n",
       " 'model.decoder.layers.8.final_layer_norm.bias',\n",
       " 'model.decoder.layers.9.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.9.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.9.fc1.bias',\n",
       " 'model.decoder.layers.9.fc2.bias',\n",
       " 'model.decoder.layers.9.final_layer_norm.weight',\n",
       " 'model.decoder.layers.9.final_layer_norm.bias',\n",
       " 'model.decoder.layers.10.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.10.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.10.fc1.bias',\n",
       " 'model.decoder.layers.10.fc2.bias',\n",
       " 'model.decoder.layers.10.final_layer_norm.weight',\n",
       " 'model.decoder.layers.10.final_layer_norm.bias',\n",
       " 'model.decoder.layers.11.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.11.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.11.fc1.bias',\n",
       " 'model.decoder.layers.11.fc2.bias',\n",
       " 'model.decoder.layers.11.final_layer_norm.weight',\n",
       " 'model.decoder.layers.11.final_layer_norm.bias',\n",
       " 'model.decoder.layers.12.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.12.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.12.fc1.bias',\n",
       " 'model.decoder.layers.12.fc2.bias',\n",
       " 'model.decoder.layers.12.final_layer_norm.weight',\n",
       " 'model.decoder.layers.12.final_layer_norm.bias',\n",
       " 'model.decoder.layers.13.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.13.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.13.fc1.bias',\n",
       " 'model.decoder.layers.13.fc2.bias',\n",
       " 'model.decoder.layers.13.final_layer_norm.weight',\n",
       " 'model.decoder.layers.13.final_layer_norm.bias',\n",
       " 'model.decoder.layers.14.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.14.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.14.fc1.bias',\n",
       " 'model.decoder.layers.14.fc2.bias',\n",
       " 'model.decoder.layers.14.final_layer_norm.weight',\n",
       " 'model.decoder.layers.14.final_layer_norm.bias',\n",
       " 'model.decoder.layers.15.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.15.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.15.fc1.bias',\n",
       " 'model.decoder.layers.15.fc2.bias',\n",
       " 'model.decoder.layers.15.final_layer_norm.weight',\n",
       " 'model.decoder.layers.15.final_layer_norm.bias',\n",
       " 'model.decoder.layers.16.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.16.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.16.fc1.bias',\n",
       " 'model.decoder.layers.16.fc2.bias',\n",
       " 'model.decoder.layers.16.final_layer_norm.weight',\n",
       " 'model.decoder.layers.16.final_layer_norm.bias',\n",
       " 'model.decoder.layers.17.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.17.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.17.fc1.bias',\n",
       " 'model.decoder.layers.17.fc2.bias',\n",
       " 'model.decoder.layers.17.final_layer_norm.weight',\n",
       " 'model.decoder.layers.17.final_layer_norm.bias',\n",
       " 'model.decoder.layers.18.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.18.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.18.fc1.bias',\n",
       " 'model.decoder.layers.18.fc2.bias',\n",
       " 'model.decoder.layers.18.final_layer_norm.weight',\n",
       " 'model.decoder.layers.18.final_layer_norm.bias',\n",
       " 'model.decoder.layers.19.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.19.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.19.fc1.bias',\n",
       " 'model.decoder.layers.19.fc2.bias',\n",
       " 'model.decoder.layers.19.final_layer_norm.weight',\n",
       " 'model.decoder.layers.19.final_layer_norm.bias',\n",
       " 'model.decoder.layers.20.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.20.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.20.fc1.bias',\n",
       " 'model.decoder.layers.20.fc2.bias',\n",
       " 'model.decoder.layers.20.final_layer_norm.weight',\n",
       " 'model.decoder.layers.20.final_layer_norm.bias',\n",
       " 'model.decoder.layers.21.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.21.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.21.fc1.bias',\n",
       " 'model.decoder.layers.21.fc2.bias',\n",
       " 'model.decoder.layers.21.final_layer_norm.weight',\n",
       " 'model.decoder.layers.21.final_layer_norm.bias',\n",
       " 'model.decoder.layers.22.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.22.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.22.fc1.bias',\n",
       " 'model.decoder.layers.22.fc2.bias',\n",
       " 'model.decoder.layers.22.final_layer_norm.weight',\n",
       " 'model.decoder.layers.22.final_layer_norm.bias',\n",
       " 'model.decoder.layers.23.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.23.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.23.fc1.bias',\n",
       " 'model.decoder.layers.23.fc2.bias',\n",
       " 'model.decoder.layers.23.final_layer_norm.weight',\n",
       " 'model.decoder.layers.23.final_layer_norm.bias']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms_of_dtype(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55f83018",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55f83018",
    "outputId": "f5512aee-2d35-449b-b43e-3f25d7eb3099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264.15104"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model.get_memory_footprint()/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b2537",
   "metadata": {
    "id": "6f1b2537"
   },
   "source": [
    "### PEFT\n",
    "\n",
    "\"_🤗 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware._\n",
    "\n",
    "_PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference._\"\n",
    "\n",
    "****\n",
    "**Summary of \"PEFT\"**\n",
    "- the basic configuration below should work well in many cases\n",
    "```python\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "- ranks of 8, 16, or 32 are typical, but using higher values shouldn’t significantly impact the model’s memory footprint.\n",
    "- the scaling factor, `lora_alpha` is typically twice the rank.\n",
    "- if your model has `Conv1D` layers, add `fan_in_fan_out=True` to your configuration\n",
    "- if your model was recently released, you may need to specify the `target_modules` manually\n",
    "  - typically, use the names of the massive linear layers in the attention module.\n",
    "- by default, only the adapters are trainable\n",
    "  - if you'd like to train other layers, such as layer norms, add them to the `modules_to_save` argument\n",
    "  - if you're adding your own tokens to the tokenizer, you'll need to also train vocabulary-related layers such as embeddings and the model's head\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3e78bf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3e78bf0",
    "outputId": "eb36026a-c3f0-490b-e9f3-9687e220201b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules=None, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig()\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3647c9d",
   "metadata": {
    "id": "e3647c9d"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471b64b",
   "metadata": {
    "id": "6471b64b"
   },
   "source": [
    "### `target_modules`\n",
    "\n",
    "Since there are new models and architectures being released on a weekly basis, chances are that there is no preconfigured list of target layers in your currently installed version of the PEFT library. In this case, you’ll be greeted with the following error:\n",
    "\n",
    "***\n",
    "`ValueError: Please specify `target_modules` in `peft_config``\n",
    "***\n",
    "\n",
    "Once you have the names, you can use yet another configuration argument: target_modules, which is either\n",
    "the name or a list of the names of the modules to which you want to apply the adapters.\n",
    "\n",
    "**Supported Models**\n",
    "\n",
    "If you'd like to check if a given model's architecture is already supported by the installed version of the `peft` package, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d85ac9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d85ac9d",
    "outputId": "5ca94f63-d444-45fd-d88e-96e27bd6efc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t5', 'mt5', 'bart', 'gpt2', 'bloom', 'blip-2', 'opt', 'gptj', 'gpt_neox', 'gpt_neo', 'bert', 'roberta', 'xlm-roberta', 'electra', 'deberta-v2', 'deberta', 'layoutlm', 'llama', 'llama4', 'chatglm', 'gpt_bigcode', 'mpt', 'RefinedWebModel', 'RefinedWeb', 'falcon', 'btlm', 'codegen', 'mistral', 'mixtral', 'stablelm', 'phi', 'gemma', 'gemma2', 'gemma3_text', 'qwen2', 'qwen3'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a153a16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a153a16",
    "outputId": "8dda0076-44a8-49e1-d5db-ad75d31a0ba7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'v_proj', 'fc1', 'fc2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055fc23",
   "metadata": {
    "id": "4055fc23"
   },
   "source": [
    "#### The PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bb5a74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73bb5a74",
    "outputId": "ced22e08-5cf8-4ed0-dbfa-085aa2be0477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa9ef650",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa9ef650",
    "outputId": "0f242a27-dab0-4af9-efb6-1c6a41c28f20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'v_proj']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "402db03b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402db03b",
    "outputId": "9df019a9-8a5e-4b18-9a22-99971583cfbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lora.Linear4bit(\n",
       "  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       "  (lora_magnitude_vector): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424d82f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "424d82f5",
    "outputId": "c5e4b19b-3953-4382-b885-2c83a844ece1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8e1eec1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8e1eec1",
    "outputId": "6ee8b690-acb4-496c-dfff-d200d471dd4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_parms(peft_model.base_model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565b542",
   "metadata": {
    "id": "0565b542"
   },
   "source": [
    "#### `modules_to_save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed675139",
   "metadata": {
    "id": "ed675139"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aca96ce",
   "metadata": {
    "id": "0aca96ce"
   },
   "outputs": [],
   "source": [
    "# Since the model is modified in-place, we need to unload adapters\n",
    "# from previous configuration to avoid mixing them.\n",
    "# In a regular workflow, you'd load configuration only once and\n",
    "# this wouldn't be needed.\n",
    "_ = peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43f0c0be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43f0c0be",
    "outputId": "d5547c8f-540c-4a2b-e9da-726775c164e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 332,081,152 || trainable%: 0.2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6c873",
   "metadata": {
    "id": "f3b6c873"
   },
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9414bbfa",
   "metadata": {
    "id": "9414bbfa"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm', 'embed_tokens']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5434635",
   "metadata": {
    "id": "c5434635"
   },
   "outputs": [],
   "source": [
    "# Since the model is modified in-place, we need to unload adapters\n",
    "# from previous configuration to avoid mixing them.\n",
    "# In a regular workflow, you'd load configuration only once and\n",
    "# this wouldn't be needed.\n",
    "_ = peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce91d50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ce91d50",
    "outputId": "fe59ba65-e341-4c06-9577-14e500cf8e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26,624,000 || all params: 357,820,416 || trainable%: 7.4406\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5e995e3",
   "metadata": {
    "id": "f5e995e3"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc4922bb",
   "metadata": {
    "id": "fc4922bb"
   },
   "outputs": [],
   "source": [
    "# Since the model is modified in-place, we need to unload adapters\n",
    "# from previous configuration to avoid mixing them.\n",
    "# In a regular workflow, you'd load configuration only once and\n",
    "# this wouldn't be needed.\n",
    "_ = peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f639794",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f639794",
    "outputId": "1e6bdcb7-5db2-4e59-89eb-75f57a2fd120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,192,704 || all params: 358,128,384 || trainable%: 0.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:574: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8375dd51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8375dd51",
    "outputId": "69d5b452-b09d-4633-aee3-fc263ead3e39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lora.Embedding(\n",
       "  (base_layer): Embedding(50272, 512, padding_idx=1)\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict()\n",
       "  (lora_B): ModuleDict()\n",
       "  (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x50272 (cuda:0)])\n",
       "  (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x8 (cuda:0)])\n",
       "  (lora_magnitude_vector): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.embed_tokens\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26dc0a9",
   "metadata": {
    "id": "b26dc0a9"
   },
   "source": [
    "#### Managing Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55eb913a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55eb913a",
    "outputId": "541a4200-2513-4961-c6c9-aadd0a8a2a17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.load_adapter('dvgodoy/opt-350m-lora-yoda', adapter_name='yoda')\n",
    "lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ceb7063",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ceb7063",
    "outputId": "adb6c10c-b741-415e-e2a5-1df24d3ce285"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:574: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (third): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.add_adapter(adapter_name='third', peft_config=config)\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "689d0c63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "689d0c63",
    "outputId": "5ebcac70-77f7-4340-9ef0-f6867680a647"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.delete_adapter(adapter_name='third')\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f65d0bb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f65d0bb0",
    "outputId": "ecaa2d0d-335a-4407-a261-f9b706321a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['default', 'yoda'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aed93286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aed93286",
    "outputId": "41f9f4d0-9152-477a-9ad1-a773afe9cb21"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a4a6d06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "8a4a6d06",
    "outputId": "89587b9e-f4c3-4d9a-c855-7d46fd4da1c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'yoda'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.set_adapter('yoda')\n",
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29965d0",
   "metadata": {
    "id": "c29965d0"
   },
   "source": [
    "```python\n",
    "with peft_model.disable_adapter():\n",
    "    original_outputs = peft_model(inputs)\n",
    "\n",
    "original_outputs = peft_model.base_model(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da1989ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da1989ad",
    "outputId": "bda59dcf-4d13-4d19-98fb-03064164f544"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:415: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.merge_adapter(adapter_names=['yoda'])\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2018ce5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2018ce5",
    "outputId": "4053047e-57b8-45e9-8049-8ae5457464fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTAttention(\n",
       "  (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.unload()\n",
    "peft_model.base_model.model.model.decoder.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab320816",
   "metadata": {
    "id": "ab320816"
   },
   "source": [
    "### Coming Up in \"Fine-Tuning LLMs\"\n",
    "\n",
    "Low-rank adapters saved the day by swooping in and enabling fast and cheap fine-tuning for LLMs. These humongous models, although powerful, are masters of a single trade—predicting the next token—thus remaining limited by the structure of their inputs. A new kind of input must be developed to enable these creatures to chat. Learn more about the incredible tale of chat templates in the next chapter of \"Fine-Tuning LLMs.\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
