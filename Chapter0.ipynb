{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d6bc24",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b48542",
   "metadata": {
    "id": "88b48542"
   },
   "source": [
    "## 0장 LLM 미세 튜닝 레시피\n",
    "\n",
    "사용된 패키지\n",
    "* torch 2.9.0\n",
    "* transformers 5.2.0\n",
    "* datasets 4.0.0\n",
    "* bitsandbytes 0.49.1\n",
    "* trl 0.26.2\n",
    "* peft 1.18.0\n",
    "* accelerate 1.12.0\n",
    "\n",
    "### 스포일러\n",
    "\n",
    "이 장에서는 바로 본론으로 들어가 소규모 언어 모델인 마이크로소프트의 Phi-3-mini-4k-instruct 모델을 미세 튜닝하여 영어를 요다체(Yoda-speak)로 바꾸어 보겠습니다. 이 첫 번째 장을 그냥 따라하는 일종의 레시피로 생각할 수 있습니다. \"일단 해보고 궁금한 건 나중에 알아보자\" 식의 장이죠.\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- BitsAndBytes를 사용해 양자화된 모델(quantized model)을 로드합니다.\n",
    "- 허깅 페이스(Hugging Face)의 peft를 사용해 LoRA(low-rank adapter)를 설정합니다.\n",
    "- 데이터셋을 로드하고 포맷팅합니다.\n",
    "- 허깅 페이스의 trl에서 제공하는 SFTTrainer 클래스를 사용해 모델을 지도 학습 미세 튜닝(supervised fine-tuning)합니다.\n",
    "- 미세 튜닝된 모델을 사용해 요다체 문장을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb88c3d",
   "metadata": {
    "id": "5cb88c3d"
   },
   "source": [
    "### 설정\n",
    "\n",
    "훈련 재현성을 위해 책에서 사용하는 버전과 동일한 버전을 설치합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a165c97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a165c97",
    "outputId": "d3f37fac-e5d9-4488-b7ff-4896879dfe13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, trl\n",
      "Successfully installed bitsandbytes-0.49.1 trl-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94a803",
   "metadata": {
    "id": "ab94a803"
   },
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d45274",
   "metadata": {
    "id": "b9d45274",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ffab5",
   "metadata": {
    "id": "a64ffab5"
   },
   "source": [
    "## 양자화된 베이스 모델 로드하기\n",
    "\n",
    "먼저 GPU RAM을 덜 차지하는 양자화된 모델을 로드합니다. 양자화된 모델은 원본 가중치를 더 적은 비트로 표현된 근삿값으로 바꾼 것입니다. 모델을 양자화하는 가장 간단한 방법은 가중치를 32비트 부동 소수점(FP32) 숫자에서 4비트 부동 소수점(NF4)로 바꾸는 것입니다. 간단하지만 이런 강력한 변경이 모델의 메모리 사용량을 약 8배나 줄입니다.\n",
    "\n",
    "`from_pretrained()` 메서드를 사용해 모델을 로드할 때 `quantization_config` 매개변수에 `BitsAndBytesConfig` 객체를 전달합니다. 여러 모델을 자유롭게 테스트할 수 있도록 허깅 페이스의 `AutoModelForCausalLM`를 사용하겠습니다. 이 클래스를 허깅 페이스에 있는 어떤 저장소를 선택하는지에 따라 로드되는 모델이 결정됩니다.\n",
    "\n",
    "양자화된 모델을 로드하는 코드는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ad7668",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "56400ee288d64b5792f97341a3f2a176",
      "c59b1a3210d54252baed26bce0da557a",
      "89d639d92e10423ba309b8b57e783e95",
      "09742c706b2340c981b9044ddc671773",
      "5b609b11c2fe4ad184029a1ea7ca0298",
      "10d24a30c0764decb1ec5a6f4cc6c4ad",
      "f8433d868693417ead4962436eb0e962",
      "b220f2be3fd946f78bc580f527d13db6",
      "b33cefe74dac4a2db6a2dfa5872e9bc2",
      "0ca1117b31ef47b4ba6fb3ac69933efc",
      "069cbb342c154ec0995da55d48bd2091",
      "18cf1aa2905a433a96105de234b0a7f2",
      "4c4afa88e4754d14beadb811f17a1d24",
      "865484bf70124884a5f9db2def0a4a32",
      "5266aae886e14a65be818fb70f890d2a",
      "4401201f1bff42b888bcd4741c24630b",
      "5e1424680a4a4ac6bd07e16d121d08ae",
      "fe0eea4cc71547fe9ae85a2ef9281467",
      "fe99947f858d4f1faedce40ad8c83ad2",
      "55820ca088a64c4895138b630160979a",
      "de46bc25978b4202b7e69f7672daee30",
      "7b40bed97f8241209284dfdbb944e6c1",
      "23e2e222a9654008b4eb63078cced177",
      "e7361a78e07741f283114b6bbaaaa2d0",
      "5304a46deec34d449e3b5df3a313f8cd",
      "c6430bd8cd5d4b1e94b46c2fedf5161f",
      "2c6e2d1b157e4a248fb12a762b2f1a34",
      "bd9118a3d30849699209913216c16c5b",
      "617831b633834fcdbac847e5b577666c",
      "0ee069f253da4db4adc75a305538fdb4",
      "d47f0db37bd94f8ea54bb840ad6166b1",
      "957678bcc9344c819b707d52b3961c8f",
      "d4d0125368e04988aed634cf35e79cf8",
      "3f25218ada044646901ba681375a57c2",
      "809826b882c045e4b99de8710465578a",
      "f1be8e1edc15483f820137e421a2a7c3",
      "f8403f88b2814cbf8e4c903165f1e64b",
      "786f6c922fbb4aca90b69b6b2c046c18",
      "719ea1fab68c44dca04899626989ad48",
      "f85b3fb28823485fb2cd2596a6389b9a",
      "761ee091ff8c4dfb9b726d9a79dd46f9",
      "4810d962fd4f452eb5d87c4d53113d17",
      "55d5f6e5c73a4f7fbcb0e1886a04de2d",
      "a5b9d02d2eb54f35a9e27fa8963433d6",
      "5795352a995f49f89f2949ba75291439",
      "182899140d634de5a29d2021db95b582",
      "64d4d11ffa4d41eb9ae53cd3d33febcf",
      "1ff0531aad1f46ad94c0198be82234c8",
      "2c43c2cc3f1d4ccea5c8c8a764a7a15c",
      "43dff38163ec4a8881a07950c7af44d6",
      "7836b1c1a01043a6accc350f3f3b638b",
      "f11b8b9e12d84691acd54831b94aa43f",
      "7144bcda599040d880a047266b7a6b6b",
      "b9e5d03fbceb4863aabf61d7256e215f",
      "3ea60068ad3540ef8993d93a5dfdad05",
      "0fc894adb9b042d192305651c2544908",
      "8a22153718c945d282f7cc5a27bec17b",
      "2e86d31ffc164ef6aab7692ea67c51c7",
      "87ec08cbd5b1493595d4d6dd6dc88b03",
      "f0fa33639371494baca19fe443c665ff",
      "9a50e23284cf436a8d6aa419bde6c8eb",
      "d8c548956cb048d79c83a1d3102a8a4c",
      "f2d3dd7e48c741d89fb32e9b29d304c7",
      "7520704ef1114b2ca1770bd9f3ce051c",
      "4c3c48f22c8b4d80aa3a7ae657b4e5e2",
      "b384097310e24cc8a4fc4b0dd8ba20d4",
      "221d4a641c3740629605654b974fba94",
      "2848903ba50b493f880e2e0a0e97dc55",
      "80e8f6fd05464b56937076808d1965c0",
      "8c8542d2a6014474bc36e8b8c175c890",
      "697813d8f9c543db87a27e3b3de6d8c7",
      "fcb253d773764c5ba29dcd754477b9f7",
      "73284c4bd4ab48adbb6417c2eab3d054",
      "e9fb1e26a0a24df9bbeee6dd74bb5546",
      "e4795736484d44ac97b0614e22e4cbbe",
      "1025cf6765484e429eb3163ecc70dbe6",
      "f1c16b63c01846cbbdcbfac271e04c8f"
     ]
    },
    "id": "f5ad7668",
    "outputId": "a7b7b1da-68df-4f41-9a38-5691ec09802b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56400ee288d64b5792f97341a3f2a176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cf1aa2905a433a96105de234b0a7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e2e222a9654008b4eb63078cced177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f25218ada044646901ba681375a57c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5795352a995f49f89f2949ba75291439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc894adb9b042d192305651c2544908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221d4a641c3740629605654b974fba94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f487f",
   "metadata": {
    "id": "f26f487f"
   },
   "source": [
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    <em>\"Phi-3-Mini-4K-Instruct는 38억 개의 파라미터를 가지고 있으며 Phi-3 데이터셋에서 훈련된 경량의 최신 오픈 소스 모델입니다. 이 데이터셋은 합성 데이터와 필터링된 공개 웹사이트의 데이터로 구성되며 고품질의 추론 중심 데이터입니다. 이 모델은 Phi-3 제품군에 속합니다. Mini 모델은 4K와 128K 문맥 길이(토큰 수)를 가진 버전이 있습니다.\"</em>\n",
    "    <br>\n",
    "    출처: <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">허깅 페이스</a>\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "모델이 로드된 후 `get_memory_footprint()` 메서드를 사용해 모델이 얼마만큼의 메모리를 차지하고 있는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3ac411",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3ac411",
    "outputId": "c4cde2eb-c01c-42e6-bc4f-79ed86ee555f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.341312\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f09ff7",
   "metadata": {
    "id": "24f09ff7"
   },
   "source": [
    "양자화되었더라도 모델은 2기가바이트 이상의 RAM을 차지합니다. 양자화 과정은 주로 **트랜스포머 디코더 블록**(Transformer decoder block)(종종 층이라고두 부릅니다)에 있는 **선형 층**(linear layer)을 대상으로 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc64043",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdc64043",
    "outputId": "91781b3f-062f-43fc-fe68-64927c493925"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461b229",
   "metadata": {
    "id": "1461b229"
   },
   "source": [
    "**양자화된 모델**(quantized model)은 바로 추론에 사용할 수 있지만 추가적으로 더 훈련할 수는 없습니다. 양자화의 핵심인 Linear4bit 층이 메모리를 적게 차지하지만 업데이트할 수 없기 때문입니다.\n",
    "\n",
    "대신 어댑터를 추가하여 모델의 동작을 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e6061",
   "metadata": {
    "id": "0f9e6061"
   },
   "source": [
    "## LoRA 설정하기\n",
    "\n",
    "LoRA(Low-Rank Adapter) 어댑터를 양자화된 각 층에 추가할 수 있습니다. **어댑터**(adapter)는 일반적인 Linear 층과 거의 비슷하며 쉽게 업데이트할 수 있습니다. 여기서 기발한 점은 이 어댑터가 양자화된 층보다 훨씬 작다는 것입니다.\n",
    "\n",
    "양자화된 층이 동결(frozen)되었기 때문에 (즉, 업데이트되지 않으므로) 양자화된 모델에 LoRA 어댑터를 추가했을 때 훈련 가능한 파라미터의 전체 개수가 원래의 1% (또는 그 미만으로) 크게 줄어 듭니다.\n",
    "\n",
    "LoRA 어댑터를 설정하는 단계는 세 개입니다.\n",
    "\n",
    "* 훈련 과정에서 수치 안정성을 향상시키기 위해 `prepare_model_for_kbit_training()`를 호출합니다.\n",
    "* `LoraConfig` 인스턴스를 만듭니다.\n",
    "* `get_peft_model()` 메서드를 사용해 양자화된 베이스 모델(base model)에 이 설정을 적용합니다.\n",
    "\n",
    "이 단계를 앞서 로드한 모델에 적용해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edc24ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3edc24ed",
    "outputId": "667aa131-c319-41f7-a8a0-1634530b4092"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,                   # 어댑터의 랭크(rank)가 작을수록 훈련할 파라미터가 적습니다.\n",
    "    lora_alpha=16,         # 일반적으로 2*r\n",
    "    bias=\"none\",           # 주의: 편향을 훈련하면 베이스 모델의 동작이 바뀔 수 있습니다.\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # 이 글을 쓰는 시점에 Phi-3와 같은 최신 모델은\n",
    "    # 대상 모듈을 수동으로 지정해야 합니다.\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f1cf6",
   "metadata": {
    "id": "611f1cf6"
   },
   "source": [
    "다른 LoRA 층(`qkv_proj`, `gate_up_proj`, `down_proj`)은 출력 길이를 짧게하기 위해 간단하게 나타냈습니다.\n",
    "\n",
    "<blockquote class=\"warning\">\n",
    "  <p>\n",
    "    혹시 다음과 같은 오류가 발생했나요?\n",
    "    <br>\n",
    "    <br>\n",
    "    <tt>ValueError: Please specify `target_modules` in `peft_config`</tt>\n",
    "    <br>\n",
    "    <br>\n",
    "    유명한 모델을 사용한다면 대부분 target_modules를 지정할 필요가 없습니다. peft 라이브러리가 자동으로 적절한 대상을 선택합니다. 하지만 인기있는 모델이 출시되는 시점과 라이브러리가 업데이트되는 시점 사이에는 간격이 있을 수 있습니다. 따라서 위와 같은 오류를 만났다면 모델에 있는 양자화된 층을 찾아 target_modules 매개변수에 해당 층의 이름을 입력하세요.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "양자화된 층(`Linear4bit`)이 `lora.Linear4bit`로 바뀌었습니다. 이 층은 양자화된 층인 `base_layer`와 일반적인 `Linear` 층(`lora_A`와 `lora_B`)을 섞은 것입니다.\n",
    "\n",
    "추가된 층은 모델의 크기를 조금만 더 증가시킵니다. 하지만 prepare_model_for_kbit_training() 함수가 양자화되지 않은 다른 모든 층을 **단정밀도**(single precision)(**FP32**)로 바꿉니다. 이로 인해 모델이 20% 더 커집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f30ecbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f30ecbd",
    "outputId": "05d80951-5909-4b4f-88c6-bb7ed8051904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.074752\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2718a6",
   "metadata": {
    "id": "2e2718a6"
   },
   "source": [
    "대부분의 파라미터가 동결되었기 때문에 전체 파라미터 중에서 적은 부분만 훈련됩니다. LoRA 만세!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06c42b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c06c42b4",
    "outputId": "4644c858-05c5-4cf6-905b-89d0c120bc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 가능한 파라미터:             12.58M\n",
      "총 파라미터:                 3833.66M\n",
      "훈련 가능한 파라미터의 비율: 0.33%\n"
     ]
    }
   ],
   "source": [
    "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
    "print(f'훈련 가능한 파라미터:             {trainable_parms/1e6:.2f}M')\n",
    "print(f'총 파라미터:                 {tot_parms/1e6:.2f}M')\n",
    "print(f'훈련 가능한 파라미터의 비율: {100*trainable_parms/tot_parms:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149381b5",
   "metadata": {
    "id": "149381b5"
   },
   "source": [
    "모델을 미세 튜닝할 준비를 마쳤지만 한 가지 중요한 요소인 데이터셋이 아직 준비되지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547d258",
   "metadata": {
    "id": "2547d258"
   },
   "source": [
    "## 데이터셋 포맷팅하기\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"요다 처럼, 말해라, 반드시. 흐음.\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    마스터 요다\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "[`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) 데이터셋은 영어를 요다체로 바꾼 720개 문장으로 구성되어 있습니다. 이 데이터셋은 허깅 페이스 허브(Hugging Face Hub)에서 다운받을 수 있으며 `datasets` 라이브러리의 load_dataset() 메서드로 손쉽게 로드할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3251cca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189,
     "referenced_widgets": [
      "2b6bec08673b4b1fb2c7b8ad3561bdd3",
      "75667480da8a465d8091ad4414b22ac3",
      "b432d905310b48669dec813a2356869a",
      "031c178a2528482f9c0126b41df8b9cf",
      "ebacff58ef2e4d6abcd4846939cd2445",
      "c9ed5f9b476647598f7f5bab96b98e93",
      "d4efd341c2754e7496583adafb4ab861",
      "04743fc6bf9544d3a67adb1dfb8254cb",
      "c1a3fb684c4e4b88b26d6bf9e1fe1912",
      "484b482e09134f60b5c33fd58841cebb",
      "dcf15b6e9cef459f949b1fd7f3e6f935",
      "96b94726ce6941caa5b29b5ce6622d5a",
      "0a95c85727854636b0c127a7035370b2",
      "8b2887e2a30e456a9d151a3bca963d20",
      "53d8955150ba414aa50411017baa7b47",
      "22496893f7e848c3b8349d56b681ca87",
      "0f27e81ddef34fe99ec0d3a77db85421",
      "1cf02c4ac7fc41e8b7392cab155627a5",
      "7dd8507ee4244b62a1772fe1788006d6",
      "b19be3aef2204ffca19a34bf4f73cfaa",
      "24756b6a3da646608079890b29cc5140",
      "49b1463de2e94686a34ddfdaec9f92bc",
      "70c6f7289aa04c96ba28d3fb0c1d5b19",
      "19b1fe76563a4b18b4b1f98bf7fc5fe9",
      "6a4cb0a352bf42b79504729fa9a6dd29",
      "8faffa1bab094d1d9902e9517e804092",
      "105170c2a82243a798b4b9a5a3d633a9",
      "dcec8a803e2242d897f730e40f29fb12",
      "29c01fae5a264813990cd2eec13fbb77",
      "9cd2b30c69bf467db3a8df1051a61d6c",
      "de9a20f259104c669f9e96156b187d30",
      "324e905299e04735a447b8295a622877",
      "721d7551329c4ac2be60db9a837bc17a"
     ]
    },
    "id": "a3251cca",
    "outputId": "9b8587fb-a7d0-46ac-c921-d84b7dddd041"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6bec08673b4b1fb2c7b8ad3561bdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b94726ce6941caa5b29b5ce6622d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentences.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c6f7289aa04c96ba28d3fb0c1d5b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da3b11",
   "metadata": {
    "id": "a1da3b11"
   },
   "source": [
    "이 데이터셋에는 세 개의 열이 있습니다:\n",
    "\n",
    "* 원본 영어 문장 (`sentence`)\n",
    "* 요다체로 바꾼 문장(`translation`)\n",
    "* Yesss와 Hrrmm 감탄사를 포함한 향상된 번역(`translation_extra`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c804839",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c804839",
    "outputId": "cbf2a1ce-6b97-47ee-ad0f-bec060b96951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37005b36",
   "metadata": {
    "id": "37005b36"
   },
   "source": [
    "모델을 미세 튜팅하기 위해 사용할 `SFTTrainer` 클래스가 자동으로 데이터셋을 **대화 포맷**(conversational format)으로 처리할 수 있습니다.\n",
    "\n",
    "```\n",
    "{\"messages\":[\n",
    "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
    "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
    "]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YMqs7yPqbO_R",
   "metadata": {
    "id": "YMqs7yPqbO_R"
   },
   "source": [
    "이전 버전에서는 `prompt`와 `completion` 열만 두 개만 있으면 데이터셋이 자동으로 대화 포맷으로 변환되었습니다. 이제는 더이상 이 방식이 지원되지 않으므로 직접 변환하는 것이 좋습니다. 다음에 나오는 `format_dataset()` 함수는 `trl` 패키지를 참고하여 작성한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca9e9fa",
   "metadata": {
    "id": "eca9e9fa"
   },
   "outputs": [],
   "source": [
    "# trl.extras.dataset_formatting.instructions_formatting_function을 참고함.\n",
    "# 데이터셋을 (더이상 지원되지 않는) prompt/completion 포맷에서 대화 포맷으로 변경합니다.\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P6Uqsd1ubZbl",
   "metadata": {
    "id": "P6Uqsd1ubZbl"
   },
   "source": [
    "이 함수는 `prompt`와 `completion` 열을 가진 샘플을 기대하므로 데이터셋에 적용하기 전에 열 이름을 바꾸어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "601b15b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "referenced_widgets": [
      "5e9ca9e34f1a4b7f968ee782c9e3edca",
      "3ade1e2b7203404b9692bfedfd849370",
      "20e2303c65d24469a72675151b47b8b9",
      "c35837ebf30e422b9b18c1d77748eccf",
      "d9a7c8c180964815905ab0142dfda10e",
      "8127083f1b2044989b090085ec00a56f",
      "43eab72313ab41d7a0b8f4a93ddc96f0",
      "db380bfc0fb642909f7d794c013640ec",
      "d7e858403a494a21b605881c1fcd98b4",
      "5a6657ee58244035b1f576674133423c",
      "60280b05958243d3a72c878c74b8f4c7"
     ]
    },
    "id": "601b15b7",
    "outputId": "afa62577-7462-4942-852b-3e9e0c0d595a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9ca9e34f1a4b7f968ee782c9e3edca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'},\n",
       " {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
    "messages = dataset[0]['messages']\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ab1a7",
   "metadata": {
    "id": "288ab1a7"
   },
   "source": [
    "### 토크나이저\n",
    "\n",
    "실제 훈련으로 넘어가기 전에 이 모델에 해당하는 **토크나이저**(tokenizer)를 로드해야 합니다. 토크나이저는 전체 과정에서 중요한 역할을 담당하며, 모델을 훈련할 때와 동일한 방식으로 텍스트를 토큰(token)으로 바꾸어 줍니다.\n",
    "\n",
    "지시와 채팅 모델의 경우 토크나이저에는 **채팅 템플릿**(chat template)도 담겨 있으며, 이를 통해 다음과 같은 내용을 알 수 있습니다.\n",
    "\n",
    "- 사용할 **특수 토큰**과 위치\n",
    "- 시스템 지시 사항, **사용자 프롬프트**(prompt), 모델 응답의 위치\n",
    "- **생성 프롬프트**, 즉 모델의 응답을 트리거(trigger)하는 특수 토큰(\"모델에게 질의하기\" 절에서 자세히 다룹니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faOWLpDGbvpU",
   "metadata": {
    "id": "faOWLpDGbvpU"
   },
   "source": [
    "**<중요>**\n",
    "EOS 토큰을 고유하게 유지하는 것이 중요합니다. Phi-3 같은 일부 모델에서는 EOS 토큰이 PAD 토큰으로도 사용됩니다. 이로 인해 훈련 중에 EOS 토큰이 마스킹되어 토큰 생성이 끊임없이 이어질 수 있습니다. 이 문제를 피하기 위해 UNK 토큰을 PAD 토큰으로 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a647f985",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "referenced_widgets": [
      "593e6eb883c74bd88826015f64a93b3b",
      "9dd2a9e4398e4fdc8b4b598a109885b2",
      "7a7de5607f6b45c2813eeeebdd9a144b",
      "63ec7f8c0afd4953bec14a0730b84440",
      "65f6a432597c4e3faf1065d36034ca01",
      "e07d271979ed429ca6b653b68a95df88",
      "8a7fb42d6fa9495eb44ce4d619cb43aa",
      "5e3dbb23861843bf86172fadcce40f49",
      "9468f1ef1f844e3aa33e0d374e3ae93d",
      "007a626e20c546e698c7d973c7fdbced",
      "abddca2a64ef4b19a42ce7440f89a659",
      "af4a1ad04e9641eba3da61329ea492f2",
      "df5110858b4047e1becf060c92d95021",
      "5d9636ce4c904aa48488704de36084dd",
      "59fc6279375c4169bcf9ed8e700ccac1",
      "cd40f9584d084344853f82e0a69a1a9d",
      "0888595045f54b36bef0e66c8edadcdc",
      "3cc6ab67cfbf4c37b1f683cd887d064d",
      "8d5fcf317a294a9fb853e23adf8ec021",
      "fce2a0608a7c4aa188bf2e249a24e255",
      "dc5af77a58d0404495106e996cc569ba",
      "128378c148a140a0a10122c1760ffcee",
      "7efbd93d93224cb185842d890cbfce5e",
      "3f65e42cfe8e4b2ba96a7df0ba6cbdc8",
      "0769c01c2c804af58186f8f3544964e8",
      "1717a006cb624bb9822c854f10711b0d",
      "d2118b286a0b4a328b09d4bdfa5b0248",
      "ae63ef18599f4c30b45395f91903bad7",
      "c61dbc7664804c2a8041774920655115",
      "31c86ac62e8c4742ac4460ef642c53ac",
      "f6afc1a939af45f7800d3cb61c1140b6",
      "4666a9cdfb4b422eaca2325e45efc671",
      "60954f99ad7c470f84e8f0e6943738aa",
      "9da785bbbcb9438eb78b6b1afa43777f",
      "982f3ad3274e4032923c68536f0bed67",
      "775fdd93c14944b7b055f11e6b2d49d3",
      "a2c4e6a2e4de48a5bc2d7a70e65f9f11",
      "7296b6843d9d4a45b156a9e72e032bba",
      "65b666d5e62e4465b052cf777e0329c0",
      "4b2c4d75c723413b9dba95f938e4a88a",
      "e2ba0ab419ee424792df3e8511424b23",
      "11e00cd01b0847569d1049431bc3c06f",
      "dff627076a8e406e9dec86f2c03bc6c8",
      "9249163879be404384ec24b17b62f398",
      "de30a797199249b19ae079656fe609e5",
      "6da340bc550d4a93bb23bfd64578eb0d",
      "dfc15b1d56d948e7b998afb9d94f4aae",
      "59701c3198294db982d3eb3dd73f902b",
      "2c1e10ca258f490e924a50130d29da1a",
      "a94a8e17ef324dd2b16a7c0e849818b0",
      "a0c514231e6b4083a2d06eed3c35e63f",
      "ae2af3b9112f48eba24f0eadf62dc22c",
      "53f1a6865e8147f2b0fc902267b46213",
      "9378b012d1324909be1953c2b8f5e510",
      "15c0ca2780c843fda9353eb71f60f9b1"
     ]
    },
    "id": "a647f985",
    "outputId": "398d4612-1cad-424c-eab8-c7474df4fea1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593e6eb883c74bd88826015f64a93b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4a1ad04e9641eba3da61329ea492f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efbd93d93224cb185842d890cbfce5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da785bbbcb9438eb78b6b1afa43777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de30a797199249b19ae079656fe609e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "# EOS 토큰을 고유하게 만들기 위해 UNK 토큰을 PAD 토큰으로 할당합니다.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f7d28",
   "metadata": {
    "id": "1b4f7d28"
   },
   "source": [
    "복잡해 보이는 템플릿은 신경쓰지 마세요(보기 좋도록 줄바꿈과 들여쓰기를 추가했습니다). 토크나이저는 다음처럼 메시지를 적절한 태그를 사용해 일관성 있는 블록으로 구성합니다(`tokenize=False`로 지정하면 숫자 토큰 ID의 시퀀스가 아니라 텍스트가 반환됩니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2cbfd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b2cbfd6",
    "outputId": "7c2e7a7d-6edb-4934-e07e-0bdb186c9021",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fd5c6",
   "metadata": {
    "id": "a44fd5c6"
   },
   "source": [
    "\n",
    "각 대화는 `<|user|>` 또는 `<|assistant|>`로 시작하고 `<|end|>`로 끝납니다. 또한 `<|endoftext|>`는 전체 블록의 끝을 나타냅니다.\n",
    "\n",
    "모델이 다르면 템플릿이 다르고, 문장과 블록의 시작과 끝을 나타내는 토큰이 다를 수 있습니다.\n",
    "\n",
    "이제 미세 튜닝을 수행할 준비를 마쳤습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada15ad",
   "metadata": {
    "id": "4ada15ad"
   },
   "source": [
    "## SFTTrainer를 사용해 미세 튜닝하기\n",
    "\n",
    "모델의 규모에 상관없이 미세 튜닝은 모델을 처음부터 훈련하는 것과 정확히 동일한 훈련 과정을 거칩니다. 파이토치로 훈련 루프를 직접 구현하거나 허깅 페이스의 `Trainer` 클래스를 사용해 모델을 미세 튜닝할 수 있습니다.\n",
    "\n",
    "하지만 (`Trainer`를 상속한) `SFTTrainer`를 사용하는 것이 훨씬 쉽습니다. 다음의 네 가지 매개변수 값을 제공하기만 하면 대부분의 세부 사항을 처리해 주기 때문입니다.\n",
    "\n",
    "* 모델\n",
    "* 토크나이저\n",
    "* 데이터셋\n",
    "* 설정 객체\n",
    "\n",
    "처음 세 개는 이미 준비되었으므로 마지막 설정 객체를 만들어 보죠.\n",
    "\n",
    "### SFTConfig\n",
    "\n",
    "설정 객체에는 많은 매개변수가 있습니다. 이를 네 개의 그룹으로 나누어 보죠:\n",
    "\n",
    "* 메모리 사용 최적화 매개변수는 **그레이디언트 누적**(gradient accumulation) 및 **그레이디언트 체크포인팅**(gradient checkpointing)과 관련이 있습니다.\n",
    "* `max_seq_length`와 같은 데이터셋 관련 매개변수와 시퀀스 패킹(packing) 여부\n",
    "* `learning_rate` 및 `num_train_epochs` 같은 일반적인 훈련 매개변수\n",
    "* `output_dir`(모델을 훈련한 다음 허깅 페이스 허브에 저장할 경우 모델의 이름으로 사용됩니다), `logging_dir`, `logging_steps`와 같은 환경 및 로깅 매개변수\n",
    "\n",
    "**학습률**(learning rate)이 매우 중요한 매개변수입니다(처음에는 베이스 모델 훈련에 사용한 학습률을 시도해볼 수 있습니다). 하지만 실제로는 **최대 시퀀스 길이**(maximum sequence length)가 메모리 부족 문제를 일으킬 가능성이 더 높습니다.\n",
    "\n",
    "주어진 문제에서 가능한 가장 짧은 max_seq_length를 선택하세요. 여기에서는 영어와 요다체 문장 모두 매우 짧습니다. 64개 토큰이면 프롬프트, 텍스트 완성, 특수 토큰을 포함하기에 충분합니다.\n",
    "\n",
    "<blockquote class=\"tip\">\n",
    "  <p>\n",
    "    나중에 보게 되겠지만 플래시 어텐션(flash attention)을 사용하면 메모리 부족 문제를 피하면서 더 긴 시퀀스를 사용할 수 있습니다.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "****\n",
    "**중요 업데이트**: `trl` 버전 0.20에서 `SFTConfig`에 몇 가지 중요한 변경 사항이 있습니다:\n",
    "- `packing_strategy='wrapped'`로 지정하지 않으면 패킹이 이전과 다르게 수행됩니다.\n",
    "- `max_seq_length` 매개변수가 `max_length`로 이름이 바뀌었습니다.\n",
    "- `bf16` 매개변수 기본값이 `True`이지만 이 글을 쓰는 시점(2025년 10월)에 실제로 BF16 타입이 가능한지 여부를 확인하지 앟습니다. 따라서 설정에 이 매개변수를 명시적으로 포함시킵니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9979b2",
   "metadata": {
    "id": "7a9979b2"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## 그룹 1: 메모리 사용\n",
    "    # 이 매개변수들은 GPU RAM을 최대한 활용하도록 돕습니다.\n",
    "    # 체크포인팅\n",
    "    gradient_checkpointing=True,   # 메모리가 많이 절약됩니다.\n",
    "    # 파이토치 새 버전에서 예외를 피하기 위해 지정합니다.\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # 그레이디언트 누적과 배치 크기\n",
    "    # (업데이트를 위한) 실제 배치 크기는 마이크로 배치 크기와 같습니다.\n",
    "    gradient_accumulation_steps=1,\n",
    "    # 초기 (마이크로) 배치 크기\n",
    "    per_device_train_batch_size=16,\n",
    "    # 배치 크기가 메모리 부족을 일으키면 문제가 해결될 때까지 반으로 나눕니다.\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## 그룹 2: 데이터셋 관련\n",
    "    max_length=64,\n",
    "    # 데이터셋 패킹을 한다는 것은 패딩이 필요 없다는 의미입니다.\n",
    "    packing=True,\n",
    "    packing_strategy='wrapped',\n",
    "\n",
    "    ## 그룹 3: 일반적인 훈련 매개변수\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # 8-비트 Adam 옵티마이저 - LoRA를 사용하는 경우 큰 도움이 되지 않습니다!\n",
    "    optim='paged_adamw_8bit',\n",
    "\n",
    "    ## 그룹 4: 로깅 매개변수\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none',\n",
    "\n",
    "    ## 그외\n",
    "    # 가능한 경우에만 bf16으로 훈련하도록\n",
    "    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dmxdC7iEcWP-",
   "metadata": {
    "id": "dmxdC7iEcWP-"
   },
   "source": [
    "**<중요>** 이 글을 쓰는 시점에 `trl`의 버전(0.21)은 지원 환경 여부에 상관없이 기본적으로 bf16 데이터 타입을 사용해 혼합 정밀도 훈련을 수행합니다. 문제를 방지하기 위해 GPU 지원 여부에 따라 `bf16` 매개변수를 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e3099",
   "metadata": {
    "id": "971e3099"
   },
   "source": [
    "### `SFTTrainer`\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"이제 훈련 시간이다!\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    헐크\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "****\n",
    "**중요 업데이트**: `trl` 0.23 버전짜기 LoRA 설정이 모델에 이미 적용되어 있으면 훈련이 실패하는 문제가 있었습니다. `trainer` 객체가 어댑터를 포함해 전체 모델을 동결하기 때문입니다.\n",
    "\n",
    "이 예제에서처럼 모델이 어댑터를 포함하고 있다면 원본 모델(`model.base_model.model`)과 `peft_config` 매개변수를 함께 사용해야 훈련이 됩니다.\n",
    "\n",
    "이 문제는 2025년 10월 릴리스된 `trl` 0.23.1 버전에서 수정되었습니다.\n",
    "****\n",
    "\n",
    "드디어 지도 학습 미세 튜닝을 위한 훈련 객체를 만들 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f194a639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "d55dcb7f82434c6fb1f57a61b4222153",
      "f6fc6af981a24fef8f50c252ffc51d5b",
      "868fa516193c4b4e928f0ce3d69f9051",
      "fc3c3a8d0cf948f8842a8ac955556c35",
      "5651ad52e6f04c9aaa4dce3b50e8a37c",
      "a9aa02d71bab48f7ba0b56ccb08a7685",
      "c28c868f86b84d1aaf4cb5ba303dd48a",
      "b1f485e20edc47259132ec576b440f0b",
      "aa3e06f8caad45eb8572f76f454df1ad",
      "f714370e5cbd4b998f3f2fb8d549c5f2",
      "dfb1055c311e42fab548b5b6226f5cbb",
      "5a9c5fdf197340d9aa281d100e734e80",
      "bba1ab860117488792c2ba73d2bfa661",
      "ed9ae88531ad4578b8718fc901b1413d",
      "8ff2f018a876455e8b265d1b982e0c73",
      "86c55a0978d944d2a12d69fd617be072",
      "e276c5c349dc42e891d70275b287f45a",
      "3c6bed737f5d41ca8302601822821ef8",
      "e10e984a921f47519bd2c53181e711bd",
      "9d2bd84a3615444b864e96dd88230e00",
      "b4346909bd7b444998c89e89c39e2594",
      "3e7faaa30cef4b74a95499678ea99b47"
     ]
    },
    "id": "f194a639",
    "outputId": "908ea93c-cc0b-479f-cbf8-4ca2db07aa3c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55dcb7f82434c6fb1f57a61b4222153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9c5fdf197340d9aa281d100e734e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c4efd",
   "metadata": {
    "id": "105c4efd"
   },
   "source": [
    "`SFTTrainer`에 사전에 전처리된 데이터셋을 전달했으므로 미니 배치가 어떻게 구성되었는지 확인해 볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ffc0403",
   "metadata": {
    "id": "1ffc0403"
   },
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df",
   "metadata": {
    "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df"
   },
   "source": [
    "레이블(label)을 확인해 보죠. 무엇보다도 앞에서 직접 레이블을 지정하지 않았습니다. 그렇죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfaf09e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfaf09e2",
    "outputId": "88af506e-758c-4578-e2a4-be430cf11a2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "         32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "          5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "          1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "           935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "         32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "         29889, 32007, 32001,   512], device='cuda:0'),\n",
       " tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "         32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "          5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "          1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "           935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "         32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "         29889, 32007, 32001,   512], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedef238",
   "metadata": {
    "id": "eedef238"
   },
   "source": [
    "입력과 정확하게 동일한 값으로 레이블이 자동으로 추가되었습니다. 따라서 이는 **자기 지도 학습 미세 튜닝**(self-supervised fine-tuning)입니다.\n",
    "\n",
    "레이블을 하나씩 이동시키는 것도 자동으로 처리되기 때문에 이에 대해 신경쓸 필요가 없습니다.\n",
    "\n",
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    38억 개의 파라미터를 가진 모델이지만 앞의 설정을 사용하면 6GB RAM의 GTX 1060 같은 개인용 GPU에서 8개의 미니 배치로 훈련을 수행할 수 있습니다. 정말이에요!\n",
    "    <br>\n",
    "    이 경우 훈련을 완료하는데 약 35분이 걸립니다.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "그다음 `train()` 메서드를 호출합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12a099d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "id": "b12a099d",
    "outputId": "f6345d19-fff0-4f88-d012-573d2d33b278"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 01:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.901700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.8352644606070085, metrics={'train_runtime': 110.7509, 'train_samples_per_second': 30.79, 'train_steps_per_second': 1.986, 'total_flos': 4890970340720640.0, 'train_loss': 0.8352644606070085, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b666a",
   "metadata": {
    "id": "2f4b666a"
   },
   "source": [
    "## 모델에게 질의하기\n",
    "\n",
    "이제 모델에게 짧은 문장을 제공하면 요다 말투의 문장이 생성되어야 합니다.\n",
    "\n",
    "이 모델은 적절하게 포맷팅된 입력이 필요합니다. (이 경우 `user`에 해당하는) 메시지 리스트를 만들고 모델이 대답할 차례라는 것을 알려 주어야 합니다.\n",
    "\n",
    "이것이 `add_generation_prompt` 매개변수의 목적입니다. 대화의 끝에 `<|assistant|>`를 추가하여 모델이 다음 단어를 예측하게 하고 `<|endoftext|>` 토큰이 예측될 때까지 계속 진행합니다.\n",
    "\n",
    "다음 헬퍼 함수는 (대화 포맷으로) 메시지를 만들고 채팅 템플릿을 적용한 후 마지막에 특수 토큰 `<|assistant|>`를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5854efcd",
   "metadata": {
    "id": "5854efcd"
   },
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ba48b",
   "metadata": {
    "id": "438ba48b"
   },
   "source": [
    "샘플 문장으로 프폼프트를 생성해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "480a479b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "480a479b",
    "outputId": "ef25c8a7-f107-4d52-cdb5-35cef5c2e020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The Force is strong in you!<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773df8d",
   "metadata": {
    "id": "4773df8d"
   },
   "source": [
    "프롬프트가 제대로 만들어진 것 같습니다. 이를 사용해 응답을 생성해 보죠. `generate()` 함수는 다음과 같은 작업을 합니다:\n",
    "\n",
    "* 프롬프트를 토큰화하여 토큰 ID의 텐서를 만듭니다(채팅 템플릿으로 특수 토큰을 이미 추가했으므로 `add_special_tokens`를 `False`로 지정합니다).\n",
    "* 모델을 **평가 모드**(`evaluation mode`)로 설정합니다.\n",
    "* 모델의 `generate()` 메서드를 호출하여 출력(토큰 ID)을 생성합니다.\n",
    "  * 모델이 혼합 정밀도로 훈련되었다면 데이터 타입 간의 변환을 자동으로 처리하기 위해 `autocast()` 컨텍스트 관리자로 생성 과정을 감쌉니다.\n",
    "* 생성된 토큰 ID를 텍스트로 디코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65a53ea9",
   "metadata": {
    "id": "65a53ea9"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(prompt, add_special_tokens=False,\n",
    "                                return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(tokenized_input)\n",
    "    model.eval()\n",
    "    # 혼합 정밀도를 사용해 훈련하는 경우 autocast 컨택스트를 사용합니다.\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
    "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:\n",
    "        generation_output = model.generate(**tokenized_input,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           max_new_tokens=max_new_tokens)\n",
    "\n",
    "    output = tokenizer.batch_decode(generation_output,\n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6b3bf",
   "metadata": {
    "id": "e1f6b3bf"
   },
   "source": [
    "이제 모델이 실제로 요다체 문장을 생성하는지 테스트해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33dca746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33dca746",
    "outputId": "968730d2-943b-4dec-d24a-ea6367f7667f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[32010,   450, 11004,   338,  4549,   297,   366, 29991, 32007, 32001]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is! Yes, hrrrm.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e821c3",
   "metadata": {
    "id": "56e821c3"
   },
   "source": [
    "훌륭하네요! 잘 됩니다! 요다처럼, 모델이 말하네요. 흐음..\n",
    "\n",
    "축하합니다. 첫 번째 LLM을 미세 튜닝했습니다!\n",
    "\n",
    "Phi-3-mini-4k-instruct 모델에 작은 어댑터를 추가하여 요다 번역기를 만들었습니다! 멋지지 않나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dc809",
   "metadata": {
    "id": "898dc809"
   },
   "source": [
    "### 어댑터 저장하기\n",
    "\n",
    "훈련이 완료된 후 `Trainer` 객체의 `save_model()` 메서드를 호출해 어댑터를 디스크에 저장할 수 있습니다. 이 메서드는 지정된 폴더에 모든 것을 저장합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b41a86d2",
   "metadata": {
    "id": "b41a86d2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('local-phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae1c2d",
   "metadata": {
    "id": "aaae1c2d"
   },
   "source": [
    "다음과 같은 파일들이 저장됩니다:\n",
    "\n",
    "* 어댑터 설정(`adapter_config.json`)과 가중치(`adapter_model.safetensors`). 어댑터 자체 크기는 50MB에 불과합니다.\n",
    "* 훈련 매개변수값(`training_args.bin`)\n",
    "* 토크나이저(`tokenizer.json`와 `tokenizer.model`)와 설정(`tokenizer_config.json`), 특수 토큰(`added_tokens.json`와 `speciak_tokens_map.json`)\n",
    "* README 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "761d1ba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "761d1ba3",
    "outputId": "d3c12164-cb75-413c-a38c-b0c6ab804a4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adapter_config.json',\n",
       " 'README.md',\n",
       " 'training_args.bin',\n",
       " 'chat_template.jinja',\n",
       " 'tokenizer.json',\n",
       " 'tokenizer_config.json',\n",
       " 'tokenizer.model',\n",
       " 'added_tokens.json',\n",
       " 'adapter_model.safetensors',\n",
       " 'special_tokens_map.json']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('local-phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014134ea",
   "metadata": {
    "id": "014134ea"
   },
   "source": [
    "이 장에서 만든 어댑터를 다른 사람과 공유하고 싶다면 이를 허깅 페이스 허브에 업로드할 수 있습니다. 먼저 쓰기 권한이 있는 액세스 토큰(access token)을 사용해 허깅 페이스에 로그인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33c1005a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets": [
      "9ddf449a13c84aceaaea370288c1332d",
      "f43f5ada55aa49c68a66b75f32a1e747",
      "b23a766af3544ebab89b081c3db2bc7e",
      "0b5cd2d156094f2b9280025987dd54eb",
      "054e8f9664eb4a8fab03e818cabdb98d",
      "e25492201b3c455dab2000f55dc8f372",
      "c87df46eccab4966800914eea5f06daa",
      "652bc53759c9426bb7b29237c6318755",
      "917c3ca3630f412fb42ec9e6cc62df44",
      "3827014f7ace4154b792d5bbe817c9a8",
      "b329ce3f9c6a464b83a1310e63c23f3d",
      "124bd3d7669f4978a19be0d1c5543ef8",
      "3dcc25a70b1349d884c98e04f55be772",
      "7d47f848cb344a0cadd2d4d2324ecbb3",
      "2b7a2d5680b44a838c2a109077488b5f",
      "4dd67cf6eb5b408bbf007cac9d91f524",
      "035489fc8c3740be8937980d5e97e46d"
     ]
    },
    "id": "33c1005a",
    "outputId": "ff235930-faec-488a-9ac6-2a83ae098b70"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddf449a13c84aceaaea370288c1332d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5de92",
   "metadata": {
    "id": "38c5de92"
   },
   "source": [
    "위 코드를 실행하면 다음과 같이 액세스 토큰을 요청합니다:\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub0.png?raw=True)\n",
    "<center>Figure 0.1 - Logging into the Hugging Face Hub</center>\n",
    "\n",
    "A successful login should look like this (pay attention to the permissions):\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub1.png?raw=True)\n",
    "<center>Figure 0.2 - Successful Login</center>\n",
    "\n",
    "그다음 `trainer` 객체의 `push_to_hub()` 메서드를 사용해 자신의 허브 계정에 모두 업로드할 수 있습니다. 모델 이름은 훈련 매개변수 `output_dir` 값에 따라 정해집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e11e931a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "6f96a573223847fd9068fac00a473f9e",
      "31148298bf45459688491bcf8bf37fd8",
      "64c85ca5506440d186097dc7027741e1",
      "db3c5716ab9f4e73a92a28f715bc59b9",
      "26310c0297534dd7b6cf6b4872d1d21d",
      "13100e22afd740bab4138f8a2000e934",
      "d04b7f3339f54d3f8642f88916c1e54f",
      "8b7690df45e641029e99e294c78795b5",
      "472b938549324aedab6d132e2ad7e008",
      "832a5b6aacd146deb372cf4bc216d945",
      "7d6109017d3148de8c4e0eeee2140b1f",
      "db31b835bc134da19ed2c64f2307f04b",
      "8ab1b1825fee48c5be0e4e0fb20382ce",
      "bdbc9a738737409c93be15de656f99b5",
      "c358f095159c486bb8c363ae90508168",
      "49fd4e4e8aec43319df7a84f2a6c511a",
      "d6ea5dba1a9e45c88158cfdd92a135e0",
      "895112a1c12d4639b8426d3c50f0a723",
      "e77a794769f24861b2a7ca8e36c51f96",
      "d3ba2e869b8541b0af99b2b57e2c07e6",
      "7f1c0d6ca35a4c4bac012773357191fe",
      "764620eef43e4b7c82d4e71cff734fc7",
      "a45831b5f9074ce5a57b082fdc4f0d4a",
      "f8854f4e4dcf432c89026995c2e252ea",
      "89c474384dd44969964df877dad99efe",
      "36f6d994dea54548ab6d9135472c5475",
      "2bfe06c69e5945c6a125e2c2a041591a",
      "86def9ab140842a7b004add7ff58fbb4",
      "3770f926ac944ee2a8298d211f854331",
      "445781dcf64543e18f396822476cee5d",
      "53877c8218c74b7ab1416b64a31d78a4",
      "33f11ffc77a84d23b6033de8d80df086",
      "71dcc207533049b7a2c272a72771ba08",
      "bdafce2cc69a46ef90548bbe84b2b2b2",
      "2072b621d352454184e85f8b141577d9",
      "54b21ba9f9dd472ca3c196024aacfe13",
      "478e66f2fbaf43c69de3a45e1a2dd407",
      "edb72e9068dc409cb14641c10d07f179",
      "37db5191fa214d749025ee8ecc80a0aa",
      "10080dc006b94da6b4cf2636366d392e",
      "1c2732ce43cf47d1882879d9ec19ed2f",
      "2cad9ceaa5b14424b271b5f4b9996af6",
      "bbb142570d6c41e9b478bbaad195025c",
      "d854847ef5e84120a2e9ba1318f706e6",
      "a2bf6a45bb284c729a6da0f94d6998be",
      "0ee3cda8b9a7489eb89622da6f28f370",
      "57453fafa4be4e83acbf875ad80e5c53",
      "9c94a2272aa34790ad733a40a660fbdc",
      "03057448112e4f1680944c045fd09997",
      "2f937c8b76eb4ffeafba65dd0e74d674",
      "e5db3f285c9a45cd92719e981e974442",
      "91ed0367c2c14b8fb0d4f649b8d66e3c",
      "0a00353786fd42eab1b298732c3aa95f",
      "12c81b9f8c244cabb6b50c8029ed76ea",
      "6a6fab70e0784428b9861460851b6c05"
     ]
    },
    "id": "e11e931a",
    "outputId": "d5fb4b0c-59ab-4373-e0d3-5e06dc7d70a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f96a573223847fd9068fac00a473f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db31b835bc134da19ed2c64f2307f04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45831b5f9074ce5a57b082fdc4f0d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter_model.safetensors:   0%|          | 27.8kB / 25.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdafce2cc69a46ef90548bbe84b2b2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...a-adapter/tokenizer.model:  75%|#######4  |  374kB /  500kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bf6a45bb284c729a6da0f94d6998be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter/training_args.bin:   2%|2         |   152B / 6.22kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/haesun/phi3-mini-yoda-adapter/commit/a672452f5aee83d00db47f6c8fd83952e51ad489', commit_message='End of training', commit_description='', oid='a672452f5aee83d00db47f6c8fd83952e51ad489', pr_url=None, repo_url=RepoUrl('https://huggingface.co/haesun/phi3-mini-yoda-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='haesun/phi3-mini-yoda-adapter'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca8a42",
   "metadata": {
    "id": "a5ca8a42"
   },
   "source": [
    "이게 끝입니다! 모델이 세상에 공개되었고 누구나 이 모델을 사용해 영어 문장을 요다체로 바꿀 수 있습니다!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
