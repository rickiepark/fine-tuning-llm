{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248c5d8e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b48542",
   "metadata": {
    "id": "88b48542"
   },
   "source": [
    "## 0장 LLM 미세 튜닝 레시피\n",
    "\n",
    "### 스포일러\n",
    "\n",
    "이 장에서는 바로 본론으로 들어가 소규모 언어 모델인 마이크로소프트의 Phi-3-mini-4k-instruct 모델을 미세 튜닝하여 영어를 요다체(Yoda-speak)로 바꾸어 보겠습니다. 이 첫 번째 장을 그냥 따라하는 일종의 레시피로 생각할 수 있습니다. \"일단 해보고 궁금한 건 나중에 알아보자\" 식의 장이죠.\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- BitsAndBytes를 사용해 양자화된 모델(quantized model)을 로드합니다.\n",
    "- 허깅 페이스(Hugging Face)의 peft를 사용해 LoRA(low-rank adapter)를 설정합니다.\n",
    "- 데이터셋을 로드하고 포맷팅합니다.\n",
    "- 허깅 페이스의 trl에서 제공하는 SFTTrainer 클래스를 사용해 모델을 지도 학습 미세 튜닝(supervised fine-tuning)합니다.\n",
    "- 미세 튜닝된 모델을 사용해 요다체 문장을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb88c3d",
   "metadata": {
    "id": "5cb88c3d"
   },
   "source": [
    "### 설정\n",
    "\n",
    "훈련 재현성을 위해 책에서 사용하는 버전과 동일한 버전을 설치합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a165c97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2a165c97",
    "outputId": "602dc24c-7a53-4abd-979e-b40325c129db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.46.2\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.13.2\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate==1.1.1\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.12.1\n",
      "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting bitsandbytes==0.45.2\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting datasets==3.1.0\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting huggingface-hub==0.26.2\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors==0.4.5\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Collecting matplotlib==3.8.0\n",
      "  Downloading matplotlib-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.2) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.2) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.2)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (2.6.0+cu124)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.70.15)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (4.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (3.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.2) (2025.6.15)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.2)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.13.2) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.12.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.13.2) (3.0.2)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, matplotlib, transformers, datasets, bitsandbytes, accelerate, trl, peft\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.0\n",
      "    Uninstalling matplotlib-3.10.0:\n",
      "      Successfully uninstalled matplotlib-3.10.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.0\n",
      "    Uninstalling transformers-4.53.0:\n",
      "      Successfully uninstalled transformers-4.53.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.15.2\n",
      "    Uninstalling peft-0.15.2:\n",
      "      Successfully uninstalled peft-0.15.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.26.2 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.26.2 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.1.1 bitsandbytes-0.45.2 datasets-3.1.0 fsspec-2024.9.0 huggingface-hub-0.26.2 matplotlib-3.8.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.13.2 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2 trl-0.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "6225208ebcde480ab124d35952242061",
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 코랩에서 오류를 피하기 위해 bitsandbytes 0.45.2 버전을 설치해야 합니다.\n",
    "!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94a803",
   "metadata": {
    "id": "ab94a803"
   },
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d45274",
   "metadata": {
    "id": "b9d45274",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ffab5",
   "metadata": {
    "id": "a64ffab5"
   },
   "source": [
    "## 양자화된 베이스 모델 로드하기\n",
    "\n",
    "먼저 GPU RAM을 덜 차지하는 양자화된 모델을 로드합니다. 양자화된 모델은 원본 가중치를 더 적은 비트로 표현된 근삿값으로 바꾼 것입니다. 모델을 양자화하는 가장 간단한 방법은 가중치를 32비트 부동 소수점(FP32) 숫자에서 4비트 부동 소수점(NF4)로 바꾸는 것입니다. 간단하지만 이런 강력한 변경이 모델의 메모리 사용량을 약 8배나 줄입니다.\n",
    "\n",
    "`from_pretrained()` 메서드를 사용해 모델을 로드할 때 `quantization_config` 매개변수에 `BitsAndBytesConfig` 객체를 전달합니다. 여러 모델을 자유롭게 테스트할 수 있도록 허깅 페이스의 `AutoModelForCausalLM`를 사용하겠습니다. 이 클래스를 허깅 페이스에 있는 어떤 저장소를 선택하는지에 따라 로드되는 모델이 결정됩니다.\n",
    "\n",
    "양자화된 모델을 로드하는 코드는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ad7668",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "ae6df62151ee4c1ebe590866ca1fa0c4",
      "8374fc5f8d4744a3879ea4b088daf72a",
      "726a24994c614750b9c694cf3e541e04",
      "1be863b4af464b24956f342c33959faa",
      "22424297ed2e4e08b0fa89b70ff35308",
      "65628a602eea4e449c60a99a5a446056",
      "10ddb45ac172414dbbf94e39d08b81c2",
      "40b074af458345cb9ff32ca72a382487",
      "498403a30b7b4e1e90539514f33d57a0",
      "852cee2bce854a8da8077f7638215e71",
      "50253d6d10334322b25908ea9c630101",
      "2089aa18cd94420a834b7714b4540570",
      "7d2b4ef67efb4afdaeb2cd01f2450b64",
      "7e5853438514499e95b00e1a43306927",
      "6601ed0f6a7d44068bd24e4f82b7c7b2",
      "1008651a2f9a48e69847100a4efc3e3c",
      "234b0134da6341e0a07471bbfae62528",
      "9ce5d1e29fb74492be407c8cf29d5c22",
      "40400c378dee43078416186699755dca",
      "b8f1d4b27a2249b38511eafdc87f0b40",
      "64ad0d7e255c4ac484f943c9ce212c09",
      "1606fb85cefc4109b7ca397785eb71dc",
      "4debd6491ed44fbd864dd3080338eb1e",
      "85302b883f204c5bab06847ee3540f1b",
      "63d6281d83fd4a7c8ca25b0b5caa036b",
      "14a4272c8da7487b84e191bcaa8cdbfc",
      "5e4edd8c5f184928af95ef2aee26f169",
      "a13b6b5176ce4aef8c7b2756d6b82d70",
      "f99e5f5df7e94a9caa437a317a2c48ae",
      "e0cdcce2e33f40449a5fe6454691b5ef",
      "63ade91910c34bf3a2cd2cd25e5d6afb",
      "de67c23e999045c4b366ca09072c0109",
      "a52779ba877e497fb1ed8045637a2496",
      "9769fdc6732348b79014a342c877ba4a",
      "22a5ea1b68754f629ebe698c1af90363",
      "991a7b1ffb7d45ed981d1859167df13f",
      "85097bac29fd4f36916e732fe659589f",
      "84dda30502f14ede9d1f2df54e0b758f",
      "5b5fb795824c48de9fb0480620d89b26",
      "bc9b869650c6417e86f8648f2ca407d3",
      "202030f4844a43958c050ba08f493956",
      "3c32070deac7400baadfc69a45bf052c",
      "e89ab6b114184438a5ade3ee3b76c84a",
      "a753c9de1fd44606b626ea4668c74356",
      "c24836785ddf40dcab38b791e9d4c35f",
      "aa5407bc79f5438f89c108fe6e85189f",
      "ec384fb1f3144e5bbeb1c203d0b29cce",
      "5b933f7ea24e4d7b9441f8ead3669542",
      "7affb2afa941443fa45659ec1d2e1bf9",
      "0e30c2a637ce42bb877a201dc52471b9",
      "088773b4157548aba030e55d2879ba17",
      "80da2c20324947ac8a8ad07f5db96841",
      "3986ba241c5444eab8302b8acb56043e",
      "69f64e1aee7c4f1a97bfd96ab83d36bc",
      "4fc68fff140d4592801869a8ff0f346a",
      "97260c5d08b1468cac47addf7daec6bc",
      "646bbc7f8290404799e1901e71c764c0",
      "65c8a59acec9449a8605c4a069a4f314",
      "0b3e16eae7544620849b77264b167efb",
      "d81ecd76dc2e4e8cbb3a3f34a1593047",
      "7beba3cb90f440f58281601c91335dd5",
      "1382275b0dcd40be856fae9126c937ed",
      "7480401acc5e4d0da2eb9f1e9181ad08",
      "24de82a2dc5d4ee7869c221e5cc49b31",
      "5297194ff4164148b3a40848f669e704",
      "340873b2076344aabd374f3e363b609d",
      "752b0167950f4b9687da4aa07874558a",
      "d61c9778b443408e8bdf0ffd1ff95c1e",
      "369f566b64614355b5c0850d3f0a2158",
      "931d340b801f46d286c206f8c7a10cf7",
      "bb7eeb8144974e42a6847a040e94d8f0",
      "b5659f44ed1b4b79a9cb4f8664b8566b",
      "3ff9ed9411724175bc198d4dc12c1b69",
      "2a1526a05b1647b4acc788e2368486aa",
      "4bc9bba56a574f458a3cb7d26901d269",
      "1ca28060e9f741f79f9d6f7bc7bb6339",
      "b3eaf8bc7c7345738f2563048cb696a0"
     ]
    },
    "id": "f5ad7668",
    "outputId": "24a0eb9d-62f7-432b-da46-205688bc32e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6df62151ee4c1ebe590866ca1fa0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2089aa18cd94420a834b7714b4540570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4debd6491ed44fbd864dd3080338eb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9769fdc6732348b79014a342c877ba4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24836785ddf40dcab38b791e9d4c35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97260c5d08b1468cac47addf7daec6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752b0167950f4b9687da4aa07874558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f487f",
   "metadata": {
    "id": "f26f487f"
   },
   "source": [
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    <em>\"Phi-3-Mini-4K-Instruct는 38억 개의 파라미터를 가지고 있으며 Phi-3 데이터셋에서 훈련된 경량의 최신 오픈 소스 모델입니다. 이 데이터셋은 합성 데이터와 필터링된 공개 웹사이트의 데이터로 구성되며 고품질의 추론 중심 데이터입니다. 이 모델은 Phi-3 제품군에 속합니다. Mini 모델은 4K와 128K 문맥 길이(토큰 수)를 가진 버전이 있습니다.\"</em>\n",
    "    <br>\n",
    "    출처: <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">허깅 페이스</a>\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "모델이 로드된 후 `get_memory_footprint()` 메서드를 사용해 모델이 얼마만큼의 메모리를 차지하고 있는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3ac411",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3ac411",
    "outputId": "af0a604f-e3dc-45e7-bd6f-f2bc4911eb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.347264\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f09ff7",
   "metadata": {
    "id": "24f09ff7"
   },
   "source": [
    "양자화되었더라도 모델은 2기가바이트 이상의 RAM을 차지합니다. 양자화 과정은 주로 **트랜스포머 디코더 블록**(Transformer decoder block)(종종 층이라고두 부릅니다)에 있는 **선형 층**(linear layer)을 대상으로 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc64043",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdc64043",
    "outputId": "4e76ed5b-d7e2-448a-f73e-ac6fa00e99da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3SdpaAttention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461b229",
   "metadata": {
    "id": "1461b229"
   },
   "source": [
    "**양자화된 모델**(quantized model)은 바로 추론에 사용할 수 있지만 추가적으로 더 훈련할 수는 없습니다. 양자화의 핵심인 Linear4bit 층이 메모리를 적게 차지하지만 업데이트할 수 없기 때문입니다.\n",
    "\n",
    "대신 어댑터를 추가하여 모델의 동작을 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e6061",
   "metadata": {
    "id": "0f9e6061"
   },
   "source": [
    "## LoRA 설정하기\n",
    "\n",
    "LoRA(Low-Rank Adapter) 어댑터를 양자화된 각 층에 추가할 수 있습니다. **어댑터**(adapter)는 일반적인 Linear 층과 거의 비슷하며 쉽게 업데이트할 수 있습니다. 여기서 기발한 점은 이 어댑터가 양자화된 층보다 훨씬 작다는 것입니다.\n",
    "\n",
    "양자화된 층이 동결(frozen)되었기 때문에 (즉, 업데이트되지 않으므로) 양자화된 모델에 LoRA 어댑터를 추가했을 때 훈련 가능한 파라미터의 전체 개수가 원래의 1% (또는 그 미만으로) 크게 줄어 듭니다.\n",
    "\n",
    "LoRA 어댑터를 설정하는 단계는 세 개입니다.\n",
    "\n",
    "* 훈련 과정에서 수치 안정성을 향상시키기 위해 `prepare_model_for_kbit_training()`를 호출합니다.\n",
    "* `LoraConfig` 인스턴스를 만듭니다.\n",
    "* `get_peft_model()` 메서드를 사용해 양자화된 베이스 모델(base model)에 이 설정을 적용합니다.\n",
    "\n",
    "이 단계를 앞서 로드한 모델에 적용해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edc24ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3edc24ed",
    "outputId": "e2c9fcf9-6d14-4e63-931c-fc4c985dc338"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3SdpaAttention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,                   # 어댑터의 랭크(rank)가 작을수록 훈련할 파라미터가 적습니다.\n",
    "    lora_alpha=16,         # 일반적으로 2*r\n",
    "    bias=\"none\",           # 주의: 편향을 훈련하면 베이스 모델의 동작이 바뀔 수 있습니다.\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # 이 글을 쓰는 시점에 Phi-3와 같은 최신 모델은 \n",
    "    # 대상 모듈을 수동으로 지정해야 합니다.\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f1cf6",
   "metadata": {
    "id": "611f1cf6"
   },
   "source": [
    "다른 LoRA 층(`qkv_proj`, `gate_up_proj`, `down_proj`)은 출력 길이를 짧게하기 위해 간단하게 나타냈습니다.\n",
    "\n",
    "<blockquote class=\"warning\">\n",
    "  <p>\n",
    "    혹시 다음과 같은 오류가 발생했나요?\n",
    "    <br>\n",
    "    <br>\n",
    "    <tt>ValueError: Please specify `target_modules` in `peft_config`</tt>\n",
    "    <br>\n",
    "    <br>\n",
    "    유명한 모델을 사용한다면 대부분 target_modules를 지정할 필요가 없습니다. peft 라이브러리가 자동으로 적절한 대상을 선택합니다. 하지만 인기있는 모델이 출시되는 시점과 라이브러리가 업데이트되는 시점 사이에는 간격이 있을 수 있습니다. 따라서 위와 같은 오류를 만났다면 모델에 있는 양자화된 층을 찾아 target_modules 매개변수에 해당 층의 이름을 입력하세요.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "양자화된 층(`Linear4bit`)이 `lora.Linear4bit`로 바뀌었습니다. 이 층은 양자화된 층인 `base_layer`와 일반적인 `Linear` 층(`lora_A`와 `lora_B`)을 섞은 것입니다.\n",
    "\n",
    "추가된 층은 모델의 크기를 조금만 더 증가시킵니다. 하지만 prepare_model_for_kbit_training() 함수가 양자화되지 않은 다른 모든 층을 **단정밀도**(single precision)(**FP32**)로 바꿉니다. 이로 인해 모델이 20% 더 커집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f30ecbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f30ecbd",
    "outputId": "71be5d21-2e32-4aa9-ced2-d635ab6b17da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.080704\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2718a6",
   "metadata": {
    "id": "2e2718a6"
   },
   "source": [
    "대부분의 파라미터가 동결되었기 때문에 전체 파라미터 중에서 적은 부분만 훈련됩니다. LoRA 만세!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06c42b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c06c42b4",
    "outputId": "9041045a-87d1-4950-cf1f-e026d9edcc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:             12.58M\n",
      "Total parameters:                 3833.66M\n",
      "Fraction of trainable parameters: 0.33%\n"
     ]
    }
   ],
   "source": [
    "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
    "print(f'훈련 가능한 파라미터:             {trainable_parms/1e6:.2f}M')\n",
    "print(f'총 파라미터:                 {tot_parms/1e6:.2f}M')\n",
    "print(f'훈련 가능한 파라미터의 비율: {100*trainable_parms/tot_parms:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149381b5",
   "metadata": {
    "id": "149381b5"
   },
   "source": [
    "모델을 미세 튜닝할 준비를 마쳤지만 한 가지 중요한 요소인 데이터셋이 아직 준비되지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547d258",
   "metadata": {
    "id": "2547d258"
   },
   "source": [
    "## 데이터셋 포맷팅하기\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"요다 처럼, 말해라, 반드시. 흐음.\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    마스터 요다\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "[`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) 데이터셋은 영어를 요다체로 바꾼 720개 문장으로 구성되어 있습니다. 이 데이터셋은 허깅 페이스 허브(Hugging Face Hub)에서 다운받을 수 있으며 `datasets` 라이브러리의 load_dataset() 메서드로 손쉽게 로드할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3251cca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189,
     "referenced_widgets": [
      "d96a5b50486c4a2aab7486ccdf61c147",
      "dd32eb6265c441d29f2d71f32fb488ad",
      "260eafcd641e476187c9ed62db631aec",
      "e37ef260d0a045c1a21eb4e9e1231094",
      "f1dd5d3f2b854c84897994c4595abf12",
      "e871a3438ce144dbb6f769560a49d739",
      "22897563fde04377a24813b8fed7f6aa",
      "9b746cc31ed74382a81a787ee26addc3",
      "45884701b61043fb9e625edf7a2cda5a",
      "f7c478341e6a4001bfd5c2e1ee7c91fc",
      "e0b99650470b4c6b887ef9e2254a1125",
      "88aec4a0b7b142cd9b0a00c6c54b8d8f",
      "985a1f362ea34ba19fa9feb886263bb0",
      "1e790dc40cbe4182aa070ef9432cf788",
      "a0ebc50ec65a4f81a24786874bbe1cd6",
      "a1d56f27907943b58a27176de3958197",
      "74c06c1388e14e119fa630d5cc94ea06",
      "454a29ef973046de98cc2568c5bc5f80",
      "36212057098149778f7fa886823d1eb4",
      "68469844afe3464fb7b52e45a8537318",
      "3091bfc3f9c34a52a3a002256a20b8eb",
      "b0afbf93828c449eb0305a6a4c48ff10",
      "f263286689db4c6c95fa66e8410f0ce2",
      "bcc4389d67204b4c9e57aa477f0ad254",
      "91d1069e38a140729219c4b40aa185e7",
      "e2486987d9ec467a8d674578ebb87488",
      "9acdea6c639e43fcbc2416ee281fc7f9",
      "cfc277d26d1b4a05bbcbd4cd1e636399",
      "916db3ff4c7643e8abe509bba1f09ea9",
      "825e43b4182444698f40a87c6215b949",
      "8c3ffe2ac6454864937a6c8554e1528b",
      "eecfb813b9b0439a8d480c40c16ef542",
      "30e7fa6820fe4194b95cb75f55175f9f"
     ]
    },
    "id": "a3251cca",
    "outputId": "e12c3bbf-7890-4e9e-c71a-7e1e62cfb715"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96a5b50486c4a2aab7486ccdf61c147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aec4a0b7b142cd9b0a00c6c54b8d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentences.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f263286689db4c6c95fa66e8410f0ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da3b11",
   "metadata": {
    "id": "a1da3b11"
   },
   "source": [
    "이 데이터셋에는 세 개의 열이 있습니다:\n",
    "\n",
    "* 원본 영어 문장 (`sentence`)\n",
    "* 요다체로 바꾼 문장(`translation`)\n",
    "* Yesss와 Hrrmm 감탄사를 포함한 향상된 번역(`translation_extra`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c804839",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c804839",
    "outputId": "89bbd2a8-c9d4-48dc-e8e5-6ea4386e1864"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37005b36",
   "metadata": {
    "id": "37005b36"
   },
   "source": [
    "모델을 미세 튜팅하기 위해 사용할 `SFTTrainer` 클래스가 자동으로 데이터셋을 **대화 포맷**(conversational format) 또는 **지시 포맷**(instruction format)으로 처리할 수 있습니다.\n",
    "\n",
    "* 대화 포맷\n",
    "\n",
    "```\n",
    "{\"messages\":[\n",
    "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
    "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
    "]}\n",
    "```\n",
    "\n",
    "* 지시 포맷: **[안타깝지만 최신 버전의 `trl`에서는 지시 포맷이 더이상 제대로 지원되지 않습니다. 아래 <중요 업데이트> 섹션을 참고하세요.]**\n",
    "\n",
    "```\n",
    "{\"prompt\": \"<prompt text>\",\n",
    "\"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "지시 포맷이 다루기 쉽기 때문에 데이터셋에 있는 열 이름을 지시 포맷 스타일로 바꾸고 필요 없는 열은 삭제하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "601b15b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "601b15b7",
    "outputId": "95ecd4f5-fd62-4ce4-e2d2-1bed6f872e46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae9739f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ae9739f",
    "outputId": "d09756ff-6ace-440b-8a0f-3ba82ba1f81a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The birch canoe slid on the smooth planks.',\n",
       " 'completion': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfd798",
   "metadata": {
    "id": "bfdfd798"
   },
   "source": [
    "내부적으로 훈련 데이터는 지시 포맷에서 대화 포맷으로 바뀔것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b150ab79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b150ab79",
    "outputId": "3ec51b1a-b8a5-49ea-deb7-a11438c53456"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'The birch canoe slid on the smooth planks.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0]['prompt']},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0]['completion']}\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab6fe3",
   "metadata": {
    "id": "45ab6fe3"
   },
   "source": [
    "***\n",
    "\n",
    "**중요 업데이트**: 안타깝지만 최신 버전의 `trl`에서는 지시 포맷이 잘 지원되지 않습니다. 이로 인해 채팅 템플릿이 데이터셋에 적용되지 않습니다. 이 문제를 피하려면 데이터셋을 대화 포맷으로 변경해야 합니다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca9e9fa",
   "metadata": {
    "id": "eca9e9fa"
   },
   "outputs": [],
   "source": [
    "# trl.extras.dataset_formatting.instructions_formatting_function을 참고함.\n",
    "# 데이터셋을 (더이상 지원되지 않는) prompt/completion 포맷에서 대화 포맷으로 변경합니다.\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf5c97e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0a743d15755446eeb3f05ee6040ceb98",
      "b38917d93ef84d399924ca2d5a0048ea",
      "c6a1de3ac2ad416096935bc22796b5a8",
      "15d2f963b117492b863182f3ef6d9e88",
      "b6feb1e8905b44bf89d3daa407088852",
      "a2e8a5deb3ca49afb70f9220918a0682",
      "67621ede9e304b198f5fb5c9dc0d0fe9",
      "d8d87265e114453eb8184f6f298708f8",
      "2508b112227f41b2b710a007de5f77f0",
      "4ab887ce9e924c908c3ca188b718d241",
      "a0252a293f5044ebb295d335c606dcff"
     ]
    },
    "id": "cbf5c97e",
    "outputId": "8e70f651-ed96-4b5a-fc9b-809ae108287a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a743d15755446eeb3f05ee6040ceb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ab1a7",
   "metadata": {
    "id": "288ab1a7"
   },
   "source": [
    "### 토크나이저\n",
    "\n",
    "실제 훈련으로 넘어가기 전에 이 모델에 해당하는 **토크나이저**(tokenizer)를 로드해야 합니다. 토크나이저는 전체 과정에서 중요한 역할을 담당하며, 모델을 훈련할 때와 동일한 방식으로 텍스트를 토큰(token)으로 바꾸어 줍니다.\n",
    "\n",
    "지시와 채팅 모델의 경우 토크나이저에는 **채팅 템플릿**(chat template)도 담겨 있으며, 이를 통해 다음과 같은 내용을 알 수 있습니다.\n",
    "\n",
    "- 사용할 **특수 토큰**과 위치\n",
    "- 시스템 지시 사항, **사용자 프롬프트**(prompt), 모델 응답의 위치\n",
    "- **생성 프롬프트**, 즉 모델의 응답을 트리거(trigger)하는 특수 토큰(\"모델에게 질의하기\" 절에서 자세히 다룹니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a647f985",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "referenced_widgets": [
      "445b62b89167419c80af57e426ca548b",
      "1bed983b132240f4ad90621cdfe58454",
      "0c8bae640fa042c4a29d85affcbf4955",
      "15b88f9ce6864c06a8917943c9f28c58",
      "ff351296bf934bc1afc8992dcc06ac49",
      "948bb96bc0cc48e4a9efcbe73f79056b",
      "a75d087e43d5401b86a71472450cb7ce",
      "8d8ce884fe86437da99468f3e0fd6ac1",
      "484fd6e6fae34165a06b5e9347b0f071",
      "f001063fafac426b835d1f1b2e9f0f47",
      "8c050c7235da4ef0aa991567e1710ba1",
      "f6ee3972c66e43b0bdcd013241254900",
      "d1a222e6bd9b4c219dc645eb5af3f7c7",
      "30174530593d4f728c2eb32e33e61576",
      "c78d434affd74db1bdf4b76e114f2ba3",
      "09a0cb81a5754fbdaafbdd2bca72039c",
      "f9cc0a7b631d43a0b27496ede31333ed",
      "7a029c923b014037825b539f9c84f560",
      "e1e8c7452f8a4190a2b5bf5da15de47e",
      "3cb57a02a422498b968dfa6742a67deb",
      "81cd70b48cc24db080461d1e459e203e",
      "5dac2df6ddc649c7a2fa257eb3ed61b7",
      "6f4f5d1422194e1aaf7821512698ee28",
      "af6fa2d1f9b14c768ffdd5dcecd4360d",
      "beb3b40c351d4d139084e18d53667671",
      "a047cdc12fe74b3b81bd216ffeee25f5",
      "58cff1dd91c7495796e354f70eb1020c",
      "1d14e21b7229452c83abfbb7422a033c",
      "b289886ed20d4128b093b263c0863f33",
      "213a95ffcaab4382925391013307b585",
      "b63a10a05bf046eebd2c4ae948f8ed76",
      "10c6ab12e8464adc88bbd3277e478a56",
      "08869f22a30a46bc91335d46ec33ae94",
      "13162d125de5470786b3e89040727903",
      "3ea6df93a42540ac9bbedf05ffeb67e6",
      "a23488b89045466296cce6b1c6a10ca4",
      "b07b493d10924d7c9d6a95f55ed2ade2",
      "238f1492c04841e3ac957d6c1af40834",
      "a2b4123665e14749940d4ee7b95cc529",
      "8396e3392c354b368ad14104325dcccb",
      "1b075f7e11fd4e4abe513d98454df007",
      "d7bb6b0e08604092a7ee740f0842805b",
      "aaf47d33a70a40de8e68868efea68448",
      "197df2b0c9ae4a129c6607dfcd62dfcd",
      "ab4314bae44a470b95c91cbbd10c46ed",
      "4af7b0a816744dce924a09147b1ed6a2",
      "d9efe335c5c343daa9c5b6511f6e940e",
      "7a562e15d425477ebebc79e89d864c8c",
      "105c971b9a9c49a9b611819d8f8776c5",
      "e82b10e355ab467f85ba1a6fe5871696",
      "6c46d8f0b94d487fbd99a6b3d9a4c944",
      "31f34e73a7874765917b7e327b768b77",
      "b768ba5eda6c4a1f81edbe8874c2eb76",
      "eac2989a3f1b41778c7f1bbe1f948443",
      "ad905a464ed642fea37a00f929e795ed"
     ]
    },
    "id": "a647f985",
    "outputId": "1d9dfe24-55b1-408a-e4bd-6605550381d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445b62b89167419c80af57e426ca548b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee3972c66e43b0bdcd013241254900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4f5d1422194e1aaf7821512698ee28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13162d125de5470786b3e89040727903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4314bae44a470b95c91cbbd10c46ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f7d28",
   "metadata": {
    "id": "1b4f7d28"
   },
   "source": [
    "복잡해 보이는 템플릿은 신경쓰지 마세요(보기 좋도록 줄바꿈과 들여쓰기를 추가했습니다). 토크나이저는 다음처럼 메시지를 적절한 태그를 사용해 일관성 있는 블록으로 구성합니다(`tokenize=False`로 지정하면 숫자 토큰 ID의 시퀀스가 아니라 텍스트가 반환됩니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b2cbfd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b2cbfd6",
    "outputId": "b5fb2009-933b-40d9-f75b-af194386d1ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fd5c6",
   "metadata": {
    "id": "a44fd5c6"
   },
   "source": [
    "\n",
    "각 대화는 `<|user|>` 또는 `<|assistant|>`로 시작하고 `<|end|>`로 끝납니다. 또한 `<|endoftext|>`는 전체 블록의 끝을 나타냅니다.\n",
    "\n",
    "모델이 다르면 템플릿이 다르고, 문장과 블록의 시작과 끝을 나타내는 토큰이 다를 수 있습니다.\n",
    "\n",
    "이제 미세 튜닝을 수행할 준비를 마쳤습니다!\n",
    "\n",
    "***\n",
    "**중요 업데이트**: `SFTTrainer` 클래스가 데이터셋 구축에 사용하는 기본 콜레이터의 변경으로 인해 (Phi-3에서는 PAD 토큰과 동일한) EOS 토큰도 레이블에서 마스킹됩니다. 이로 인해 모델이 토큰 생성을 적절하게 중지하지 못할 수 있습니다.\n",
    "\n",
    "이 문제를 해결하기 위해 UNK 토큰을 PAD 토큰으로 할당할 수 있습니다. 이렇게 하면 EOS 토큰이 고유하게 되며 레이블의 일부로 마스킹되지 않습니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af61ed6",
   "metadata": {
    "id": "4af61ed6"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada15ad",
   "metadata": {
    "id": "4ada15ad"
   },
   "source": [
    "## SFTTrainer를 사용해 미세 튜닝하기\n",
    "\n",
    "모델의 규모에 상관없이 미세 튜닝은 모델을 처음부터 훈련하는 것과 정확히 동일한 훈련 과정을 거칩니다. 파이토치로 훈련 루프를 직접 구현하거나 허깅 페이스의 `Trainer` 클래스를 사용해 모델을 미세 튜닝할 수 있습니다.\n",
    "\n",
    "하지만 (`Trainer`를 상속한) `SFTTrainer`를 사용하는 것이 훨씬 쉽습니다. 다음의 네 가지 매개변수 값을 제공하기만 하면 대부분의 세부 사항을 처리해 주기 때문입니다.\n",
    "\n",
    "* 모델\n",
    "* 토크나이저\n",
    "* 데이터셋\n",
    "* 설정 객체\n",
    "\n",
    "처음 세 개는 이미 준비되었으므로 마지막 설정 객체를 만들어 보죠.\n",
    "\n",
    "### SFTConfig\n",
    "\n",
    "설정 객체에는 많은 매개변수가 있습니다. 이를 네 개의 그룹으로 나누어 보죠:\n",
    "\n",
    "* 메모리 사용 최적화 매개변수는 **그레이디언트 누적**(gradient accumulation) 및 **그레이디언트 체크포인팅**(gradient checkpointing)과 관련이 있습니다.\n",
    "* `max_seq_length`와 같은 데이터셋 관련 매개변수와 시퀀스 패킹(packing) 여부\n",
    "* `learning_rate` 및 `num_train_epochs` 같은 일반적인 훈련 매개변수\n",
    "* `output_dir`(모델을 훈련한 다음 허깅 페이스 허브에 저장할 경우 모델의 이름으로 사용됩니다), `logging_dir`, `logging_steps`와 같은 환경 및 로깅 매개변수\n",
    "\n",
    "**학습률**(learning rate)이 매우 중요한 매개변수입니다(처음에는 베이스 모델 훈련에 사용한 학습률을 시도해볼 수 있습니다). 하지만 실제로는 **최대 시퀀스 길이**(maximum sequence length)가 메모리 부족 문제를 일으킬 가능성이 더 높습니다.\n",
    "\n",
    "주어진 문제에서 가능한 가장 짧은 max_seq_length를 선택하세요. 여기에서는 영어와 요다체 문장 모두 매우 짧습니다. 64개 토큰이면 프롬프트, 텍스트 완성, 특수 토큰을 포함하기에 충분합니다.\n",
    "\n",
    "<blockquote class=\"tip\">\n",
    "  <p>\n",
    "    나중에 보게 되겠지만 플래시 어텐션(flash attention)을 사용하면 메모리 부족 문제를 피하면서 더 긴 시퀀스를 사용할 수 있습니다.\n",
    "  </p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a9979b2",
   "metadata": {
    "id": "7a9979b2"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## 그룹 1: 메모리 사용\n",
    "    # 이 매개변수들은 GPU RAM을 최대한 활용하도록 돕습니다.\n",
    "    # 체크포인팅\n",
    "    gradient_checkpointing=True,   # 메모리가 많이 절약됩니다.\n",
    "    # 파이토치 새 버전에서 예외를 피하기 위해 지정합니다.\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # 그레이디언트 누적과 배치 크기\n",
    "    # (업데이트를 위한) 실제 배치 크기는 마이크로 배치 크기와 같습니다.\n",
    "    gradient_accumulation_steps=1,\n",
    "    # 초기 (마이크로) 배치 크기\n",
    "    per_device_train_batch_size=16,\n",
    "    # 배치 크기가 메모리 부족을 일으키면 문제가 해결될 때까지 반으로 나눕니다.\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## 그룹 2: 데이터셋 관련\n",
    "    max_seq_length=64,\n",
    "    # 데이터셋 패킹을 한다는 것은 패딩이 필요 없다는 의미입니다.\n",
    "    packing=True,\n",
    "\n",
    "    ## 그룹 3: 일반적인 훈련 매개변수\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # 8-비트 Adam 옵티마이저 - LoRA를 사용하는 경우 큰 도움이 되지 않습니다!\n",
    "    optim='paged_adamw_8bit',\n",
    "\n",
    "    ## 그룹 4: 로깅 매개변수\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e3099",
   "metadata": {
    "id": "971e3099"
   },
   "source": [
    "### `SFTTrainer`\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"이제 훈련 시간이다!\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    헐크\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "드디어 지도 학습 미세 튜닝을 위한 훈련 객체를 만들 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f194a639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107,
     "referenced_widgets": [
      "4e269b332b0e45368d511164394d516b",
      "6df2cc152fd04a86bfaa1aa97dfbb97e",
      "7f95171527cd4695abe11c76195c7222",
      "0cf41187145940a7b276114395e561a9",
      "6284b09fce204aa3a152bef98f9be393",
      "2f72933bd0de444a8c9043890c1370ca",
      "c95a2c040e2e462483f2e0e34e5ff5d1",
      "eee04ecd62bc462f874d989bfdfedb24",
      "9d2c92365e314e648540c1cb05c909a9",
      "964909f63e664e388b6ca689ecc93fc0",
      "0000850317bd4fc0ae71c221d1805c41"
     ]
    },
    "id": "f194a639",
    "outputId": "fbd7240b-db0e-44c7-8b27-50a5ade30956"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e269b332b0e45368d511164394d516b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c4efd",
   "metadata": {
    "id": "105c4efd"
   },
   "source": [
    "`SFTTrainer`에 사전에 전처리된 데이터셋을 전달했으므로 미니 배치가 어떻게 구성되었는지 확인해 볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ffc0403",
   "metadata": {
    "id": "1ffc0403"
   },
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df",
   "metadata": {},
   "source": [
    "레이블(label)을 확인해 보죠. 무엇보다도 앞에서 직접 레이블을 지정하지 않았습니다. 그렇죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfaf09e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfaf09e2",
    "outputId": "14d7a002-99b0-47e2-eadc-faa39dda283f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  310,   278, 27683,  4094, 29892,  5785,   278, 29559, 29892,   366,\n",
       "          1818, 29889, 32007, 32000, 32000, 32010,   450,  2217, 29763,   896,\n",
       "          2649,   526,  2089, 29889, 32007, 32001,  7700, 29892,   278,  2217,\n",
       "         29763,   896,  2649,   526, 29889, 32007, 32000, 32000, 32010,   450,\n",
       "         27843, 21544,  5320,   274,  1237,   373, 12125,  1036, 29889, 32007,\n",
       "         32001,  1551, 12125,  1036, 29892,   278, 27843, 29892,  5320,   274,\n",
       "          1237, 29892, 21544, 29889], device='cuda:0'),\n",
       " tensor([  310,   278, 27683,  4094, 29892,  5785,   278, 29559, 29892,   366,\n",
       "          1818, 29889, 32007, 32000, 32000, 32010,   450,  2217, 29763,   896,\n",
       "          2649,   526,  2089, 29889, 32007, 32001,  7700, 29892,   278,  2217,\n",
       "         29763,   896,  2649,   526, 29889, 32007, 32000, 32000, 32010,   450,\n",
       "         27843, 21544,  5320,   274,  1237,   373, 12125,  1036, 29889, 32007,\n",
       "         32001,  1551, 12125,  1036, 29892,   278, 27843, 29892,  5320,   274,\n",
       "          1237, 29892, 21544, 29889], device='cuda:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedef238",
   "metadata": {
    "id": "eedef238"
   },
   "source": [
    "입력과 정확하게 동일한 값으로 레이블이 자동으로 추가되었습니다. 따라서 이는 **자기 지도 학습 미세 튜닝**(self-supervised fine-tuning)입니다.\n",
    "\n",
    "레이블을 하나씩 이동시키는 것도 자동으로 처리되기 때문에 이에 대해 신경쓸 필요가 없습니다.\n",
    "\n",
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    38억 개의 파라미터를 가진 모델이지만 앞의 설정을 사용하면 6GB RAM의 GTX 1060 같은 개인용 GPU에서 8개의 미니 배치로 훈련을 수행할 수 있습니다. 정말이에요!\n",
    "    <br>\n",
    "    이 경우 훈련을 완료하는데 약 35분이 걸립니다.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "그다음 train() 메서드를 호출합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b12a099d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "id": "b12a099d",
    "outputId": "b46be6fc-070a-4edb-d561-6d9943240425"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 05:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.348600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.8142823360183022, metrics={'train_runtime': 356.1236, 'train_samples_per_second': 9.856, 'train_steps_per_second': 0.618, 'total_flos': 5034400555991040.0, 'train_loss': 0.8142823360183022, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b666a",
   "metadata": {
    "id": "2f4b666a"
   },
   "source": [
    "## 모델에게 질의하기\n",
    "\n",
    "이제 모델에게 짧은 문장을 제공하면 요다 말투의 문장이 생성되어야 합니다.\n",
    "\n",
    "이 모델은 적절하게 포맷팅된 입력이 필요합니다. (이 경우 `user`에 해당하는) 메시지 리스트를 만들고 모델이 대답할 차례라는 것을 알려 주어야 합니다.\n",
    "\n",
    "이것이 `add_generation_prompt` 매개변수의 목적입니다. 대화의 끝에 `<|assistant|>`를 추가하여 모델이 다음 단어를 예측하게 하고 `<|endoftext|>` 토큰이 예측될 때까지 계속 진행합니다.\n",
    "\n",
    "다음 헬퍼 함수는 (대화 포맷으로) 메시지를 만들고 채팅 템플릿을 적용한 후 마지막에 특수 토큰 `<|assistant|>`를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5854efcd",
   "metadata": {
    "id": "5854efcd"
   },
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ba48b",
   "metadata": {
    "id": "438ba48b"
   },
   "source": [
    "샘플 문장으로 프폼프트를 생성해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "480a479b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "480a479b",
    "outputId": "464ed432-32ec-4113-e7f5-84e5bffb3d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The Force is strong in you!<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773df8d",
   "metadata": {
    "id": "4773df8d"
   },
   "source": [
    "프롬프트가 제대로 만들어진 것 같습니다. 이를 사용해 응답을 생성해 보죠. `generate()` 함수는 다음과 같은 작업을 합니다:\n",
    "\n",
    "* 프롬프트를 토큰화하여 토큰 ID의 텐서를 만듭니다(채팅 템플릿으로 특수 토큰을 이미 추가했으므로 `add_special_tokens`를 `False`로 지정합니다).\n",
    "* 모델을 **평가 모드**(`evaluation mode`)로 설정합니다.\n",
    "* 모델의 `generate()` 메서드를 호출하여 출력(토큰 ID)을 생성합니다.\n",
    "* 생성된 토큰 ID를 텍스트로 디코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65a53ea9",
   "metadata": {
    "id": "65a53ea9"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generation_output = model.generate(**tokenized_input,\n",
    "                                       eos_token_id=tokenizer.eos_token_id,\n",
    "                                       max_new_tokens=max_new_tokens)\n",
    "\n",
    "    output = tokenizer.batch_decode(generation_output,\n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6b3bf",
   "metadata": {
    "id": "e1f6b3bf"
   },
   "source": [
    "이제 모델이 실제로 요다체 문장을 생성하는지 테스트해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33dca746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33dca746",
    "outputId": "961ab911-8230-4045-aa01-8af2989354f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is!<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e821c3",
   "metadata": {
    "id": "56e821c3"
   },
   "source": [
    "훌륭하네요! 잘 됩니다! 요다처럼, 모델이 말하네요. 흐음..\n",
    "\n",
    "축하합니다. 첫 번째 LLM을 미세 튜닝했습니다!\n",
    "\n",
    "Phi-3-mini-4k-instruct 모델에 작은 어댑터를 추가하여 요다 번역기를 만들었습니다! 멋지지 않나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dc809",
   "metadata": {
    "id": "898dc809"
   },
   "source": [
    "### 어댑터 저장하기\n",
    "\n",
    "훈련이 완료된 후 `Trainer` 객체의 `save_model()` 메서드를 호출해 어댑터를 디스크에 저장할 수 있습니다. 이 메서드는 지정된 폴더에 모든 것을 저장합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b41a86d2",
   "metadata": {
    "id": "b41a86d2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('local-phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae1c2d",
   "metadata": {
    "id": "aaae1c2d"
   },
   "source": [
    "다음과 같은 파일들이 저장됩니다:\n",
    "\n",
    "* 어댑터 설정(`adapter_config.json`)과 가중치(`adapter_model.safetensors`). 어댑터 자체 크기는 50MB에 불과합니다.\n",
    "* 훈련 매개변수값(`training_args.bin`)\n",
    "* 토크나이저(`tokenizer.json`와 `tokenizer.model`)와 설정(`tokenizer_config.json`), 특수 토큰(`added_tokens.json`와 `speciak_tokens_map.json`)\n",
    "* README 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "761d1ba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "761d1ba3",
    "outputId": "335f4b25-e577-4508-8bff-c8a43156cc89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adapter_config.json',\n",
       " 'README.md',\n",
       " 'added_tokens.json',\n",
       " 'special_tokens_map.json',\n",
       " 'adapter_model.safetensors',\n",
       " 'training_args.bin',\n",
       " 'tokenizer.json',\n",
       " 'tokenizer_config.json',\n",
       " 'tokenizer.model']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('local-phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014134ea",
   "metadata": {
    "id": "014134ea"
   },
   "source": [
    "이 장에서 만든 어댑터를 다른 사람과 공유하고 싶다면 이를 허깅 페이스 허브에 업로드할 수 있습니다. 먼저 쓰기 권한이 있는 액세스 토큰(access token)을 사용해 허깅 페이스에 로그인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33c1005a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360,
     "referenced_widgets": [
      "498706a955db4c769d863c36f5071c13",
      "2805d8f9ea3a4ce69157eb60edaad038",
      "1bed4b487fc74e02a1eb4e8cd3fafe30",
      "7bb4a1e88600499a9cdfd9031b895764",
      "e94c3b863a974dd48b37d140b65c5785",
      "9ddd4b9501e6425eab4006d443e01fe9",
      "35ef02b3fb85467e93ea2e1b288186f6",
      "01eed5d453a148cb8be5f6450a48f0b1",
      "4dc2b48dc90b4e83a9fa22b8de67db09",
      "e2e5dab1ea40462da157f8ecbd7a8a18",
      "5d22e5936ab3453b863f0ba82ef84c4c",
      "07e008e9cfe0464e995905a39a174f62",
      "3b0547a4a8c8497e83fa3bf963326aae",
      "43a06b98954a4609aec33042e27a33a8",
      "896c70c7018e4e38b283a2c9ecc700bf",
      "1a8ef272340c4a25bb426e165e846308",
      "efca53d6f55d45a7a7d08191c8571273"
     ]
    },
    "id": "33c1005a",
    "outputId": "7a4143fa-85db-4b99-c449-a06dd4f2a16c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498706a955db4c769d863c36f5071c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5de92",
   "metadata": {
    "id": "38c5de92"
   },
   "source": [
    "위 코드를 실행하면 다음과 같이 액세스 토큰을 요청합니다:\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub0.png?raw=True)\n",
    "<center>Figure 0.1 - Logging into the Hugging Face Hub</center>\n",
    "\n",
    "A successful login should look like this (pay attention to the permissions):\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub1.png?raw=True)\n",
    "<center>Figure 0.2 - Successful Login</center>\n",
    "\n",
    "그다음 `trainer` 객체의 `push_to_hub()` 메서드를 사용해 자신의 허브 계정에 모두 업로드할 수 있습니다. 모델 이름은 훈련 매개변수 `output_dir` 값에 따라 정해집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e11e931a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "0d59fc2c16654d92aa16a8f6938d9cc9",
      "c33cfd81423d4274af19d5e9f2274aea",
      "d4830d2d6a8a43a0bdab7c0e63a5045e",
      "0c846989d0a94e9189cf33c0f8c2d414",
      "fa609d5f25734e1b9be1b910cf0d30d5",
      "c9785e463d314ebab06a5e55fcb87b3f",
      "ca95b5796a9c493b8305e86e65efb1dd",
      "8fe69b546931449391b42bc166a28612",
      "243eb71fbbc846f184a7b070754626d0",
      "b91ecedcd3ef45dda04e062227b39c4d",
      "0271912f1f284383b0868d730b7e023d",
      "860f5557ea16427aa9e5a1c38b4c6864",
      "d631bb3550af4f02b4e4f231139d807a",
      "48ce62ac4b424b31be70a7cfe8555559",
      "d525bef2dca343a8bd28521ee85a72f0",
      "0023adb19b064bb18ccaa4396eefac12",
      "567b4d4852784f81a6f74a69a084b30d",
      "3c7d025f934c4a97876a9c1c8464e4ce",
      "d738de45ef6e4f77b455b8a7563dba7a",
      "c134a570b3464b3bb692049f70e8b026",
      "16b28ebb7804496da655088c89aeaf89",
      "e8d9a6e0895a49a5b678949b7ecf4108",
      "d0703abb321c413f94fb417fee024abd",
      "3344fe5cc715416cbae1ae86e0bb1c67",
      "e8adff9fb1e34f2582d78e056e582ad4",
      "e1f08a3ed23149139a6682ab09fcceea",
      "d9c5d416dbb641f385e7b1911e7e5caa",
      "1e6362573d614957924cd0d811b0722f",
      "f84a2762bd9e421db0fecd2a4765326d",
      "a5c5296ebcdd4ffcb7fdca8a56cf2fbf",
      "3df8e6f837164d14bc0deea2476b2fc9",
      "af6db3ab4ad242bf99619dd9c09ecec5",
      "e9ed22068914430086991039c75ef7d6",
      "d861f685a67541878038e4cc74a968b5",
      "b8284ef5ba9a4334b9c6c86f34f983f1",
      "da3b3e70644e4f2f8c8ce524dba08de8",
      "ccb960ab0baa4929996c16dd0bb26350",
      "735ab5d119b54a229997bfe7e54a20f4",
      "0a4a7f2e01b24fb798faa43ee0ca967d",
      "2a36b2a3ff73445aae093aac9bc6e53b",
      "1b66654016584f1e86ce7a1df1daa540",
      "0a6ee682aab54e1db453cb04deaf3732",
      "81c07d7a21a54c6aab4812ac35877251",
      "7c77ac6740f84ebca6625eb8ac330e85"
     ]
    },
    "id": "e11e931a",
    "outputId": "bb3521d7-117e-4739-b6e3-8e95eee79380"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d59fc2c16654d92aa16a8f6938d9cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860f5557ea16427aa9e5a1c38b4c6864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0703abb321c413f94fb417fee024abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d861f685a67541878038e4cc74a968b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/haesun/phi3-mini-yoda-adapter/commit/b2dd3b3f0b0343e9b74469a2fc3604f098fde537', commit_message='End of training', commit_description='', oid='b2dd3b3f0b0343e9b74469a2fc3604f098fde537', pr_url=None, repo_url=RepoUrl('https://huggingface.co/haesun/phi3-mini-yoda-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='haesun/phi3-mini-yoda-adapter'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca8a42",
   "metadata": {
    "id": "a5ca8a42"
   },
   "source": [
    "이게 끝입니다! 모델이 세상에 공개되었고 누구나 이 모델을 사용해 영어 문장을 요다체로 바꿀 수 있습니다!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
