{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e38439",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b48542",
   "metadata": {
    "id": "88b48542"
   },
   "source": [
    "## 0장 LLM 미세 튜닝 레시피\n",
    "\n",
    "### 스포일러\n",
    "\n",
    "이 장에서는 바로 본론으로 들어가 소규모 언어 모델인 마이크로소프트의 Phi-3-mini-4k-instruct 모델을 미세 튜닝하여 영어를 요다체(Yoda-speak)로 바꾸어 보겠습니다. 이 첫 번째 장을 그냥 따라하는 일종의 레시피로 생각할 수 있습니다. \"일단 해보고 궁금한 건 나중에 알아보자\" 식의 장이죠.\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- BitsAndBytes를 사용해 양자화된 모델(quantized model)을 로드합니다.\n",
    "- 허깅 페이스(Hugging Face)의 peft를 사용해 LoRA(low-rank adapter)를 설정합니다.\n",
    "- 데이터셋을 로드하고 포맷팅합니다.\n",
    "- 허깅 페이스의 trl에서 제공하는 SFTTrainer 클래스를 사용해 모델을 지도 학습 미세 튜닝(supervised fine-tuning)합니다.\n",
    "- 미세 튜닝된 모델을 사용해 요다체 문장을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb88c3d",
   "metadata": {
    "id": "5cb88c3d"
   },
   "source": [
    "### 설정\n",
    "\n",
    "훈련 재현성을 위해 책에서 사용하는 버전과 동일한 버전을 설치합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a165c97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a165c97",
    "outputId": "8147bb60-14e4-4412-a37b-42c21e2427c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.56.1\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.17.0\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting accelerate==1.10.0\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.23.1\n",
      "  Downloading trl-0.23.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting bitsandbytes==0.47.0\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting huggingface-hub==0.34.4\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: safetensors==0.6.2 in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (2.8.0+cu126)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (3.2.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.0) (3.0.3)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m145.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.1-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, transformers, bitsandbytes, accelerate, trl, peft\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.35.3\n",
      "    Uninstalling huggingface-hub-0.35.3:\n",
      "      Successfully uninstalled huggingface-hub-0.35.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "Successfully installed accelerate-1.10.0 bitsandbytes-0.47.0 huggingface-hub-0.34.4 peft-0.17.0 transformers-4.56.1 trl-0.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.56.1 peft==0.17.0 accelerate==1.10.0 trl==0.23.1 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94a803",
   "metadata": {
    "id": "ab94a803"
   },
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d45274",
   "metadata": {
    "id": "b9d45274",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ffab5",
   "metadata": {
    "id": "a64ffab5"
   },
   "source": [
    "## 양자화된 베이스 모델 로드하기\n",
    "\n",
    "먼저 GPU RAM을 덜 차지하는 양자화된 모델을 로드합니다. 양자화된 모델은 원본 가중치를 더 적은 비트로 표현된 근삿값으로 바꾼 것입니다. 모델을 양자화하는 가장 간단한 방법은 가중치를 32비트 부동 소수점(FP32) 숫자에서 4비트 부동 소수점(NF4)로 바꾸는 것입니다. 간단하지만 이런 강력한 변경이 모델의 메모리 사용량을 약 8배나 줄입니다.\n",
    "\n",
    "`from_pretrained()` 메서드를 사용해 모델을 로드할 때 `quantization_config` 매개변수에 `BitsAndBytesConfig` 객체를 전달합니다. 여러 모델을 자유롭게 테스트할 수 있도록 허깅 페이스의 `AutoModelForCausalLM`를 사용하겠습니다. 이 클래스를 허깅 페이스에 있는 어떤 저장소를 선택하는지에 따라 로드되는 모델이 결정됩니다.\n",
    "\n",
    "양자화된 모델을 로드하는 코드는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ad7668",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "bb27fc1ef12645cdb03db4614ab89a9e",
      "b46b9574a1124425a43b0d307dce7c36",
      "e248f671d9be46179c6eb64706d06733",
      "693e5c9798f34a41aede6dddb224715f",
      "7fe9644be9fe45baa51e75b10ed6262d",
      "7a2a8bd501f64d39aec987c447da0eb5",
      "c7fe4b1796904ddb82bafc6e841aac58",
      "4feaa6b2de274d089f42e2c2842fc96c",
      "daa98ac086344d82bc4c8f80e5bc6253",
      "2db499b1f8e2412c9357b0fe9fa540f5",
      "7efe2ad8e76e4e3b96a1def8423403b3",
      "0e1967d45c3741799b7b6811ccb8f457",
      "9466e1efa0a6456293e699a39075ac79",
      "48fcb8b4c3be4b948b35fa2e4f5923a5",
      "c947c74f678e4743991c5004e39522c0",
      "16c4d082038a44b6a5b79075c2833d41",
      "7bd646a6fe374b4abcbaa9b816492eb7",
      "c414a83887bd4ed8b7180803ad59c4f2",
      "fc3f0cb9862d4aae88e4d24b4890d523",
      "1ed5deb4e0114f409d2242071bcf886f",
      "1b9f712e584c420f9c10e5be4ada7150",
      "54b81228399a475daa6fa9b1a456e13f",
      "3418c30822134af19ba5bf04ab0bd365",
      "fc1f8f9841d84f4292f96c5eb598b587",
      "c19ebb3be66a4562997b87c1dd5f8e2d",
      "fba326bb62bd49d9ab081dae611decd3",
      "c3f1784514eb49df877e4aecf7de78b1",
      "9a68114cf46f4ddbb109558cc51778f9",
      "9e80c0f45476451882ee7d01b184fe3d",
      "34b8182829a94e84ac61ab5a32ac9275",
      "84cb70c208654cb79321dc58b55ad526",
      "a497d37d310c4aacb771e51bd4211945",
      "74884e4921b544c6a18c10734b2bc6f6",
      "6b529844121b4fe59216e485f2a225ba",
      "08c445886bbb4c26a97794099cb51c67",
      "1dfbdd9a2fed415b8bfd73212133f936",
      "2ca217e4838e47f695b7c8b53fcbfb62",
      "5a88cc56c85c48e096e030d1b9d81287",
      "0a5ab2d7f75842ba94a00b66f524dee2",
      "6c10521eedd543618eaedfdcb2b730d7",
      "c8f766e54dfc47868ba45d1b19da863d",
      "3a3ca70a8610491a99fb0b7b78680de5",
      "9c92f24f67a3440f847c874e977c6d9e",
      "34514b07923747c8accb014d89ab8bf6",
      "2a83e3ae365c4743a7ae55a92f35143c",
      "229ac17307ec4dc598f53b8f6ad97fec",
      "9083a55aac3f45f1b998181f8c9b76f1",
      "63dba27181ed4f35b092c28e65753bb3",
      "35048413cc5346878d3f476983d3b83c",
      "6400db541a194bfa82709c1432681dc5",
      "0f0bfac0cd654b99b1c58decc43fd92e",
      "8cf494da851c41dc8392640279cd44e9",
      "ef740123c1ab4883ab500307bea51d74",
      "58263881eaef4b47afe2954f97dbcf3e",
      "04d8f614141047b4936ce99a3493fc05",
      "c684938bdd484249bcf72fccc78a0735",
      "626f5ff320514507830d5a2dd0beec68",
      "df73258e7e8f410398b990ceb0c26af5",
      "eaf9b32982114066b26f033faa7ed846",
      "1db316599fa94c1490a4ad9f83919458",
      "a3b95b951dd841598a0e28935e0a9ed7",
      "e582f0f80eb94f85b673028a9eb3a98a",
      "652c1a81bf104c478fce4a89caa7d876",
      "d81fe77e634d433e988dc2b093a52876",
      "eb12d301007649cb8764e4aad3d2be33",
      "1869fd39602b49e0a9828085aee63f41",
      "e6fc2250415444cdaaf328902651845e",
      "75877f2b2333428b93a1f93d348ab467",
      "cb3a9dbe014f456d808be2bac75bdae5",
      "35380d98310747e0a229738f4b405c77",
      "305fc6463ade40abaec8ce0f2ccc2cbf",
      "f629a69eab43470c8edd2eb98de11424",
      "b6708eaa088548378a4b0246558ff687",
      "ee69dd8570de48a1ae5698a7dee99cb1",
      "224ab4cd6aa049b690d50bb4056a04db",
      "c0ac821b222946dfab8aeca0942bbb21",
      "b914934497ba436d8aec03cf0f5b7000"
     ]
    },
    "id": "f5ad7668",
    "outputId": "065c3941-a1f3-468c-bd05-bbaa8d4af296"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb27fc1ef12645cdb03db4614ab89a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1967d45c3741799b7b6811ccb8f457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3418c30822134af19ba5bf04ab0bd365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b529844121b4fe59216e485f2a225ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a83e3ae365c4743a7ae55a92f35143c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c684938bdd484249bcf72fccc78a0735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fc2250415444cdaaf328902651845e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f487f",
   "metadata": {
    "id": "f26f487f"
   },
   "source": [
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    <em>\"Phi-3-Mini-4K-Instruct는 38억 개의 파라미터를 가지고 있으며 Phi-3 데이터셋에서 훈련된 경량의 최신 오픈 소스 모델입니다. 이 데이터셋은 합성 데이터와 필터링된 공개 웹사이트의 데이터로 구성되며 고품질의 추론 중심 데이터입니다. 이 모델은 Phi-3 제품군에 속합니다. Mini 모델은 4K와 128K 문맥 길이(토큰 수)를 가진 버전이 있습니다.\"</em>\n",
    "    <br>\n",
    "    출처: <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">허깅 페이스</a>\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "모델이 로드된 후 `get_memory_footprint()` 메서드를 사용해 모델이 얼마만큼의 메모리를 차지하고 있는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3ac411",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3ac411",
    "outputId": "0048747a-a8ad-457d-babd-51eb7293f951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.341312\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f09ff7",
   "metadata": {
    "id": "24f09ff7"
   },
   "source": [
    "양자화되었더라도 모델은 2기가바이트 이상의 RAM을 차지합니다. 양자화 과정은 주로 **트랜스포머 디코더 블록**(Transformer decoder block)(종종 층이라고두 부릅니다)에 있는 **선형 층**(linear layer)을 대상으로 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc64043",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdc64043",
    "outputId": "be7304de-c9e1-454d-83f2-d00e4303c57e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461b229",
   "metadata": {
    "id": "1461b229"
   },
   "source": [
    "**양자화된 모델**(quantized model)은 바로 추론에 사용할 수 있지만 추가적으로 더 훈련할 수는 없습니다. 양자화의 핵심인 Linear4bit 층이 메모리를 적게 차지하지만 업데이트할 수 없기 때문입니다.\n",
    "\n",
    "대신 어댑터를 추가하여 모델의 동작을 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e6061",
   "metadata": {
    "id": "0f9e6061"
   },
   "source": [
    "## LoRA 설정하기\n",
    "\n",
    "LoRA(Low-Rank Adapter) 어댑터를 양자화된 각 층에 추가할 수 있습니다. **어댑터**(adapter)는 일반적인 Linear 층과 거의 비슷하며 쉽게 업데이트할 수 있습니다. 여기서 기발한 점은 이 어댑터가 양자화된 층보다 훨씬 작다는 것입니다.\n",
    "\n",
    "양자화된 층이 동결(frozen)되었기 때문에 (즉, 업데이트되지 않으므로) 양자화된 모델에 LoRA 어댑터를 추가했을 때 훈련 가능한 파라미터의 전체 개수가 원래의 1% (또는 그 미만으로) 크게 줄어 듭니다.\n",
    "\n",
    "LoRA 어댑터를 설정하는 단계는 세 개입니다.\n",
    "\n",
    "* 훈련 과정에서 수치 안정성을 향상시키기 위해 `prepare_model_for_kbit_training()`를 호출합니다.\n",
    "* `LoraConfig` 인스턴스를 만듭니다.\n",
    "* `get_peft_model()` 메서드를 사용해 양자화된 베이스 모델(base model)에 이 설정을 적용합니다.\n",
    "\n",
    "이 단계를 앞서 로드한 모델에 적용해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edc24ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3edc24ed",
    "outputId": "a63fc521-e236-4757-dfdc-ed5845ab0e0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,                   # 어댑터의 랭크(rank)가 작을수록 훈련할 파라미터가 적습니다.\n",
    "    lora_alpha=16,         # 일반적으로 2*r\n",
    "    bias=\"none\",           # 주의: 편향을 훈련하면 베이스 모델의 동작이 바뀔 수 있습니다.\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # 이 글을 쓰는 시점에 Phi-3와 같은 최신 모델은\n",
    "    # 대상 모듈을 수동으로 지정해야 합니다.\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f1cf6",
   "metadata": {
    "id": "611f1cf6"
   },
   "source": [
    "다른 LoRA 층(`qkv_proj`, `gate_up_proj`, `down_proj`)은 출력 길이를 짧게하기 위해 간단하게 나타냈습니다.\n",
    "\n",
    "<blockquote class=\"warning\">\n",
    "  <p>\n",
    "    혹시 다음과 같은 오류가 발생했나요?\n",
    "    <br>\n",
    "    <br>\n",
    "    <tt>ValueError: Please specify `target_modules` in `peft_config`</tt>\n",
    "    <br>\n",
    "    <br>\n",
    "    유명한 모델을 사용한다면 대부분 target_modules를 지정할 필요가 없습니다. peft 라이브러리가 자동으로 적절한 대상을 선택합니다. 하지만 인기있는 모델이 출시되는 시점과 라이브러리가 업데이트되는 시점 사이에는 간격이 있을 수 있습니다. 따라서 위와 같은 오류를 만났다면 모델에 있는 양자화된 층을 찾아 target_modules 매개변수에 해당 층의 이름을 입력하세요.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "양자화된 층(`Linear4bit`)이 `lora.Linear4bit`로 바뀌었습니다. 이 층은 양자화된 층인 `base_layer`와 일반적인 `Linear` 층(`lora_A`와 `lora_B`)을 섞은 것입니다.\n",
    "\n",
    "추가된 층은 모델의 크기를 조금만 더 증가시킵니다. 하지만 prepare_model_for_kbit_training() 함수가 양자화되지 않은 다른 모든 층을 **단정밀도**(single precision)(**FP32**)로 바꿉니다. 이로 인해 모델이 20% 더 커집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f30ecbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f30ecbd",
    "outputId": "c8d87469-bcca-4e0d-c35a-fd18744a4a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.074752\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2718a6",
   "metadata": {
    "id": "2e2718a6"
   },
   "source": [
    "대부분의 파라미터가 동결되었기 때문에 전체 파라미터 중에서 적은 부분만 훈련됩니다. LoRA 만세!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06c42b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c06c42b4",
    "outputId": "bb09f53f-1f71-4332-de87-924d636db2b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 가능한 파라미터:             12.58M\n",
      "총 파라미터:                 3833.66M\n",
      "훈련 가능한 파라미터의 비율: 0.33%\n"
     ]
    }
   ],
   "source": [
    "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
    "print(f'훈련 가능한 파라미터:             {trainable_parms/1e6:.2f}M')\n",
    "print(f'총 파라미터:                 {tot_parms/1e6:.2f}M')\n",
    "print(f'훈련 가능한 파라미터의 비율: {100*trainable_parms/tot_parms:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149381b5",
   "metadata": {
    "id": "149381b5"
   },
   "source": [
    "모델을 미세 튜닝할 준비를 마쳤지만 한 가지 중요한 요소인 데이터셋이 아직 준비되지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547d258",
   "metadata": {
    "id": "2547d258"
   },
   "source": [
    "## 데이터셋 포맷팅하기\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"요다 처럼, 말해라, 반드시. 흐음.\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    마스터 요다\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "[`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) 데이터셋은 영어를 요다체로 바꾼 720개 문장으로 구성되어 있습니다. 이 데이터셋은 허깅 페이스 허브(Hugging Face Hub)에서 다운받을 수 있으며 `datasets` 라이브러리의 load_dataset() 메서드로 손쉽게 로드할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3251cca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "a80d4b07809048c99547b14e83f437fc",
      "3dd97926f8c943909e274666ce1a1adf",
      "930c99b1dafb45d487ab8974ca379d72",
      "dded12c7d9044591bbe940d9e77fbc4a",
      "d335351537d34856bdfd3a33d806e5eb",
      "4551c81b7a0040f2831adef4fd22f440",
      "6c08099084264f318fa0ccc07ae4c55d",
      "24741b4ac3af460db177de96c90d9c5e",
      "1d9c3e9e1b84474893c90cffd61eb238",
      "314d64e17b74496280910caae3283bc8",
      "dae53ec0fe2549b08e4076d142566188",
      "f49713e621eb41288cb61ba90d02406d",
      "f090ebd057634214b34a247d7f0900fb",
      "c47f3e5974a24cd38ac34c07aa93120d",
      "162861acafb8453eb0a228e47e95af1a",
      "dc0428bba5c94a5eb2fdbbb15e42b31f",
      "f418e064c8e844acb86b2463b78caeab",
      "d79b1e31b2744ce78321a9c5468d4fae",
      "dde1c23046be428bb90b23f96ff23440",
      "473c5367552d4ff9be3dd9ca29cec3d1",
      "850710ce7b0d4245b90dec22559d1d04",
      "aa742d66e927441fba83f1871854d10e",
      "b435eb8b035b44aba2afe7c7ecb93af7",
      "f76388e7223e424b8e45a9e984561d57",
      "325506411a014d98ad78aabf0bd04021",
      "085deea3d52f40d4bfdf40a48ffd2172",
      "bb5c60ec39374bb5b5cc7922ac7cb73b",
      "7ab2cc05b94040869de80416fa11e40a",
      "e059611a8e924497868fd7d8c7b5e6fa",
      "9d2b0d3fd3a1429fb90cd674a15be5ed",
      "0bcc968ee21440baaf055fc658ca6691",
      "29259b9abf8e4ba38be5210963ea332d",
      "188e1930d2194414a2596c2658f560f0"
     ]
    },
    "id": "a3251cca",
    "outputId": "fa5154bd-4793-4e6a-f184-a0a1119de500"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80d4b07809048c99547b14e83f437fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49713e621eb41288cb61ba90d02406d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentences.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b435eb8b035b44aba2afe7c7ecb93af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da3b11",
   "metadata": {
    "id": "a1da3b11"
   },
   "source": [
    "이 데이터셋에는 세 개의 열이 있습니다:\n",
    "\n",
    "* 원본 영어 문장 (`sentence`)\n",
    "* 요다체로 바꾼 문장(`translation`)\n",
    "* Yesss와 Hrrmm 감탄사를 포함한 향상된 번역(`translation_extra`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c804839",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c804839",
    "outputId": "d08b5cc7-0da1-4a65-e0c9-e3c8114333c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37005b36",
   "metadata": {
    "id": "37005b36"
   },
   "source": [
    "모델을 미세 튜팅하기 위해 사용할 `SFTTrainer` 클래스가 자동으로 데이터셋을 **대화 포맷**(conversational format)으로 처리할 수 있습니다.\n",
    "\n",
    "```\n",
    "{\"messages\":[\n",
    "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
    "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
    "]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YMqs7yPqbO_R",
   "metadata": {
    "id": "YMqs7yPqbO_R"
   },
   "source": [
    "이전 버전에서는 `prompt`와 `completion` 열만 두 개만 있으면 데이터셋이 자동으로 대화 포맷으로 변환되었습니다. 이제는 더이상 이 방식이 지원되지 않으므로 직접 변환하는 것이 좋습니다. 다음에 나오는 `format_dataset()` 함수는 `trl` 패키지를 참고하여 작성한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca9e9fa",
   "metadata": {
    "id": "eca9e9fa"
   },
   "outputs": [],
   "source": [
    "# trl.extras.dataset_formatting.instructions_formatting_function을 참고함.\n",
    "# 데이터셋을 (더이상 지원되지 않는) prompt/completion 포맷에서 대화 포맷으로 변경합니다.\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P6Uqsd1ubZbl",
   "metadata": {
    "id": "P6Uqsd1ubZbl"
   },
   "source": [
    "이 함수는 `prompt`와 `completion` 열을 가진 샘플을 기대하므로 데이터셋에 적용하기 전에 열 이름을 바꾸어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "601b15b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "referenced_widgets": [
      "01342c00d79f4d648cd93ca78afdcb6b",
      "b2c601351845450393357132675d814c",
      "1a2673c20d85461b9a7d7dc7aa1308eb",
      "31539b4b89004cb6a058fe9c6d009d27",
      "dbbc676b3f634ca0a2c1ce1da1a93eb7",
      "7eb51f0494af456eb91f50831866bd6c",
      "1230c5cc806d4e68840888dbde715f03",
      "4289a84af01c42478029ef0271d2d831",
      "361f8a3adad6411d8fc1eab22e10dc23",
      "0e7cef7c01914803a85ae0e95b51dbbf",
      "57091ba328144591aeefd74e48c9202a"
     ]
    },
    "id": "601b15b7",
    "outputId": "171e50fe-90c9-48db-d4eb-d62ca5af2ce3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01342c00d79f4d648cd93ca78afdcb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'},\n",
       " {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
    "messages = dataset[0]['messages']\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ab1a7",
   "metadata": {
    "id": "288ab1a7"
   },
   "source": [
    "### 토크나이저\n",
    "\n",
    "실제 훈련으로 넘어가기 전에 이 모델에 해당하는 **토크나이저**(tokenizer)를 로드해야 합니다. 토크나이저는 전체 과정에서 중요한 역할을 담당하며, 모델을 훈련할 때와 동일한 방식으로 텍스트를 토큰(token)으로 바꾸어 줍니다.\n",
    "\n",
    "지시와 채팅 모델의 경우 토크나이저에는 **채팅 템플릿**(chat template)도 담겨 있으며, 이를 통해 다음과 같은 내용을 알 수 있습니다.\n",
    "\n",
    "- 사용할 **특수 토큰**과 위치\n",
    "- 시스템 지시 사항, **사용자 프롬프트**(prompt), 모델 응답의 위치\n",
    "- **생성 프롬프트**, 즉 모델의 응답을 트리거(trigger)하는 특수 토큰(\"모델에게 질의하기\" 절에서 자세히 다룹니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faOWLpDGbvpU",
   "metadata": {
    "id": "faOWLpDGbvpU"
   },
   "source": [
    "**<중요>**\n",
    "EOS 토큰을 고유하게 유지하는 것이 중요합니다. Phi-3 같은 일부 모델에서는 EOS 토큰이 PAD 토큰으로도 사용됩니다. 이로 인해 훈련 중에 EOS 토큰이 마스킹되어 토큰 생성이 끊임없이 이어질 수 있습니다. 이 문제를 피하기 위해 UNK 토큰을 PAD 토큰으로 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a647f985",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "7ddd22038cca402d9b601370c073144f",
      "bfacc94c41cf438e9aec8c721dd49d40",
      "a872e6cbb00e4959bf6b906278ef8f21",
      "f93cf5e9f2664f848ed0ecbc03c477ec",
      "498c7ba088154f73ae753629bae7aa12",
      "c2f449e09c1644919151bf8936bd0677",
      "a258fd36a489429e9c095a47b5a9bc01",
      "46d22d9c9c104846916051a9b83b319b",
      "e4d9e77bf3f049cd8eb9a49836966929",
      "88727d7944ee441b9702c6413159bee9",
      "fe3fd6578710428d96fee443c252377a",
      "5034dd75fe8a44718c6fffb524aa941b",
      "7992647b140644f29084d68e1c7f1285",
      "61a3c579864c49f087a50d18b04a3601",
      "7e1b52832eac4f98a1d8b0556f7aa908",
      "8b08c970390b4724a24d3b38d3f55350",
      "1098fdfeca0e475da9f8b80c4ef921c4",
      "f25bc76232df481abe8123b776bf04fc",
      "67e4ed6cee4d4a4cb08c0c59ecd416c4",
      "994cf57e6b0d460f8e67634744eb94c9",
      "3182979732854a42af4beed5db16d852",
      "69c7fef151014fe582cd070f1f9d3926",
      "300306ff2c9e4337872eb012c39479e2",
      "ab60dd01d4fa4945939ead901f0de9bc",
      "03f2850e92e041d583fc1ef2b090b383",
      "28fbcb7b553a408c995435005665206f",
      "4b422be0a9744db6b6fa56877e6ef0cc",
      "1433594563b9465386185e8276a05a81",
      "d2c93167f8774029bb32faec31277287",
      "687fe0607095486ea1c3310443b7299e",
      "4a87a92bdf944338967390a6ae87a456",
      "18f0be9171eb41b1ae84be0e68d2cc8e",
      "d048aa1e1e904927ae77f3608f821a7d",
      "d1a3c15bd10b4da08852e7ff3944724a",
      "b6736c4f1ec74cfcbbf7e3039291901b",
      "5e3d2ee73d7442eeb21f9d869e0a93f7",
      "5265131840f84f0f9833b76a8f7b1ff0",
      "c99650ce0a804aed8cdfe9f64b07a212",
      "feca26f82c60417ab1176a8e4f525f9a",
      "8b230ddacbdb41c7b0ccc3578998b68e",
      "ca74e6b80a444cbf9f7353b204bc9879",
      "f03b89eded6c4c02b9055499a2b37c53",
      "208032d0ca9346a4bf7e521383689095",
      "0ff3fbc22e7f43a0a25e4eeccd106da5",
      "864853dfdc67412d88b074b04860399e",
      "f8c6ee0cd6804529b8929017f920a661",
      "fea50839f98348af85f444e6864b31a1",
      "b2aed353eff643e18332af937a413250",
      "e8390fc833ec4335b773815c13df5940",
      "adf7fa1d5c0a43eb85f95a4d3dfb6f6a",
      "465a7dba688e4c6eb22bf9b44b19285c",
      "2651be031f9e43d8a8aa38a4cce6b7c9",
      "6c30aa5d7af44e3faf7b4d25e7541a69",
      "3db4a7e19dc740679a2c09d0eaf9d593",
      "4feaeefd7df74c22af983925001d43b8"
     ]
    },
    "id": "a647f985",
    "outputId": "21edb2bc-7881-40db-daa6-373c207fd9bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddd22038cca402d9b601370c073144f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5034dd75fe8a44718c6fffb524aa941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300306ff2c9e4337872eb012c39479e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a3c15bd10b4da08852e7ff3944724a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864853dfdc67412d88b074b04860399e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "# EOS 토큰을 고유하게 만들기 위해 UNK 토큰을 PAD 토큰으로 할당합니다.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f7d28",
   "metadata": {
    "id": "1b4f7d28"
   },
   "source": [
    "복잡해 보이는 템플릿은 신경쓰지 마세요(보기 좋도록 줄바꿈과 들여쓰기를 추가했습니다). 토크나이저는 다음처럼 메시지를 적절한 태그를 사용해 일관성 있는 블록으로 구성합니다(`tokenize=False`로 지정하면 숫자 토큰 ID의 시퀀스가 아니라 텍스트가 반환됩니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2cbfd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b2cbfd6",
    "outputId": "0dc56c29-4f54-42b7-82e8-ff3eb61519f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fd5c6",
   "metadata": {
    "id": "a44fd5c6"
   },
   "source": [
    "\n",
    "각 대화는 `<|user|>` 또는 `<|assistant|>`로 시작하고 `<|end|>`로 끝납니다. 또한 `<|endoftext|>`는 전체 블록의 끝을 나타냅니다.\n",
    "\n",
    "모델이 다르면 템플릿이 다르고, 문장과 블록의 시작과 끝을 나타내는 토큰이 다를 수 있습니다.\n",
    "\n",
    "이제 미세 튜닝을 수행할 준비를 마쳤습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada15ad",
   "metadata": {
    "id": "4ada15ad"
   },
   "source": [
    "## SFTTrainer를 사용해 미세 튜닝하기\n",
    "\n",
    "모델의 규모에 상관없이 미세 튜닝은 모델을 처음부터 훈련하는 것과 정확히 동일한 훈련 과정을 거칩니다. 파이토치로 훈련 루프를 직접 구현하거나 허깅 페이스의 `Trainer` 클래스를 사용해 모델을 미세 튜닝할 수 있습니다.\n",
    "\n",
    "하지만 (`Trainer`를 상속한) `SFTTrainer`를 사용하는 것이 훨씬 쉽습니다. 다음의 네 가지 매개변수 값을 제공하기만 하면 대부분의 세부 사항을 처리해 주기 때문입니다.\n",
    "\n",
    "* 모델\n",
    "* 토크나이저\n",
    "* 데이터셋\n",
    "* 설정 객체\n",
    "\n",
    "처음 세 개는 이미 준비되었으므로 마지막 설정 객체를 만들어 보죠.\n",
    "\n",
    "### SFTConfig\n",
    "\n",
    "설정 객체에는 많은 매개변수가 있습니다. 이를 네 개의 그룹으로 나누어 보죠:\n",
    "\n",
    "* 메모리 사용 최적화 매개변수는 **그레이디언트 누적**(gradient accumulation) 및 **그레이디언트 체크포인팅**(gradient checkpointing)과 관련이 있습니다.\n",
    "* `max_seq_length`와 같은 데이터셋 관련 매개변수와 시퀀스 패킹(packing) 여부\n",
    "* `learning_rate` 및 `num_train_epochs` 같은 일반적인 훈련 매개변수\n",
    "* `output_dir`(모델을 훈련한 다음 허깅 페이스 허브에 저장할 경우 모델의 이름으로 사용됩니다), `logging_dir`, `logging_steps`와 같은 환경 및 로깅 매개변수\n",
    "\n",
    "**학습률**(learning rate)이 매우 중요한 매개변수입니다(처음에는 베이스 모델 훈련에 사용한 학습률을 시도해볼 수 있습니다). 하지만 실제로는 **최대 시퀀스 길이**(maximum sequence length)가 메모리 부족 문제를 일으킬 가능성이 더 높습니다.\n",
    "\n",
    "주어진 문제에서 가능한 가장 짧은 max_seq_length를 선택하세요. 여기에서는 영어와 요다체 문장 모두 매우 짧습니다. 64개 토큰이면 프롬프트, 텍스트 완성, 특수 토큰을 포함하기에 충분합니다.\n",
    "\n",
    "<blockquote class=\"tip\">\n",
    "  <p>\n",
    "    나중에 보게 되겠지만 플래시 어텐션(flash attention)을 사용하면 메모리 부족 문제를 피하면서 더 긴 시퀀스를 사용할 수 있습니다.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "****\n",
    "**중요 업데이트**: `trl` 버전 0.20에서 `SFTConfig`에 몇 가지 중요한 변경 사항이 있습니다:\n",
    "- `packing_strategy='wrapped'`로 지정하지 않으면 패킹이 이전과 다르게 수행됩니다.\n",
    "- `max_seq_length` 매개변수가 `max_length`로 이름이 바뀌었습니다.\n",
    "- `bf16` 매개변수 기본값이 `True`이지만 이 글을 쓰는 시점(2025년 10월)에 실제로 BF16 타입이 가능한지 여부를 확인하지 앟습니다. 따라서 설정에 이 매개변수를 명시적으로 포함시킵니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9979b2",
   "metadata": {
    "id": "7a9979b2"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## 그룹 1: 메모리 사용\n",
    "    # 이 매개변수들은 GPU RAM을 최대한 활용하도록 돕습니다.\n",
    "    # 체크포인팅\n",
    "    gradient_checkpointing=True,   # 메모리가 많이 절약됩니다.\n",
    "    # 파이토치 새 버전에서 예외를 피하기 위해 지정합니다.\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # 그레이디언트 누적과 배치 크기\n",
    "    # (업데이트를 위한) 실제 배치 크기는 마이크로 배치 크기와 같습니다.\n",
    "    gradient_accumulation_steps=1,\n",
    "    # 초기 (마이크로) 배치 크기\n",
    "    per_device_train_batch_size=16,\n",
    "    # 배치 크기가 메모리 부족을 일으키면 문제가 해결될 때까지 반으로 나눕니다.\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## 그룹 2: 데이터셋 관련\n",
    "    max_length=64,\n",
    "    # 데이터셋 패킹을 한다는 것은 패딩이 필요 없다는 의미입니다.\n",
    "    packing=True,\n",
    "    packing_strategy='wrapped',\n",
    "\n",
    "    ## 그룹 3: 일반적인 훈련 매개변수\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # 8-비트 Adam 옵티마이저 - LoRA를 사용하는 경우 큰 도움이 되지 않습니다!\n",
    "    optim='paged_adamw_8bit',\n",
    "\n",
    "    ## 그룹 4: 로깅 매개변수\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none',\n",
    "\n",
    "    ## 그외\n",
    "    # 가능한 경우에만 bf16으로 훈련하도록\n",
    "    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dmxdC7iEcWP-",
   "metadata": {
    "id": "dmxdC7iEcWP-"
   },
   "source": [
    "**<중요>** 이 글을 쓰는 시점에 `trl`의 버전(0.21)은 지원 환경 여부에 상관없이 기본적으로 bf16 데이터 타입을 사용해 혼합 정밀도 훈련을 수행합니다. 문제를 방지하기 위해 GPU 지원 여부에 따라 `bf16` 매개변수를 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e3099",
   "metadata": {
    "id": "971e3099"
   },
   "source": [
    "### `SFTTrainer`\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"이제 훈련 시간이다!\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    헐크\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "****\n",
    "**중요 업데이트**: `trl` 0.23 버전짜기 LoRA 설정이 모델에 이미 적용되어 있으면 훈련이 실패하는 문제가 있었습니다. `trainer` 객체가 어댑터를 포함해 전체 모델을 동결하기 때문입니다.\n",
    "\n",
    "이 예제에서처럼 모델이 어댑터를 포함하고 있다면 원본 모델(`model.base_model.model`)과 `peft_config` 매개변수를 함께 사용해야 훈련이 됩니다.\n",
    "\n",
    "이 문제는 2025년 10월 릴리스된 `trl` 0.23.1 버전에서 수정되었습니다.\n",
    "****\n",
    "\n",
    "드디어 지도 학습 미세 튜닝을 위한 훈련 객체를 만들 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f194a639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "dd58ceeeadf047ef9e2e74eeda17af50",
      "7b5a34a9ebfa4b60857a114d65ca2d65",
      "8b0031fcf41a4bf0a9dd8667a1814fcd",
      "24156c57eded400b913a41546e01ab39",
      "b1719b67972b41098d2b05ae03c5bdfa",
      "692ee2fd680e4c06b0ebc2376597b9e0",
      "4a6bfead170345ff8efec60285ab7310",
      "e53b056707fd4c8497be70f66710516b",
      "8d59a1e4631e46e28df0e0171e2cf39e",
      "76d5a55b046b46089ce091dd740bbc00",
      "03e5c16f17b44dbe9a7dd94995eb07c3",
      "063243634a8b4f98b64a166e75f5dd82",
      "f7e46f585e0748ed9cb44fcdeac928c5",
      "762a63d6da05439a8b228472ff9fe0d6",
      "a4b1a2a4893d4228b6f2671bd8404dac",
      "e74910d4d9a948aabaaab244c332bbae",
      "72ac614760704fb282055894c556dee5",
      "823fb38cdd7349c084bf41d1e4248b76",
      "aa32dd8765f34b018b1a1be989816a6d",
      "9529dc49fdcb4317a94763060a13c4bd",
      "a87b89ab2bee410bbcbf62650c9850a7",
      "a2189a0606e745f1bd50a8528ab3e021"
     ]
    },
    "id": "f194a639",
    "outputId": "aa87d49e-0ae5-4342-9125-a379090631c2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd58ceeeadf047ef9e2e74eeda17af50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063243634a8b4f98b64a166e75f5dd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c4efd",
   "metadata": {
    "id": "105c4efd"
   },
   "source": [
    "`SFTTrainer`에 사전에 전처리된 데이터셋을 전달했으므로 미니 배치가 어떻게 구성되었는지 확인해 볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ffc0403",
   "metadata": {
    "id": "1ffc0403"
   },
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df",
   "metadata": {
    "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df"
   },
   "source": [
    "레이블(label)을 확인해 보죠. 무엇보다도 앞에서 직접 레이블을 지정하지 않았습니다. 그렇죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfaf09e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfaf09e2",
    "outputId": "70046a8a-a347-4340-9695-73d0ae54368a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "         32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "          5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "          1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "           935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "         32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "         29889, 32007, 32001,   512], device='cuda:0'),\n",
       " tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "         32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "          5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "          1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "           935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "         32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "         29889, 32007, 32001,   512], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedef238",
   "metadata": {
    "id": "eedef238"
   },
   "source": [
    "입력과 정확하게 동일한 값으로 레이블이 자동으로 추가되었습니다. 따라서 이는 **자기 지도 학습 미세 튜닝**(self-supervised fine-tuning)입니다.\n",
    "\n",
    "레이블을 하나씩 이동시키는 것도 자동으로 처리되기 때문에 이에 대해 신경쓸 필요가 없습니다.\n",
    "\n",
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    38억 개의 파라미터를 가진 모델이지만 앞의 설정을 사용하면 6GB RAM의 GTX 1060 같은 개인용 GPU에서 8개의 미니 배치로 훈련을 수행할 수 있습니다. 정말이에요!\n",
    "    <br>\n",
    "    이 경우 훈련을 완료하는데 약 35분이 걸립니다.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "그다음 `train()` 메서드를 호출합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12a099d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "b12a099d",
    "outputId": "315faf56-0c78-49c5-91cd-151188a1e534"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 01:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.250300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.8373394012451172, metrics={'train_runtime': 114.863, 'train_samples_per_second': 29.688, 'train_steps_per_second': 1.915, 'total_flos': 4890970340720640.0, 'train_loss': 0.8373394012451172, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b666a",
   "metadata": {
    "id": "2f4b666a"
   },
   "source": [
    "## 모델에게 질의하기\n",
    "\n",
    "이제 모델에게 짧은 문장을 제공하면 요다 말투의 문장이 생성되어야 합니다.\n",
    "\n",
    "이 모델은 적절하게 포맷팅된 입력이 필요합니다. (이 경우 `user`에 해당하는) 메시지 리스트를 만들고 모델이 대답할 차례라는 것을 알려 주어야 합니다.\n",
    "\n",
    "이것이 `add_generation_prompt` 매개변수의 목적입니다. 대화의 끝에 `<|assistant|>`를 추가하여 모델이 다음 단어를 예측하게 하고 `<|endoftext|>` 토큰이 예측될 때까지 계속 진행합니다.\n",
    "\n",
    "다음 헬퍼 함수는 (대화 포맷으로) 메시지를 만들고 채팅 템플릿을 적용한 후 마지막에 특수 토큰 `<|assistant|>`를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5854efcd",
   "metadata": {
    "id": "5854efcd"
   },
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ba48b",
   "metadata": {
    "id": "438ba48b"
   },
   "source": [
    "샘플 문장으로 프폼프트를 생성해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "480a479b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "480a479b",
    "outputId": "13ee0e74-14d0-4758-cf71-e7f10063b338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The Force is strong in you!<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773df8d",
   "metadata": {
    "id": "4773df8d"
   },
   "source": [
    "프롬프트가 제대로 만들어진 것 같습니다. 이를 사용해 응답을 생성해 보죠. `generate()` 함수는 다음과 같은 작업을 합니다:\n",
    "\n",
    "* 프롬프트를 토큰화하여 토큰 ID의 텐서를 만듭니다(채팅 템플릿으로 특수 토큰을 이미 추가했으므로 `add_special_tokens`를 `False`로 지정합니다).\n",
    "* 모델을 **평가 모드**(`evaluation mode`)로 설정합니다.\n",
    "* 모델의 `generate()` 메서드를 호출하여 출력(토큰 ID)을 생성합니다.\n",
    "  * 모델이 혼합 정밀도로 훈련되었다면 데이터 타입 간의 변환을 자동으로 처리하기 위해 `autocast()` 컨텍스트 관리자로 생성 과정을 감쌉니다.\n",
    "* 생성된 토큰 ID를 텍스트로 디코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65a53ea9",
   "metadata": {
    "id": "65a53ea9"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(prompt, add_special_tokens=False,\n",
    "                                return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(tokenized_input)\n",
    "    model.eval()\n",
    "    # 혼합 정밀도를 사용해 훈련하는 경우 autocast 컨택스트를 사용합니다.\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
    "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:\n",
    "        generation_output = model.generate(**tokenized_input,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           max_new_tokens=max_new_tokens)\n",
    "\n",
    "    output = tokenizer.batch_decode(generation_output,\n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6b3bf",
   "metadata": {
    "id": "e1f6b3bf"
   },
   "source": [
    "이제 모델이 실제로 요다체 문장을 생성하는지 테스트해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33dca746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33dca746",
    "outputId": "8040543f-35b6-4950-c0e8-ea7a28ee54f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[32010,   450, 11004,   338,  4549,   297,   366, 29991, 32007, 32001]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is! Yes, hrrrm.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e821c3",
   "metadata": {
    "id": "56e821c3"
   },
   "source": [
    "훌륭하네요! 잘 됩니다! 요다처럼, 모델이 말하네요. 흐음..\n",
    "\n",
    "축하합니다. 첫 번째 LLM을 미세 튜닝했습니다!\n",
    "\n",
    "Phi-3-mini-4k-instruct 모델에 작은 어댑터를 추가하여 요다 번역기를 만들었습니다! 멋지지 않나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dc809",
   "metadata": {
    "id": "898dc809"
   },
   "source": [
    "### 어댑터 저장하기\n",
    "\n",
    "훈련이 완료된 후 `Trainer` 객체의 `save_model()` 메서드를 호출해 어댑터를 디스크에 저장할 수 있습니다. 이 메서드는 지정된 폴더에 모든 것을 저장합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b41a86d2",
   "metadata": {
    "id": "b41a86d2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('local-phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae1c2d",
   "metadata": {
    "id": "aaae1c2d"
   },
   "source": [
    "다음과 같은 파일들이 저장됩니다:\n",
    "\n",
    "* 어댑터 설정(`adapter_config.json`)과 가중치(`adapter_model.safetensors`). 어댑터 자체 크기는 50MB에 불과합니다.\n",
    "* 훈련 매개변수값(`training_args.bin`)\n",
    "* 토크나이저(`tokenizer.json`와 `tokenizer.model`)와 설정(`tokenizer_config.json`), 특수 토큰(`added_tokens.json`와 `speciak_tokens_map.json`)\n",
    "* README 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "761d1ba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "761d1ba3",
    "outputId": "cd517902-e98a-4e43-fefd-c6974a3bc0e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adapter_config.json',\n",
       " 'added_tokens.json',\n",
       " 'chat_template.jinja',\n",
       " 'README.md',\n",
       " 'special_tokens_map.json',\n",
       " 'adapter_model.safetensors',\n",
       " 'tokenizer_config.json',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'tokenizer.model']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('local-phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014134ea",
   "metadata": {
    "id": "014134ea"
   },
   "source": [
    "이 장에서 만든 어댑터를 다른 사람과 공유하고 싶다면 이를 허깅 페이스 허브에 업로드할 수 있습니다. 먼저 쓰기 권한이 있는 액세스 토큰(access token)을 사용해 허깅 페이스에 로그인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33c1005a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482,
     "referenced_widgets": [
      "f301c6fcb80c420f9902f3614ea1e36f",
      "e94ee1e45317427f8e6f1ebc93ac3b46",
      "a834af212478491787f24371a9c36fc5",
      "92ad8eb828f1403ea1d44ed8f5b4b241",
      "4024e81ef46c49e7a7e024fc4a9952da",
      "23b215c0da16408bbf892931b39979ac",
      "d3ca3453d2994d059f4b7ee9ea1ba2ef",
      "2c6cb04dd821496fa9b2e01c66839a71",
      "b6f16f6e8c084dbdb158ef8364adddab",
      "d04fe5418b6b4e35b8cd933282c8031a",
      "1130df2cbbb94e55860eb8d9b72c768d",
      "a09fa9c6f9a1424a834bdbd1ec3336bb",
      "0f4bc888dd86460086cb98d90b719a6b",
      "d3be41ad78c4480d8b35d998d4269a99",
      "a685a9808f2646fb9add6c2341798d95",
      "315d52abab0b4e69b7cbfc993bb3942d",
      "d22428ad6b8243618bd69e9de80e822c"
     ]
    },
    "id": "33c1005a",
    "outputId": "900e9bca-608c-4878-907a-4649895f35b1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f301c6fcb80c420f9902f3614ea1e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5de92",
   "metadata": {
    "id": "38c5de92"
   },
   "source": [
    "위 코드를 실행하면 다음과 같이 액세스 토큰을 요청합니다:\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub0.png?raw=True)\n",
    "<center>Figure 0.1 - Logging into the Hugging Face Hub</center>\n",
    "\n",
    "A successful login should look like this (pay attention to the permissions):\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub1.png?raw=True)\n",
    "<center>Figure 0.2 - Successful Login</center>\n",
    "\n",
    "그다음 `trainer` 객체의 `push_to_hub()` 메서드를 사용해 자신의 허브 계정에 모두 업로드할 수 있습니다. 모델 이름은 훈련 매개변수 `output_dir` 값에 따라 정해집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e11e931a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433,
     "referenced_widgets": [
      "9684eb4564c74116af5a77bc1195ce38",
      "d37ae26e5b344a9390715e38ac0d9883",
      "242f4886f6e549698b58a82b6c56a268",
      "f533979f5e5a4d00ae5b0d5179694f4e",
      "c225118c327342848d6ffac97bba6817",
      "2d3f319061864b24b6e34144c9a26b02",
      "07784c7633ea4d66996ca8a2f511adac",
      "17186f88a4294f21b2c85f4e16692a70",
      "4dfe17493c74496fafb78111295da4dc",
      "5b4ecf1a4a0d4217bcd7bf62cd1f9a71",
      "88630ae6985b430ab11f8d558ce4b971",
      "a1bfdf6390584bc18713ab337ff423f2",
      "11b929379e4948ce95ad9ed54faa6311",
      "935984ab8f1a42b982b129931360be76",
      "2d750780aea24182b01f988c7ec37517",
      "efcadb64177d44bcba31ba7eaaa793a8",
      "05a78279f44a4579bf1f0e9e10fdc692",
      "1c2d323dc7d14fc5b082fcec31ee35f7",
      "321966abc0e8472b958bcaf16a660348",
      "ff48f62c22ca4b93ada816b04d06e85b",
      "26dbd3fa77394fccb5538e86b4894cca",
      "9eb1ba0a47f44202b5c1560750cb8d3b",
      "8541179814cd497d87ffd8006fa3c079",
      "8cfdc1e68ccc4c9b9b21b47f23d9e6fa",
      "a41f2c4b9d954a09918f5aa1d6a3cf5c",
      "80914631344b400cab8a013fae53b328",
      "01c72100fe81439283cb1b89565f892e",
      "643e1b4c74bb400995e0482c5ed9d491",
      "6eda71dfd6ee473ba67985a3388d024d",
      "b8673ee6ce904a33a828eabe53d27d45",
      "b57facd15a034e6e9bf15df08d85f188",
      "c7c36a262c1d42529a541ce1127b16f9",
      "5cb1ac863bb4415caff87a1c55e345ca",
      "17579705fbd64a0293b0111a6e11f6d1",
      "d0a416f61d1943a29470552d6eea62e1",
      "55e543a115c24408aef54118189827bb",
      "37dc4c0058bd46d18f0a9dfb6ed51c19",
      "a1c1bba3900f4fc0be2082f50467924e",
      "6465f28c9c1e42e2ad8e6bd8ad4ef318",
      "849058c7701f43f0901998cabf690bfa",
      "f019b9983e2e4fa3abc375d7af027c6b",
      "da16c339eafc4b1fb9635ddd65093717",
      "fd481e1c1dad4616b2e259948681f283",
      "55316efa43d840e18f95ea652eca94c5",
      "718b446ba8804994b1ac5107ca741df3",
      "1b2b3bdedb114ace84c5070e42854944",
      "3f63645cd7434aaa92b47a7c62112449",
      "0940e59aeeb74ae9a3edebf65e30350d",
      "77a3426d263241fe9b2a3f8116c44cc0",
      "3459d6a94e014a9db341d48a86790f90",
      "0d177ff427824c4fa7fea605c0239c5d",
      "54f4f35987584dc7b175d4ff46f9b6ca",
      "b14778195b054cc4b6ec4cbe4280692f",
      "bf58d09caca14e029d3518c3f9ef24cb",
      "3f3a1d4ea1bf4d87bed6fb360e65022d"
     ]
    },
    "id": "e11e931a",
    "outputId": "7533c44d-4d31-4655-b475-e107355ee18f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9684eb4564c74116af5a77bc1195ce38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bfdf6390584bc18713ab337ff423f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8541179814cd497d87ffd8006fa3c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...3-mini-yoda-adapter/tokenizer.model: 100%|##########|  500kB /  500kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17579705fbd64a0293b0111a6e11f6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...a-adapter/adapter_model.safetensors:   6%|5         | 2.81MB / 50.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718b446ba8804994b1ac5107ca741df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...mini-yoda-adapter/training_args.bin:   6%|5         |   343B / 6.16kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/haesun/phi3-mini-yoda-adapter/commit/c2b42765d827f21a9f92668a0a867192c69ee657', commit_message='End of training', commit_description='', oid='c2b42765d827f21a9f92668a0a867192c69ee657', pr_url=None, repo_url=RepoUrl('https://huggingface.co/haesun/phi3-mini-yoda-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='haesun/phi3-mini-yoda-adapter'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca8a42",
   "metadata": {
    "id": "a5ca8a42"
   },
   "source": [
    "이게 끝입니다! 모델이 세상에 공개되었고 누구나 이 모델을 사용해 영어 문장을 요다체로 바꿀 수 있습니다!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
