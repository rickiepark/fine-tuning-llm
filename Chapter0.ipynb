{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b48542",
      "metadata": {
        "id": "88b48542"
      },
      "source": [
        "## 0장 LLM 미세 튜닝 레시피\n",
        "\n",
        "### 스포일러\n",
        "\n",
        "이 장에서는 바로 본론으로 들어가 소규모 언어 모델인 마이크로소프트의 Phi-3-mini-4k-instruct 모델을 미세 튜닝하여 영어를 요다체(Yoda-speak)로 바꾸어 보겠습니다. 이 첫 번째 장을 그냥 따라하는 일종의 레시피로 생각할 수 있습니다. \"일단 해보고 궁금한 건 나중에 알아보자\" 식의 장이죠.\n",
        "\n",
        "이 장에서는 다음과 같은 내용을 배웁니다.\n",
        "\n",
        "- BitsAndBytes를 사용해 양자화된 모델(quantized model)을 로드합니다.\n",
        "- 허깅 페이스(Hugging Face)의 peft를 사용해 LoRA(low-rank adapter)를 설정합니다.\n",
        "- 데이터셋을 로드하고 포맷팅합니다.\n",
        "- 허깅 페이스의 trl에서 제공하는 SFTTrainer 클래스를 사용해 모델을 지도 학습 미세 튜닝(supervised fine-tuning)합니다.\n",
        "- 미세 튜닝된 모델을 사용해 요다체 문장을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb88c3d",
      "metadata": {
        "id": "5cb88c3d"
      },
      "source": [
        "### 설정\n",
        "\n",
        "훈련 재현성을 위해 책에서 사용하는 버전과 동일한 버전을 설치합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2a165c97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a165c97",
        "outputId": "ee13c982-4ba8-41f0-af65-f2a8b7942fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.55.2 in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: peft==0.17.0 in /usr/local/lib/python3.12/dist-packages (0.17.0)\n",
            "Requirement already satisfied: accelerate==1.10.0 in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Collecting trl==0.21.0\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes==0.47.0\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface-hub==0.34.4 in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: safetensors==0.6.2 in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (3.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (0.21.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (3.2.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.12.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.0) (3.0.2)\n",
            "Downloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, trl\n",
            "Successfully installed bitsandbytes-0.47.0 trl-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.55.2 peft==0.17.0 accelerate==1.10.0 trl==0.21.0 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "import os\n",
        "import transformers\n",
        "import datasets\n",
        "\n",
        "# tqdm 비활성화\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "transformers.utils.logging.disable_progress_bar()\n",
        "datasets.utils.logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "C_kYzm7QZz2a"
      },
      "id": "C_kYzm7QZz2a",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ab94a803",
      "metadata": {
        "id": "ab94a803"
      },
      "source": [
        "### 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b9d45274",
      "metadata": {
        "id": "b9d45274",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTConfig, SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a64ffab5",
      "metadata": {
        "id": "a64ffab5"
      },
      "source": [
        "## 양자화된 베이스 모델 로드하기\n",
        "\n",
        "먼저 GPU RAM을 덜 차지하는 양자화된 모델을 로드합니다. 양자화된 모델은 원본 가중치를 더 적은 비트로 표현된 근삿값으로 바꾼 것입니다. 모델을 양자화하는 가장 간단한 방법은 가중치를 32비트 부동 소수점(FP32) 숫자에서 4비트 부동 소수점(NF4)로 바꾸는 것입니다. 간단하지만 이런 강력한 변경이 모델의 메모리 사용량을 약 8배나 줄입니다.\n",
        "\n",
        "`from_pretrained()` 메서드를 사용해 모델을 로드할 때 `quantization_config` 매개변수에 `BitsAndBytesConfig` 객체를 전달합니다. 여러 모델을 자유롭게 테스트할 수 있도록 허깅 페이스의 `AutoModelForCausalLM`를 사용하겠습니다. 이 클래스를 허깅 페이스에 있는 어떤 저장소를 선택하는지에 따라 로드되는 모델이 결정됩니다.\n",
        "\n",
        "양자화된 모델을 로드하는 코드는 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f5ad7668",
      "metadata": {
        "id": "f5ad7668"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.float32\n",
        ")\n",
        "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
        "                                             device_map=\"cuda:0\",\n",
        "                                             quantization_config=bnb_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26f487f",
      "metadata": {
        "id": "f26f487f"
      },
      "source": [
        "<blockquote class=\"note\">\n",
        "  <p>\n",
        "    <em>\"Phi-3-Mini-4K-Instruct는 38억 개의 파라미터를 가지고 있으며 Phi-3 데이터셋에서 훈련된 경량의 최신 오픈 소스 모델입니다. 이 데이터셋은 합성 데이터와 필터링된 공개 웹사이트의 데이터로 구성되며 고품질의 추론 중심 데이터입니다. 이 모델은 Phi-3 제품군에 속합니다. Mini 모델은 4K와 128K 문맥 길이(토큰 수)를 가진 버전이 있습니다.\"</em>\n",
        "    <br>\n",
        "    출처: <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">허깅 페이스</a>\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "모델이 로드된 후 `get_memory_footprint()` 메서드를 사용해 모델이 얼마만큼의 메모리를 차지하고 있는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b3ac411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b3ac411",
        "outputId": "60cdab42-41e1-4fff-fffc-37cc2ab600b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2206.341312\n"
          ]
        }
      ],
      "source": [
        "print(model.get_memory_footprint()/1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f09ff7",
      "metadata": {
        "id": "24f09ff7"
      },
      "source": [
        "양자화되었더라도 모델은 2기가바이트 이상의 RAM을 차지합니다. 양자화 과정은 주로 **트랜스포머 디코더 블록**(Transformer decoder block)(종종 층이라고두 부릅니다)에 있는 **선형 층**(linear layer)을 대상으로 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cdc64043",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc64043",
        "outputId": "6a1a8c17-e659-446d-f97c-f4e64c87ab97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Phi3ForCausalLM(\n",
              "  (model): Phi3Model(\n",
              "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x Phi3DecoderLayer(\n",
              "        (self_attn): Phi3Attention(\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
              "        )\n",
              "        (mlp): Phi3MLP(\n",
              "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "          (activation_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): Phi3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1461b229",
      "metadata": {
        "id": "1461b229"
      },
      "source": [
        "**양자화된 모델**(quantized model)은 바로 추론에 사용할 수 있지만 추가적으로 더 훈련할 수는 없습니다. 양자화의 핵심인 Linear4bit 층이 메모리를 적게 차지하지만 업데이트할 수 없기 때문입니다.\n",
        "\n",
        "대신 어댑터를 추가하여 모델의 동작을 바꿀 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f9e6061",
      "metadata": {
        "id": "0f9e6061"
      },
      "source": [
        "## LoRA 설정하기\n",
        "\n",
        "LoRA(Low-Rank Adapter) 어댑터를 양자화된 각 층에 추가할 수 있습니다. **어댑터**(adapter)는 일반적인 Linear 층과 거의 비슷하며 쉽게 업데이트할 수 있습니다. 여기서 기발한 점은 이 어댑터가 양자화된 층보다 훨씬 작다는 것입니다.\n",
        "\n",
        "양자화된 층이 동결(frozen)되었기 때문에 (즉, 업데이트되지 않으므로) 양자화된 모델에 LoRA 어댑터를 추가했을 때 훈련 가능한 파라미터의 전체 개수가 원래의 1% (또는 그 미만으로) 크게 줄어 듭니다.\n",
        "\n",
        "LoRA 어댑터를 설정하는 단계는 세 개입니다.\n",
        "\n",
        "* 훈련 과정에서 수치 안정성을 향상시키기 위해 `prepare_model_for_kbit_training()`를 호출합니다.\n",
        "* `LoraConfig` 인스턴스를 만듭니다.\n",
        "* `get_peft_model()` 메서드를 사용해 양자화된 베이스 모델(base model)에 이 설정을 적용합니다.\n",
        "\n",
        "이 단계를 앞서 로드한 모델에 적용해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3edc24ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3edc24ed",
        "outputId": "b179b723-c969-41be-e095-87ce902948a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Phi3ForCausalLM(\n",
              "      (model): Phi3Model(\n",
              "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x Phi3DecoderLayer(\n",
              "            (self_attn): Phi3Attention(\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (qkv_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): Phi3MLP(\n",
              "              (gate_up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (activation_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "        (rotary_emb): Phi3RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,                   # 어댑터의 랭크(rank)가 작을수록 훈련할 파라미터가 적습니다.\n",
        "    lora_alpha=16,         # 일반적으로 2*r\n",
        "    bias=\"none\",           # 주의: 편향을 훈련하면 베이스 모델의 동작이 바뀔 수 있습니다.\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # 이 글을 쓰는 시점에 Phi-3와 같은 최신 모델은\n",
        "    # 대상 모듈을 수동으로 지정해야 합니다.\n",
        "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611f1cf6",
      "metadata": {
        "id": "611f1cf6"
      },
      "source": [
        "다른 LoRA 층(`qkv_proj`, `gate_up_proj`, `down_proj`)은 출력 길이를 짧게하기 위해 간단하게 나타냈습니다.\n",
        "\n",
        "<blockquote class=\"warning\">\n",
        "  <p>\n",
        "    혹시 다음과 같은 오류가 발생했나요?\n",
        "    <br>\n",
        "    <br>\n",
        "    <tt>ValueError: Please specify `target_modules` in `peft_config`</tt>\n",
        "    <br>\n",
        "    <br>\n",
        "    유명한 모델을 사용한다면 대부분 target_modules를 지정할 필요가 없습니다. peft 라이브러리가 자동으로 적절한 대상을 선택합니다. 하지만 인기있는 모델이 출시되는 시점과 라이브러리가 업데이트되는 시점 사이에는 간격이 있을 수 있습니다. 따라서 위와 같은 오류를 만났다면 모델에 있는 양자화된 층을 찾아 target_modules 매개변수에 해당 층의 이름을 입력하세요.\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "양자화된 층(`Linear4bit`)이 `lora.Linear4bit`로 바뀌었습니다. 이 층은 양자화된 층인 `base_layer`와 일반적인 `Linear` 층(`lora_A`와 `lora_B`)을 섞은 것입니다.\n",
        "\n",
        "추가된 층은 모델의 크기를 조금만 더 증가시킵니다. 하지만 prepare_model_for_kbit_training() 함수가 양자화되지 않은 다른 모든 층을 **단정밀도**(single precision)(**FP32**)로 바꿉니다. 이로 인해 모델이 20% 더 커집니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5f30ecbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f30ecbd",
        "outputId": "77e8f096-da7e-4c1e-9514-0a5aa925824f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2651.074752\n"
          ]
        }
      ],
      "source": [
        "print(model.get_memory_footprint()/1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e2718a6",
      "metadata": {
        "id": "2e2718a6"
      },
      "source": [
        "대부분의 파라미터가 동결되었기 때문에 전체 파라미터 중에서 적은 부분만 훈련됩니다. LoRA 만세!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c06c42b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c06c42b4",
        "outputId": "96f3e818-a347-4b62-b8d8-27c12c9a7d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 가능한 파라미터:             12.58M\n",
            "총 파라미터:                 3833.66M\n",
            "훈련 가능한 파라미터의 비율: 0.33%\n"
          ]
        }
      ],
      "source": [
        "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
        "print(f'훈련 가능한 파라미터:             {trainable_parms/1e6:.2f}M')\n",
        "print(f'총 파라미터:                 {tot_parms/1e6:.2f}M')\n",
        "print(f'훈련 가능한 파라미터의 비율: {100*trainable_parms/tot_parms:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149381b5",
      "metadata": {
        "id": "149381b5"
      },
      "source": [
        "모델을 미세 튜닝할 준비를 마쳤지만 한 가지 중요한 요소인 데이터셋이 아직 준비되지 않았습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2547d258",
      "metadata": {
        "id": "2547d258"
      },
      "source": [
        "## 데이터셋 포맷팅하기\n",
        "\n",
        "<blockquote style=\"quotes: none !important;\">\n",
        "  <p>\n",
        "    <em>\"요다 처럼, 말해라, 반드시. 흐음.\"</em>\n",
        "    <br>\n",
        "    <br>\n",
        "    마스터 요다\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "[`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) 데이터셋은 영어를 요다체로 바꾼 720개 문장으로 구성되어 있습니다. 이 데이터셋은 허깅 페이스 허브(Hugging Face Hub)에서 다운받을 수 있으며 `datasets` 라이브러리의 load_dataset() 메서드로 손쉽게 로드할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a3251cca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3251cca",
        "outputId": "21e35104-33fc-4c0e-e5bc-c698d6b64079"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'translation', 'translation_extra'],\n",
              "    num_rows: 720\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1da3b11",
      "metadata": {
        "id": "a1da3b11"
      },
      "source": [
        "이 데이터셋에는 세 개의 열이 있습니다:\n",
        "\n",
        "* 원본 영어 문장 (`sentence`)\n",
        "* 요다체로 바꾼 문장(`translation`)\n",
        "* Yesss와 Hrrmm 감탄사를 포함한 향상된 번역(`translation_extra`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2c804839",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c804839",
        "outputId": "fc7bac6c-1177-4007-db69-b7b43a288bd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
              " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
              " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37005b36",
      "metadata": {
        "id": "37005b36"
      },
      "source": [
        "모델을 미세 튜팅하기 위해 사용할 `SFTTrainer` 클래스가 자동으로 데이터셋을 **대화 포맷**(conversational format)으로 처리할 수 있습니다.\n",
        "\n",
        "```\n",
        "{\"messages\":[\n",
        "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
        "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
        "]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YMqs7yPqbO_R",
      "metadata": {
        "id": "YMqs7yPqbO_R"
      },
      "source": [
        "이전 버전에서는 `prompt`와 `completion` 열만 두 개만 있으면 데이터셋이 자동으로 대화 포맷으로 변환되었습니다. 이제는 더이상 이 방식이 지원되지 않으므로 직접 변환하는 것이 좋습니다. 다음에 나오는 `format_dataset()` 함수는 `trl` 패키지를 참고하여 작성한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eca9e9fa",
      "metadata": {
        "id": "eca9e9fa"
      },
      "outputs": [],
      "source": [
        "# trl.extras.dataset_formatting.instructions_formatting_function을 참고함.\n",
        "# 데이터셋을 (더이상 지원되지 않는) prompt/completion 포맷에서 대화 포맷으로 변경합니다.\n",
        "def format_dataset(examples):\n",
        "    if isinstance(examples[\"prompt\"], list):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples[\"prompt\"])):\n",
        "            converted_sample = [\n",
        "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
        "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
        "            ]\n",
        "            output_texts.append(converted_sample)\n",
        "        return {'messages': output_texts}\n",
        "    else:\n",
        "        converted_sample = [\n",
        "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
        "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
        "        ]\n",
        "        return {'messages': converted_sample}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P6Uqsd1ubZbl",
      "metadata": {
        "id": "P6Uqsd1ubZbl"
      },
      "source": [
        "이 함수는 `prompt`와 `completion` 열을 가진 샘플을 기대하므로 데이터셋에 적용하기 전에 열 이름을 바꾸어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "601b15b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "601b15b7",
        "outputId": "a1636c74-2db5-4811-97c2-a8aba3e5414f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'},\n",
              " {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "dataset = dataset.map(format_dataset)\n",
        "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"]) # Sorry, I forgot to add \"translation\"\n",
        "messages = dataset[0]['messages']\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288ab1a7",
      "metadata": {
        "id": "288ab1a7"
      },
      "source": [
        "### 토크나이저\n",
        "\n",
        "실제 훈련으로 넘어가기 전에 이 모델에 해당하는 **토크나이저**(tokenizer)를 로드해야 합니다. 토크나이저는 전체 과정에서 중요한 역할을 담당하며, 모델을 훈련할 때와 동일한 방식으로 텍스트를 토큰(token)으로 바꾸어 줍니다.\n",
        "\n",
        "지시와 채팅 모델의 경우 토크나이저에는 **채팅 템플릿**(chat template)도 담겨 있으며, 이를 통해 다음과 같은 내용을 알 수 있습니다.\n",
        "\n",
        "- 사용할 **특수 토큰**과 위치\n",
        "- 시스템 지시 사항, **사용자 프롬프트**(prompt), 모델 응답의 위치\n",
        "- **생성 프롬프트**, 즉 모델의 응답을 트리거(trigger)하는 특수 토큰(\"모델에게 질의하기\" 절에서 자세히 다룹니다)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faOWLpDGbvpU",
      "metadata": {
        "id": "faOWLpDGbvpU"
      },
      "source": [
        "**<중요>**\n",
        "EOS 토큰을 고유하게 유지하는 것이 중요합니다. Phi-3 같은 일부 모델에서는 EOS 토큰이 PAD 토큰으로도 사용됩니다. 이로 인해 훈련 중에 EOS 토큰이 마스킹되어 토큰 생성이 끊임없이 이어질 수 있습니다. 이 문제를 피하기 위해 UNK 토큰을 PAD 토큰으로 할당합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a647f985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "a647f985",
        "outputId": "29dbb4f6-a85a-4d2d-f31a-dfe39dd03b79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "# EOS 토큰을 고유하게 만들기 위해 UNK 토큰을 PAD 토큰으로 할당합니다.\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "tokenizer.chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4f7d28",
      "metadata": {
        "id": "1b4f7d28"
      },
      "source": [
        "복잡해 보이는 템플릿은 신경쓰지 마세요(보기 좋도록 줄바꿈과 들여쓰기를 추가했습니다). 토크나이저는 다음처럼 메시지를 적절한 태그를 사용해 일관성 있는 블록으로 구성합니다(`tokenize=False`로 지정하면 숫자 토큰 ID의 시퀀스가 아니라 텍스트가 반환됩니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2b2cbfd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b2cbfd6",
        "outputId": "1d5ba27c-d5b0-48fc-8586-95f4508b4f33",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "The birch canoe slid on the smooth planks.<|end|>\n",
            "<|assistant|>\n",
            "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44fd5c6",
      "metadata": {
        "id": "a44fd5c6"
      },
      "source": [
        "\n",
        "각 대화는 `<|user|>` 또는 `<|assistant|>`로 시작하고 `<|end|>`로 끝납니다. 또한 `<|endoftext|>`는 전체 블록의 끝을 나타냅니다.\n",
        "\n",
        "모델이 다르면 템플릿이 다르고, 문장과 블록의 시작과 끝을 나타내는 토큰이 다를 수 있습니다.\n",
        "\n",
        "이제 미세 튜닝을 수행할 준비를 마쳤습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ada15ad",
      "metadata": {
        "id": "4ada15ad"
      },
      "source": [
        "## SFTTrainer를 사용해 미세 튜닝하기\n",
        "\n",
        "모델의 규모에 상관없이 미세 튜닝은 모델을 처음부터 훈련하는 것과 정확히 동일한 훈련 과정을 거칩니다. 파이토치로 훈련 루프를 직접 구현하거나 허깅 페이스의 `Trainer` 클래스를 사용해 모델을 미세 튜닝할 수 있습니다.\n",
        "\n",
        "하지만 (`Trainer`를 상속한) `SFTTrainer`를 사용하는 것이 훨씬 쉽습니다. 다음의 네 가지 매개변수 값을 제공하기만 하면 대부분의 세부 사항을 처리해 주기 때문입니다.\n",
        "\n",
        "* 모델\n",
        "* 토크나이저\n",
        "* 데이터셋\n",
        "* 설정 객체\n",
        "\n",
        "처음 세 개는 이미 준비되었으므로 마지막 설정 객체를 만들어 보죠.\n",
        "\n",
        "### SFTConfig\n",
        "\n",
        "설정 객체에는 많은 매개변수가 있습니다. 이를 네 개의 그룹으로 나누어 보죠:\n",
        "\n",
        "* 메모리 사용 최적화 매개변수는 **그레이디언트 누적**(gradient accumulation) 및 **그레이디언트 체크포인팅**(gradient checkpointing)과 관련이 있습니다.\n",
        "* `max_seq_length`와 같은 데이터셋 관련 매개변수와 시퀀스 패킹(packing) 여부\n",
        "* `learning_rate` 및 `num_train_epochs` 같은 일반적인 훈련 매개변수\n",
        "* `output_dir`(모델을 훈련한 다음 허깅 페이스 허브에 저장할 경우 모델의 이름으로 사용됩니다), `logging_dir`, `logging_steps`와 같은 환경 및 로깅 매개변수\n",
        "\n",
        "**학습률**(learning rate)이 매우 중요한 매개변수입니다(처음에는 베이스 모델 훈련에 사용한 학습률을 시도해볼 수 있습니다). 하지만 실제로는 **최대 시퀀스 길이**(maximum sequence length)가 메모리 부족 문제를 일으킬 가능성이 더 높습니다.\n",
        "\n",
        "주어진 문제에서 가능한 가장 짧은 max_seq_length를 선택하세요. 여기에서는 영어와 요다체 문장 모두 매우 짧습니다. 64개 토큰이면 프롬프트, 텍스트 완성, 특수 토큰을 포함하기에 충분합니다.\n",
        "\n",
        "<blockquote class=\"tip\">\n",
        "  <p>\n",
        "    나중에 보게 되겠지만 플래시 어텐션(flash attention)을 사용하면 메모리 부족 문제를 피하면서 더 긴 시퀀스를 사용할 수 있습니다.\n",
        "  </p>\n",
        "</blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7a9979b2",
      "metadata": {
        "id": "7a9979b2"
      },
      "outputs": [],
      "source": [
        "sft_config = SFTConfig(\n",
        "    ## 그룹 1: 메모리 사용\n",
        "    # 이 매개변수들은 GPU RAM을 최대한 활용하도록 돕습니다.\n",
        "    # 체크포인팅\n",
        "    gradient_checkpointing=True,   # 메모리가 많이 절약됩니다.\n",
        "    # 파이토치 새 버전에서 예외를 피하기 위해 지정합니다.\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "    # 그레이디언트 누적과 배치 크기\n",
        "    # (업데이트를 위한) 실제 배치 크기는 마이크로 배치 크기와 같습니다.\n",
        "    gradient_accumulation_steps=1,\n",
        "    # 초기 (마이크로) 배치 크기\n",
        "    per_device_train_batch_size=16,\n",
        "    # 배치 크기가 메모리 부족을 일으키면 문제가 해결될 때까지 반으로 나눕니다.\n",
        "    auto_find_batch_size=True,\n",
        "\n",
        "    ## 그룹 2: 데이터셋 관련\n",
        "    max_length=64,\n",
        "    # 데이터셋 패킹을 한다는 것은 패딩이 필요 없다는 의미입니다.\n",
        "    packing=True,\n",
        "\n",
        "    ## 그룹 3: 일반적인 훈련 매개변수\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=3e-4,\n",
        "    # 8-비트 Adam 옵티마이저 - LoRA를 사용하는 경우 큰 도움이 되지 않습니다!\n",
        "    optim='paged_adamw_8bit',\n",
        "\n",
        "    ## 그룹 4: 로깅 매개변수\n",
        "    logging_steps=10,\n",
        "    logging_dir='./logs',\n",
        "    output_dir='./phi3-mini-yoda-adapter',\n",
        "    report_to='none',\n",
        "\n",
        "    ## 그외\n",
        "    # 가능한 경우에만 bf16으로 훈련하도록\n",
        "    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dmxdC7iEcWP-",
      "metadata": {
        "id": "dmxdC7iEcWP-"
      },
      "source": [
        "**<중요>** 이 글을 쓰는 시점에 `trl`의 버전(0.21)은 지원 환경 여부에 상관없이 기본적으로 bf16 데이터 타입을 사용해 혼합 정밀도 훈련을 수행합니다. 문제를 방지하기 위해 GPU 지원 여부에 따라 bf16 매개변수를 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971e3099",
      "metadata": {
        "id": "971e3099"
      },
      "source": [
        "### `SFTTrainer`\n",
        "\n",
        "<blockquote style=\"quotes: none !important;\">\n",
        "  <p>\n",
        "    <em>\"이제 훈련 시간이다!\"</em>\n",
        "    <br>\n",
        "    <br>\n",
        "    헐크\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "드디어 지도 학습 미세 튜닝을 위한 훈련 객체를 만들 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f194a639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f194a639",
        "outputId": "eb743d9e-b5d9-45c6-a1cb-a3f9c51fbaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model.base_model.model,\n",
        "    # `trl` 패키지 이슈 때문에 다음 매개변수를 포함해야 합니다.\n",
        "    peft_config=config,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nLjfRsR0cncP",
      "metadata": {
        "id": "nLjfRsR0cncP"
      },
      "source": [
        "**<중요>**\n",
        "이 글을 쓰는 시점에 trl 버전(0.21)은 LoRA 설정이 모델에 이미 적용되어 있을 때 훈련이 실패하는 문제가 있습니다. 이는 훈련 객체가 어댑터를 포함하여 모델 전체를 동결하기 때문입니다.\n",
        "\n",
        "하지만 peft_config 매개변수로 설정을 훈련 객체에 전달하면 기존 층을 동결한 후에 이를 적용하기 때문에 올바르게 동작합니다.\n",
        "\n",
        "이 예제에서처럼 모델이 어댑터를 이미 포함하고 있다면 훈련은 되겠지만 save_model() 메서드가 올바르게 동작하도록 name_or_path 설정을 바꾸어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "u6dgbN1Pch4P",
      "metadata": {
        "id": "u6dgbN1Pch4P"
      },
      "outputs": [],
      "source": [
        "# original_name = model.base_model.model.__dict__[\"name_or_path\"]\n",
        "# model.__dict__[\"name_or_path\"] = original_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105c4efd",
      "metadata": {
        "id": "105c4efd"
      },
      "source": [
        "`SFTTrainer`에 사전에 전처리된 데이터셋을 전달했으므로 미니 배치가 어떻게 구성되었는지 확인해 볼 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1ffc0403",
      "metadata": {
        "id": "1ffc0403"
      },
      "outputs": [],
      "source": [
        "dl = trainer.get_train_dataloader()\n",
        "batch = next(iter(dl))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df",
      "metadata": {
        "id": "6cf1481e-59a4-4552-aa7f-61a15d9ee2df"
      },
      "source": [
        "레이블(label)을 확인해 보죠. 무엇보다도 앞에서 직접 레이블을 지정하지 않았습니다. 그렇죠?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dfaf09e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfaf09e2",
        "outputId": "2bafddb5-eed9-4a28-9ba2-1912d39f4ba8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([32010,   450,   270,  ..., 29889, 32007, 32000], device='cuda:0'),\n",
              " tensor([32010,   450,   270,  ..., 29889, 32007, 32000], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "batch['input_ids'][0], batch['labels'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedef238",
      "metadata": {
        "id": "eedef238"
      },
      "source": [
        "입력과 정확하게 동일한 값으로 레이블이 자동으로 추가되었습니다. 따라서 이는 **자기 지도 학습 미세 튜닝**(self-supervised fine-tuning)입니다.\n",
        "\n",
        "레이블을 하나씩 이동시키는 것도 자동으로 처리되기 때문에 이에 대해 신경쓸 필요가 없습니다.\n",
        "\n",
        "<blockquote class=\"note\">\n",
        "  <p>\n",
        "    38억 개의 파라미터를 가진 모델이지만 앞의 설정을 사용하면 6GB RAM의 GTX 1060 같은 개인용 GPU에서 8개의 미니 배치로 훈련을 수행할 수 있습니다. 정말이에요!\n",
        "    <br>\n",
        "    이 경우 훈련을 완료하는데 약 35분이 걸립니다.\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "그다음 train() 메서드를 호출합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b12a099d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "b12a099d",
        "outputId": "113250f8-cec8-44ea-8f1e-52266778466b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [230/230 01:57, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.066500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.233100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.161900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.155900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.011800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.970200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.737600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.517900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.422300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.351600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.353700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.324700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.294400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.292300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.287500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.286800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.269500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.276400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=230, training_loss=0.6582981410233871, metrics={'train_runtime': 119.6168, 'train_samples_per_second': 30.013, 'train_steps_per_second': 1.923, 'total_flos': 4885815817359360.0, 'train_loss': 0.6582981410233871})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4b666a",
      "metadata": {
        "id": "2f4b666a"
      },
      "source": [
        "## 모델에게 질의하기\n",
        "\n",
        "이제 모델에게 짧은 문장을 제공하면 요다 말투의 문장이 생성되어야 합니다.\n",
        "\n",
        "이 모델은 적절하게 포맷팅된 입력이 필요합니다. (이 경우 `user`에 해당하는) 메시지 리스트를 만들고 모델이 대답할 차례라는 것을 알려 주어야 합니다.\n",
        "\n",
        "이것이 `add_generation_prompt` 매개변수의 목적입니다. 대화의 끝에 `<|assistant|>`를 추가하여 모델이 다음 단어를 예측하게 하고 `<|endoftext|>` 토큰이 예측될 때까지 계속 진행합니다.\n",
        "\n",
        "다음 헬퍼 함수는 (대화 포맷으로) 메시지를 만들고 채팅 템플릿을 적용한 후 마지막에 특수 토큰 `<|assistant|>`를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5854efcd",
      "metadata": {
        "id": "5854efcd"
      },
      "outputs": [],
      "source": [
        "def gen_prompt(tokenizer, sentence):\n",
        "    converted_sample = [\n",
        "        {\"role\": \"user\", \"content\": sentence},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
        "                                           tokenize=False,\n",
        "                                           add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438ba48b",
      "metadata": {
        "id": "438ba48b"
      },
      "source": [
        "샘플 문장으로 프폼프트를 생성해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "480a479b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480a479b",
        "outputId": "38a45458-def5-4044-bf2e-7b68653cd3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "The Force is strong in you!<|end|>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence = 'The Force is strong in you!'\n",
        "prompt = gen_prompt(tokenizer, sentence)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4773df8d",
      "metadata": {
        "id": "4773df8d"
      },
      "source": [
        "프롬프트가 제대로 만들어진 것 같습니다. 이를 사용해 응답을 생성해 보죠. `generate()` 함수는 다음과 같은 작업을 합니다:\n",
        "\n",
        "* 프롬프트를 토큰화하여 토큰 ID의 텐서를 만듭니다(채팅 템플릿으로 특수 토큰을 이미 추가했으므로 `add_special_tokens`를 `False`로 지정합니다).\n",
        "* 모델을 **평가 모드**(`evaluation mode`)로 설정합니다.\n",
        "* 모델의 `generate()` 메서드를 호출하여 출력(토큰 ID)을 생성합니다.\n",
        "* 생성된 토큰 ID를 텍스트로 디코딩합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "65a53ea9",
      "metadata": {
        "id": "65a53ea9"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
        "    tokenized_input = tokenizer(prompt, add_special_tokens=False,\n",
        "                                return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    print(tokenized_input)\n",
        "    model.eval()\n",
        "    # 혼합 정밀도를 사용해 훈련하는 경우 autocast 컨택스트를 사용합니다.\n",
        "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
        "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
        "    with ctx:\n",
        "        generation_output = model.generate(**tokenized_input,\n",
        "                                           eos_token_id=tokenizer.eos_token_id,\n",
        "                                           max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output = tokenizer.batch_decode(generation_output,\n",
        "                                    skip_special_tokens=skip_special_tokens)\n",
        "    return output[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f6b3bf",
      "metadata": {
        "id": "e1f6b3bf"
      },
      "source": [
        "이제 모델이 실제로 요다체 문장을 생성하는지 테스트해 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "33dca746",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33dca746",
        "outputId": "e02b0282-26fa-42bd-e6f4-69a2bb0e135e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[32010,   450, 11004,   338,  4549,   297,   366, 29991, 32007, 32001]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is!<|end|><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, tokenizer, prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e821c3",
      "metadata": {
        "id": "56e821c3"
      },
      "source": [
        "훌륭하네요! 잘 됩니다! 요다처럼, 모델이 말하네요. 흐음..\n",
        "\n",
        "축하합니다. 첫 번째 LLM을 미세 튜닝했습니다!\n",
        "\n",
        "Phi-3-mini-4k-instruct 모델에 작은 어댑터를 추가하여 요다 번역기를 만들었습니다! 멋지지 않나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898dc809",
      "metadata": {
        "id": "898dc809"
      },
      "source": [
        "### 어댑터 저장하기\n",
        "\n",
        "훈련이 완료된 후 `Trainer` 객체의 `save_model()` 메서드를 호출해 어댑터를 디스크에 저장할 수 있습니다. 이 메서드는 지정된 폴더에 모든 것을 저장합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b41a86d2",
      "metadata": {
        "id": "b41a86d2"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('local-phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaae1c2d",
      "metadata": {
        "id": "aaae1c2d"
      },
      "source": [
        "다음과 같은 파일들이 저장됩니다:\n",
        "\n",
        "* 어댑터 설정(`adapter_config.json`)과 가중치(`adapter_model.safetensors`). 어댑터 자체 크기는 50MB에 불과합니다.\n",
        "* 훈련 매개변수값(`training_args.bin`)\n",
        "* 토크나이저(`tokenizer.json`와 `tokenizer.model`)와 설정(`tokenizer_config.json`), 특수 토큰(`added_tokens.json`와 `speciak_tokens_map.json`)\n",
        "* README 파일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "761d1ba3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "761d1ba3",
        "outputId": "c2be2de4-1bc9-459b-84da-5c56a6451692"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md',\n",
              " 'adapter_config.json',\n",
              " 'tokenizer.json',\n",
              " 'tokenizer.model',\n",
              " 'special_tokens_map.json',\n",
              " 'chat_template.jinja',\n",
              " 'training_args.bin',\n",
              " 'added_tokens.json',\n",
              " 'adapter_model.safetensors',\n",
              " 'tokenizer_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "os.listdir('local-phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014134ea",
      "metadata": {
        "id": "014134ea"
      },
      "source": [
        "이 장에서 만든 어댑터를 다른 사람과 공유하고 싶다면 이를 허깅 페이스 허브에 업로드할 수 있습니다. 먼저 쓰기 권한이 있는 액세스 토큰(access token)을 사용해 허깅 페이스에 로그인합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "33c1005a",
      "metadata": {
        "id": "33c1005a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "9e78176c38f5435c9229b1b8c464a8fa",
            "048a3420cf244ba695d103eb6e1c6d56",
            "5650347ac6d542379be0e14b0a91cc8b",
            "731543b5f22e4ba6aa3870d24c706bd2",
            "e11a1326ef554f73ab4e83eef82ae3ba",
            "447259008cf94fcfa9e26ce69d7bb589",
            "79a7621800dd4c2ca3ca647d9e423263",
            "058ffef98b384c178a51e9108ee8319e",
            "e6a63ef3a1d7404ab867175d8c7c8b60",
            "753744a7ae29492c9b95dfd4c00a4a40",
            "bb9badb6458b44b9aa574e61b71bde04",
            "1c87c8642e5a44a792053372824b8e5e",
            "564d6fe8a3654c8c9110ba61c28a74a3",
            "760eec4c6dcc41169d9975bceb0c336e",
            "59134436cc954403a8492cc664e2bb75",
            "58d1f4ca0795418dbcab228120378fa9",
            "6c7d7cf4c10f40978d75f11838c24faa"
          ]
        },
        "outputId": "67de6c08-9514-45b3-d3f8-604d50a0415d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e78176c38f5435c9229b1b8c464a8fa"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c5de92",
      "metadata": {
        "id": "38c5de92"
      },
      "source": [
        "위 코드를 실행하면 다음과 같이 액세스 토큰을 요청합니다:\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub0.png?raw=True)\n",
        "<center>Figure 0.1 - Logging into the Hugging Face Hub</center>\n",
        "\n",
        "A successful login should look like this (pay attention to the permissions):\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub1.png?raw=True)\n",
        "<center>Figure 0.2 - Successful Login</center>\n",
        "\n",
        "그다음 `trainer` 객체의 `push_to_hub()` 메서드를 사용해 자신의 허브 계정에 모두 업로드할 수 있습니다. 모델 이름은 훈련 매개변수 `output_dir` 값에 따라 정해집니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e11e931a",
      "metadata": {
        "id": "e11e931a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "49402684-b2e2-4f55-9032-9147336edb02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/haesun/phi3-mini-yoda-adapter/commit/bb97c3ccf1372885dfa5ba11fc259a7bb11b2c6f', commit_message='End of training', commit_description='', oid='bb97c3ccf1372885dfa5ba11fc259a7bb11b2c6f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/haesun/phi3-mini-yoda-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='haesun/phi3-mini-yoda-adapter'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ca8a42",
      "metadata": {
        "id": "a5ca8a42"
      },
      "source": [
        "이게 끝입니다! 모델이 세상에 공개되었고 누구나 이 모델을 사용해 영어 문장을 요다체로 바꿀 수 있습니다!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e78176c38f5435c9229b1b8c464a8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_048a3420cf244ba695d103eb6e1c6d56",
              "IPY_MODEL_5650347ac6d542379be0e14b0a91cc8b",
              "IPY_MODEL_731543b5f22e4ba6aa3870d24c706bd2",
              "IPY_MODEL_e11a1326ef554f73ab4e83eef82ae3ba",
              "IPY_MODEL_447259008cf94fcfa9e26ce69d7bb589"
            ],
            "layout": "IPY_MODEL_79a7621800dd4c2ca3ca647d9e423263"
          }
        },
        "048a3420cf244ba695d103eb6e1c6d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_058ffef98b384c178a51e9108ee8319e",
            "placeholder": "​",
            "style": "IPY_MODEL_e6a63ef3a1d7404ab867175d8c7c8b60",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5650347ac6d542379be0e14b0a91cc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_753744a7ae29492c9b95dfd4c00a4a40",
            "placeholder": "​",
            "style": "IPY_MODEL_bb9badb6458b44b9aa574e61b71bde04",
            "value": ""
          }
        },
        "731543b5f22e4ba6aa3870d24c706bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1c87c8642e5a44a792053372824b8e5e",
            "style": "IPY_MODEL_564d6fe8a3654c8c9110ba61c28a74a3",
            "value": true
          }
        },
        "e11a1326ef554f73ab4e83eef82ae3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_760eec4c6dcc41169d9975bceb0c336e",
            "style": "IPY_MODEL_59134436cc954403a8492cc664e2bb75",
            "tooltip": ""
          }
        },
        "447259008cf94fcfa9e26ce69d7bb589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d1f4ca0795418dbcab228120378fa9",
            "placeholder": "​",
            "style": "IPY_MODEL_6c7d7cf4c10f40978d75f11838c24faa",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "79a7621800dd4c2ca3ca647d9e423263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "058ffef98b384c178a51e9108ee8319e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a63ef3a1d7404ab867175d8c7c8b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "753744a7ae29492c9b95dfd4c00a4a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9badb6458b44b9aa574e61b71bde04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c87c8642e5a44a792053372824b8e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564d6fe8a3654c8c9110ba61c28a74a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "760eec4c6dcc41169d9975bceb0c336e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59134436cc954403a8492cc664e2bb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "58d1f4ca0795418dbcab228120378fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7d7cf4c10f40978d75f11838c24faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}