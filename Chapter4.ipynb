{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e06813f-79d8-4c0a-a823-c3dc2da65292",
   "metadata": {},
   "source": [
    "## 4장 데이터셋 포맷팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf67d8",
   "metadata": {},
   "source": [
    "### 스포일러\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- 적절한 채팅 템플릿의 중요성을 이해합니다.\n",
    "- 사용자 정의 포맷팅 함수와 템플릿을 포함해 몇 가지 포맷팅 옵션에 대해 논의합니다.\n",
    "- 토크나이저와 모델의 임베딩 층을 설정합니다.\n",
    "- 패킹(packing)된 데이터셋과 데이터 로딩을 위한 다양한 데이터 콜레이터(data collator)를 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628f687",
   "metadata": {},
   "source": [
    "### 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20be1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kODUm5BmEQhI",
    "outputId": "5a17ca8b-855a-4e55-b7a2-ab2af98d8906"
   },
   "outputs": [],
   "source": [
    "# 코랩을 사용하는 경우\n",
    "!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144dfaf",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a9ba58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, DataCollatorForLanguageModeling, DataCollatorWithPadding, DataCollatorWithFlattening, BitsAndBytesConfig\n",
    "from trl import setup_chat_format, DataCollatorForCompletionOnlyLM\n",
    "from trl.extras.dataset_formatting import FORMAT_MAPPING, instructions_formatting_function, conversations_formatting_function\n",
    "from trl.trainer import ConstantLengthDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd6389",
   "metadata": {},
   "source": [
    "### 목표\n",
    "\n",
    "LLM에게 데이터셋의 구조와 단서를 제공하기 위해 포맷팅을 합니다. 적절한 태그와 특수 토큰으로 각 구성 요소(사용자 프롬프트와 모델이 완성할 텍스트)를 감싸서 모델의 동작을 쉽게 조정(예를 들면 지시 미세 튜닝)할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bf61f",
   "metadata": {},
   "source": [
    "### 포맷팅의 핵심\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/base_prompt.png?raw=True)\n",
    "<center>그림 4.1 베이스 모델의 다음 토큰 예측</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/fine_tuned_prompt.png?raw=True)\n",
    "<center>그림 4.2 응답 템플릿을 사용하는 미세 튜닝된 모델</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_prompt_new.png?raw=True)\n",
    "<center>그림 4.3 채팅 템플릿을 사용하는 채팅 모델</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_example_new.png?raw=True)\n",
    "<center>그림 4.4 채팅 템플릿의 일반적인 구조</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a379eb0",
   "metadata": {},
   "source": [
    "### 이전 장에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3ec5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='cuda:0',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)\n",
    "\n",
    "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model_q4, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e76cbf",
   "metadata": {},
   "source": [
    "### 템플릿 적용하기\n",
    "\n",
    "****\n",
    "**\"템플릿 적용하기\" 절의 요약**\n",
    "\n",
    "데이터셋 포맷팅에 세 가지 옵션이 있습니다:\n",
    "1. 데이터셋의 포맷이 `STTrainer` 클래스가 지원하는 두 개의 포맷 중 하나인 경우\n",
    "   - 토크나이저가 채팅 템플릿을 가지고 있어야 합니다.\n",
    "   - 포맷팅 함수를 정의하거나 훈련 전에 데이터셋을 포맷팅할 필요가 없습니다.\n",
    "2. 사용자 정의 포맷팅 함수를 사용하고 싶은 경우(BYOFF 절 참고)\n",
    "   - `SFTTrainer` 클래스(5장 참조)의 `formatting_func` 매개변수로 사용자 정의 함수를 전달해야 합니다.\n",
    "   - 사용자 정의 포맷팅 함수가 배치 데이터를 다룰 수 있어야 합니다.\n",
    "     - `batched=True`로 데이터셋의 `map()` 메서드를 호출하여 테스트하세요.\n",
    "    - 훈련 전에 데이터셋에 이 함수를 적용할 필요는 없습니다.\n",
    "    - 토크나이저가 이미 채팅 템플릿을 가지고 있는 경우\n",
    "      - 사용자 정의 함수에서 `apply_chat_template()` 메서드를 호출할 수 있습니다.\n",
    "      - 템플릿의 일반적인 포맷(지시 템플릿과 응답 템플릿)을 고수하세요.\n",
    "      - 템플릿에 `EOS` 토큰이 포함되어 있지 않다면 포맷팅된 출력 끝에 `EOS` 토큰을 추가할 수 있습니다.\n",
    "   - 토크나이저가 채팅 템플릿을 가지고 있지 않은 경우\n",
    "     - 지시 템플릿과 응답 템플릿을 포함하여 일반적인 포맷을 자유롭게 정의할 수 있습니다('고급 방법 - BYOT' 절 참고).\n",
    "3. 데이터셋이 이미 포맷팅된 경우\n",
    "   - `SFTTrainer` 클래스(5장 참조)의 `dataset_text_field` 매개변수에 포맷팅된 데이터를 담고 있는 열을 전달해야 합니다.\n",
    "   - 사용자 정의 포맷팅 함수를 사용하여 데이터셋을 전처리 하더라도 훈련 클래스는 이를 사용하지 않습니다.\n",
    "   - 데이터가 토크나이저의 템플릿과 호환되는지 확인하세요.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f92fae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "print(tokenizer_phi.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e134f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
    "    {'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
    "    {'role': 'assistant', 'content': 'Buenos Aires.'}\n",
    "]\n",
    "\n",
    "formatted = tokenizer_phi.apply_chat_template(conversation=messages, \n",
    "                                          tokenize=False, \n",
    "                                          add_generation_prompt=False)\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5014cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_input = tokenizer_phi.apply_chat_template(conversation=messages[:-1], \n",
    "                                          tokenize=False, \n",
    "                                          add_generation_prompt=True)\n",
    "print(inference_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3a356",
   "metadata": {},
   "source": [
    "#### 지원 포맷\n",
    "\n",
    "##### 대화 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a40284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': Value(dtype='string', id=None),\n",
       "   'role': Value(dtype='string', id=None)}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_ds = Dataset.from_list([{'messages': messages}])\n",
    "conversation_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fd0626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORMAT_MAPPING['chatml'] == conversation_ds.features['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0129a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
    "\n",
    "print(formatting_func(conversation_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525cc9c",
   "metadata": {},
   "source": [
    "```python\n",
    "# 대화 포맷을 위한 포맷팅 함수\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[messages_field][0], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[messages_field])):\n",
    "            output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n",
    "        return output_texts\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371de955",
   "metadata": {},
   "source": [
    "##### 지시 포맷\n",
    "\n",
    "**중요 업데이트**: 안타깝게도 최근 `trl` 버전에서는 지시 포맷이 적절하게 지원되지 않아 데이터셋에 채팅 템플릿이 적용되지 않습니다. 이 문제를 피하기 위해 대신 대화 포맷을 사용하는 것이 권장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39cfd778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': Value(dtype='string', id=None),\n",
       " 'completion': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions = [{'prompt': 'What is the capital of Argentina?',\n",
    "                 'completion': 'Buenos Aires.'}]\n",
    "\n",
    "instruction_ds = Dataset.from_list(instructions)\n",
    "instruction_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aca678e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORMAT_MAPPING['instruction'] == instruction_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3fed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function trl.extras.dataset_formatting.instructions_formatting_function.<locals>.format_dataset(examples)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatting_func = instructions_formatting_function(tokenizer_phi)\n",
    "formatting_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f8d46",
   "metadata": {},
   "source": [
    "```python\n",
    "# 지시 포맷을 위한 포맷팅 함수\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(tokenizer.apply_chat_template(converted_sample, tokenize=False))\n",
    "        return output_texts\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(converted_sample, tokenize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8c7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts_completions = {\n",
    "    'prompt': ['What is the capital of Argentina?',\n",
    "               'What is the capital of the United States?'],\n",
    "    'completion': ['Buenos Aires.',\n",
    "                    'Washington D.C.']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "298fa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [{'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
    "     {'role': 'assistant', 'content': 'Buenos Aires.'}],\n",
    "    [{'role': 'user', 'content': 'What is the capital of the United States?'},\n",
    "     {'role': 'assistant', 'content': 'Washington D.C.'}]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50481b56",
   "metadata": {},
   "source": [
    "#### BYOFF (Bring Your Own Formatting Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c55159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byo_formatting_func1(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efbffc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a2e61ba5d64fffa8a48341fa0ed8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_msg = Dataset.from_dict({'messages': batch_messages})\n",
    "ds_msg.map(lambda v: tokenizer_phi(byo_formatting_func1(v)), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6488d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byo_formatting_func2(examples):\n",
    "    response_template = '### Answer:'\n",
    "    text = f\"### Question: {examples['prompt']}\\n{response_template} {examples['completion']}\"\n",
    "    text += tokenizer_phi.eos_token\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1f7c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the capital of Argentina?\n",
      "### Answer: Buenos Aires.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "ds_prompt = Dataset.from_dict(batch_prompts_completions)\n",
    "print(byo_formatting_func2(ds_prompt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bf14c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce4ebcfd66c4aefb48ed1271cde2743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named input_ids expected length 2 but got length 44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named input_ids expected length 2 but got length 44"
     ]
    }
   ],
   "source": [
    "# 이 코드는 예외를 일으킵니다.\n",
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func2(v)), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9591c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byo_formatting_func3(examples):\n",
    "    output_texts = []\n",
    "    response_template = '### Answer:'\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"### Question: {examples['prompt'][i]}\\n {response_template} {examples['completion'][i]}\"\n",
    "        text += tokenizer_phi.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0aeda6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea47244bb5e8428997404671ed088420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func3(v)), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b04e6a",
   "metadata": {},
   "source": [
    "#### BYOFD (Bring Your Own Formatted Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6a04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byofd_formatting_func(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {'text': output_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5772c71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e799118503a2433f9bbb3258c5e68709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['<|user|>\\nWhat is the capital of Argentina?<|end|>\\n<|assistant|>\\nBuenos Aires.<|end|>\\n<|endoftext|>',\n",
       " '<|user|>\\nWhat is the capital of the United States?<|end|>\\n<|assistant|>\\nWashington D.C.<|end|>\\n<|endoftext|>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ds = ds_msg.map(byofd_formatting_func, batched=True)\n",
    "formatted_ds['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78305ca1",
   "metadata": {},
   "source": [
    "#### 결론\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/formatting_flow.png?raw=True)\n",
    "\n",
    "<center>그림 4.5 - 적절한 포맷팅 방법 선택하기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753ab0a",
   "metadata": {},
   "source": [
    "### 토크나이저\n",
    "\n",
    "****\n",
    "**\"토크나이저\" 절의 요약**\n",
    "- 토크나이저의 어휘사전은 일반적으로 모델의 임베딩 층 크기보다 작습니다.\n",
    "  - 크기 차이는 임베딩 층의 크기를 바꾸지 않고도 새로운 토큰을 추가할 수 있는 빈 슬롯 때문입니다.\n",
    "  - 효율적인 메모리 할당을 위해 임베딩 층의 크기는 2의 거듭제곱의 배수(32, 64 등)인 경우가 많습니다.\n",
    "- `EOS` 토큰은 다른 것 말고 텍스트의 끝을 나타내는데만 사용해야 합니다.\n",
    "  - 패딩을 위해 `EOS` 토큰을 사용하면 토큰 생성이 끊임없이 계속되는 문제가 발생할 수 있습니다.\n",
    "- `PAD` 토큰을 정의하지 않는 경우가 많지만 여전히 이 토큰이 필요합니다.\n",
    "  - `EOS` 토큰을 `PAD` 토큰으로 할당하지 마세요.\n",
    "  - `UNK` 토큰이 정의되어 있다면 이를 `PAD` 토큰으로 할당해도 괜찮습니다.\n",
    "  - `UNK` 토큰이 정의되어 있지 않다면 `PAD` 토큰을 위해 새로운 특수 토큰을 만드세요.\n",
    "  - 주의: `PAD` 토큰을 정의하지 않은 채로 두면 많은 라이브러리에서 기본적으로 `EOS` 토큰을 패딩 토큰으로 할당합니다!!\n",
    "- 생성 모델의 경우 패딩은 왼쪽에 추가되어야 합니다.\n",
    "  - 오른쪽에 패딩하면 모델이 패딩 토큰의 시퀀스를 생성하도록 훈련됩니다.\n",
    "  - `SFTTrainer` 클래스에서 보고된 오버플로 문제 때문에 많은 튜토리얼에서 `tokenizer.padding_side='right'`를 사용합니다.\n",
    "    - 표준 패딩이 아니라 패킹이나 패킹 역할을 하는 콜레이터(\"패킹된 데이터셋\" 절 참조)를 사용하는 경우에만 괜찮습니다.\n",
    "- 새로운 특수 토큰을 만든다면 (임베딩 층의 빈 슬롯을 사용하기 때문에) 이론적으로 임베딩 층도 미세 튜닝해야 합니다.\n",
    "  - 실제로는 임베딩을 동결하더라도 모델이 잘 동작할 수 있습니다.\n",
    "  - (해당 임베딩을 훈련하지 않았으므로) 새로운 토큰 표현이 랜덤하지만, 모델의 훈련 가능한 다른 부분이 이런 임베딩을 있는 그대로 사용하는 방법을 학습할 수 있습니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29252e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "config_phi = AutoConfig.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4328bccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2803, 29915, 29879, 5993, 675, 445, 10541, 29991], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi(\"Let's tokenize this sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f1150",
   "metadata": {},
   "source": [
    "#### 어휘사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8a0766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32011, 32064)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_phi), config_phi.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e74a2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001),\n",
       " ('<|endoftext|>', 32000),\n",
       " ('给', 31999)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "163e5b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', 32000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.eos_token, tokenizer_phi.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7b370",
   "metadata": {},
   "source": [
    "#### 특수 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a1658f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<|endoftext|>', '<unk>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d361cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95090969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.cls_token, tokenizer_phi.sep_token, tokenizer_phi.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f65debf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.add_special_tokens({'cls_token': '<cls>', 'sep_token': '<sep>', 'mask_token': '<mask>'})\n",
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dc17b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<mask>', 32013),\n",
       " ('<sep>', 32012),\n",
       " ('<cls>', 32011),\n",
       " ('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001),\n",
       " ('<|endoftext|>', 32000),\n",
       " ('给', 31999)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b3026",
   "metadata": {},
   "source": [
    "#### `EOS` 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "add45042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<unk>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token = tokenizer_phi.unk_token\n",
    "tokenizer_phi.pad_token_id = tokenizer_phi.unk_token_id\n",
    "\n",
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cb4f4",
   "metadata": {},
   "source": [
    "```python\n",
    "# Updating model's configuration for the modified PAD token\n",
    "if getattr(model, \"config\", None) is not None:\n",
    "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "if (getattr(model, \"generation_config\", None) s not None):\n",
    "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333bb99",
   "metadata": {},
   "source": [
    "#### `PAD` 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfe01c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 'left')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token, tokenizer_phi.padding_side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d36e92",
   "metadata": {},
   "source": [
    "### 데이터 콜레이터\n",
    "\n",
    "****\n",
    "**\"데이터 콜레이터\" 절의 요약**\n",
    "- `SFTTrainer` 클래스(5장 참조)의 `data_collator` 매개변수를 지정할 수 있습니다.\n",
    "- `DataCollatorForLanguageModeling`가 `SFTTrainer` 클래스의 기본 콜레이터입니다.\n",
    "  - 자동으로 토큰 ID를 레이블로 복제합니다.\n",
    "  - 모델이 자동으로 처리하므로 레이블을 이동시키지 않습니다.\n",
    "  - 전체 텍스트(프롬프트와 완성)를 레이블에 포함시키므로 지시 미세 튜닝에 이상적입니다.\n",
    "- 지시 모델이나 채팅 모델을 추가적으로 미세 튜닝한다면 `DataCollatorForCompletionOnlyLM`을 사용해 모델의 응답(완성)으로만 모델을 훈련할 수 있습니다.\n",
    "  - 이 콜레이터도 토큰 ID를 레이블로 복제하지만 프롬프트 토큰에 해당하는 ID는 -100으로 마스킹합니다.\n",
    "  - 단일 상호작용(한 쌍의 프롬프트와 완성)에서는 응답 템플릿만으로 완성의 위치를 찾을 수 있습니다.\n",
    "  - 다중 상호작용(여러 쌍의 프롬프트와 완성)에서는 프롬프트 토큰을 찾고 마스킹하기 위해 지시 템플릿과 응답 템플릿이 모두 필요합니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eeacb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720,\n",
       " {'prompt': 'The birch canoe slid on the smooth planks.',\n",
       "  'completion': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9841e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "formatting_func = instructions_formatting_function(tokenizer_phi)\n",
    "dataset = dataset.map(lambda row: {'text': formatting_func(row)}, batched=True, batch_size=32)\n",
    "sequences = dataset['text']\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e400ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(lambda row: tokenizer_phi(row['text']))\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a276bd5",
   "metadata": {},
   "source": [
    "#### `DataCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fcce437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_collator = DataCollatorWithPadding(tokenizer_phi)\n",
    "pad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=pad_collator)\n",
    "pad_batch = next(iter(pad_dloader))\n",
    "pad_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a36a82",
   "metadata": {},
   "source": [
    "#### 레이블은 어디 있나요?\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/shift_labels.png?raw=True)\n",
    "\n",
    "<center>그림 4.6 입력과 한 위치 이동한 레이블</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606662ee",
   "metadata": {},
   "source": [
    "#### `DataCollatorForLanguageModeling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b22713c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_collator = DataCollatorForLanguageModeling(tokenizer_phi, mlm=False)\n",
    "lm_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=lm_collator)\n",
    "lm_batch = next(iter(lm_dloader))\n",
    "lm_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ef2f1",
   "metadata": {},
   "source": [
    "#### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9c58443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>' # token id 32001\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(response_template=response_template, \n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ebcd7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels'][0]\n",
    "valid_tokens = (labels >= 0)\n",
    "tokenizer_phi.decode(labels[valid_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b7d1d",
   "metadata": {},
   "source": [
    "##### 다중 상호작용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58067e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2153ac43d24bf185cad336bb9c394b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_chat = \"\"\"<|user|>Hello\n",
    "<|assistant|>How are you?\n",
    "<|user|>I'm fine! You?\n",
    "<|assistant|>I'm fine too!\n",
    "<|endoftext|>\"\"\"\n",
    "\n",
    "dummy_ds = Dataset.from_dict({'text': [dummy_chat]})\n",
    "dummy_ds = dummy_ds.map(lambda row: tokenizer_phi(row['text'])).select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7408873c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18bc8194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c1a80c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  1128,   526,   366, 29973,    13,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_template = '<|user|>'\n",
    "response_template = '<|assistant|>'\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
    "                                                      response_template=response_template, \n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a052edea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How are you?\\n I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203234de",
   "metadata": {},
   "source": [
    "#### 레이블 이동\n",
    "\n",
    "```python\n",
    "if labels is not None:\n",
    "    # 모델 병렬화를 위해 레이블을 올바른 장치로 이동시킵니다.\n",
    "    labels = labels.to(lm_logits.device)\n",
    "    # 다음 토큰 예측을 수행하므로 예측 점수와 입력 아이디를 하나씩 이동시킵니다.\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742073f",
   "metadata": {},
   "source": [
    "### 패킹된 데이터셋\n",
    "\n",
    "****\n",
    "**\"패킹된 데이터셋\" 절의 요약**\n",
    "- 패킹은 시퀀스를 연결하여 동일한 크기의 청크로 나눕니다.\n",
    "  - 패딩 토큰이 사용되지 않습니다.\n",
    "  - 각 청크의 길이는 모델의 최대 시퀀스 길이를 넘어서는 안됩니다.\n",
    "- 패킹은 기본적으로 `SFTTrainer`에서 지원합니다.\n",
    "  - `packing` 매개변수를 `True`로 지정합니다.\n",
    "  - 내부적으로 `ConstantLengthDataset`을 만들어 패킹을 처리합니다.\n",
    "  - 기본적으로 패킹과 콜레이터를 동시에 사용할 수 없습니다.\n",
    "- 일부 콜레이터는 효과적으로 시퀀스를 패킹할 수 있습니다.\n",
    "  - 이 경우 `packing` 매개변수를 `False`로 지정해야 하며 콜레이터가 패킹을 수행합니다.\n",
    "  - `DataCollatorWithFlattening`는 `DataCollatorForLanguageModeling`의 패킹 버전입니다.\n",
    "  - `DataCollatorForCompletionOnlyLM`에는 완성 전용 콜레이터가 패킹 같은 기능을 수행하도록 만드는 새로운 매개변수(`padding_free`)가 있습니다.\n",
    "  - 특정 모델(예를 들면, Llama, Phi, Mistral, Gemma, OLMo 등)은 플래시 어텐션 2로 이런 콜레이터를 지원합니다.\n",
    "    - 이런 모델은 `position_ids`를 사용하여 패킹된 원본 시퀀스 사이의 경계를 표시합니다.\n",
    "****\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packed_seq.png?raw=True)\n",
    "\n",
    "<center>그림 4.7 패킹된 시퀀스</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad7519b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['text']\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc4a53a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 351\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator = ConstantLengthDataset(tokenizer_phi, dataset, \n",
    "                                 dataset_text_field='text', \n",
    "                                 seq_length=64, shuffle=False)\n",
    "\n",
    "def data_generator(iterator):\n",
    "    yield from iterator\n",
    "\n",
    "packed_dataset = Dataset.from_generator(\n",
    "    data_generator, \n",
    "    gen_kwargs={\"iterator\": iterator}\n",
    ")\n",
    "packed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00d4e10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|> The birch canoe slid on the smooth planks.<|end|><|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|><|endoftext|><|user|> Glue the sheet to the dark blue background.<|end|><|assistant|> Glue the sheet to the dark blue background, you must'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = packed_dataset['input_ids']\n",
    "tokenizer_phi.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad3c82",
   "metadata": {},
   "source": [
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packing_flow.png?raw=True)\n",
    "\n",
    "<center>그림 4.8 올바른 데이터 설정 선택하기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5af2d",
   "metadata": {},
   "source": [
    "#### 패킹을 위한 콜레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8df4a",
   "metadata": {},
   "source": [
    "##### `DataCollatorWithFlattening`\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/collator_flat.png?raw=True)\n",
    "\n",
    "<center>그림 4.9 패킹 유사 콜레이터</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bfc8432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Using `DataCollatorWithFlattening` will flatten the entire mini batch into single long sequence.Make sure your attention computation is able to handle it!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'labels': tensor([[ -100,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "          36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_collator = DataCollatorWithFlattening()\n",
    "flat_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=flat_collator)\n",
    "flat_batch = next(iter(flat_dloader))\n",
    "flat_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ecb7453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 66]), tensor(38))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_batch['input_ids'].shape, flat_batch['position_ids'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81fdd2",
   "metadata": {},
   "source": [
    "##### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f900880b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "           434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "         32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>'\n",
    "completion_nopad_collator = DataCollatorForCompletionOnlyLM(response_template=response_template, \n",
    "                                                            tokenizer=tokenizer_phi,\n",
    "                                                            padding_free=True)\n",
    "completion_nopad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_nopad_collator)\n",
    "completion_nopad_batch = next(iter(completion_nopad_dloader))\n",
    "completion_nopad_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e818",
   "metadata": {},
   "source": [
    "### 고급 방법: BYOT (Bring Your Own Template)\n",
    "\n",
    "****\n",
    "**\"고급 방법: BYOT\" 절의 요약**\n",
    "\n",
    "- 모든 템플릿은 응답 템플릿을 정의해야 하며, 이상적으로는 EOS 토큰으로 끝나야 합니다.\n",
    "- 토크나이저의 `EOS`, `PAD`, `UNK` 토큰을 다시 확인하세요.\n",
    "  - `EOS` 토큰은 `PAD` 및 `UNK` 토큰과 달라야 합니다.\n",
    "  - `PAD` 및 `UNK` 토큰은 같을 수 있습니다.\n",
    "- 꼭 필요할 때 임베딩 층의 크기를 바꿉니다(\"빈 슬롯\"이 모두 사용된 경우).\n",
    "  - 모델의 `resize_token_embeddings()`를 호출할 때, `pad_to_multiple_of` 매개변수를 사용해 크기가 2의 거듭제곱의 배수로 유지되도록 합니다.\n",
    "- 진자 템플릿을 직접 만들고 싶지 않다면 ChatML과 같은 기본 템플릿을 사용할 수 있습니다. `trl` 패키지의 `setup_chat_format()` 함수를 사용할 수 있지만 몇 가지 단점이 있습니다.\n",
    "  - `EOS` 토큰을 `PAD` 토큰에 할당합니다(나중에 수동으로 수정해야 합니다).\n",
    "  - 더 짧게 만들기 위해서라도 모델의 임베딩 층 크기를 기본적으로 조정합니다(적절한 `resize_to_multiple_of`를 선택하면 크기 변경을 피할 수 있습니다).\n",
    "- 토크나이저를 위해 진자 템플릿을 만드는 대신 포맷팅 함수를 사용하여 사용자 정의 템플릿을 정의하고 적용할 수 있습니다.\n",
    "  - `SFTTrainer` 클래스(5장 참조)에서 `formatting_func`을 지정하면 토크나이저가 채팅 템플릿을 가지고 있을 필요가 없습니다.\n",
    "- 응답 템플릿을 신중하게 선택하세요.\n",
    "  - 일반 단어(예: \"## Answer:\")를 사용하면 문제가 발생할 수 있습니다. 일부 토크나이저는 문맥에 의존적이어서 응답 템플릿을 여러 토큰으로 분할할 수 있기 때문입니다.\n",
    "  - 응답 템플릿을 위해 추가적인 특수 토큰을 만드는 것이 더 안전합니다. 이렇게 하면 응답 템플릿이 단일 토큰으로 인코딩되기 때문입니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23603681",
   "metadata": {},
   "source": [
    "#### 채팅 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "30ccd450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "print(tokenizer_opt.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "55c44376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '</s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '</s>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a1505",
   "metadata": {},
   "source": [
    "**ChatML**\n",
    "****\n",
    "\n",
    "[ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)은 채팅 마크업 언어(Chat Markup Language)의 약어로 오픈AI에서 개발했습니다: \n",
    "\n",
    "_____\n",
    "\"_전통적으로 GPT 모델은 구조화되지 않은 텍스트를 사용했습니다. ChatGPT 모델은 대신에 채팅 마크업 언어(줄여서 ChatML)이라는 구조적인 포맷을 기대합니다. ChatML 문서는 일련의 메시지로 구성됩니다._\"\n",
    "_____\n",
    "\n",
    "앞서 소개한 대화 포맷과 비슷하게 각 메시지는 참여자의 역할과 그에 해당하는 콘텐츠로 구성됩니다. ChatML의 진자 템플릿은 다음과 같습니다:\n",
    "\n",
    "```\n",
    "{% for message in messages %}\n",
    "  {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n",
    "{% endfor %}\n",
    "```\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e439158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d70086b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50272"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b91f663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "pad_to_multiple_of = get_multiple_of(model_opt.config.vocab_size)\n",
    "pad_to_multiple_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4aff379c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.resize_token_embeddings(len(tokenizer_opt), \n",
    "                                  pad_to_multiple_of=pad_to_multiple_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c9973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_tokenizer(tokenizer, \n",
    "                     alternative_bos_token='<|im_start|>', \n",
    "                     alternative_unk_token='<unk>', \n",
    "                     special_tokens=None, \n",
    "                     tokens=None):\n",
    "    eos_token, bos_token = tokenizer.eos_token, tokenizer.bos_token\n",
    "    pad_token, unk_token = tokenizer.pad_token, tokenizer.unk_token\n",
    "\n",
    "    # BOS 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    if bos_token == eos_token:\n",
    "        bos_token = alternative_bos_token\n",
    "\n",
    "    # UNK 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    if unk_token == eos_token:\n",
    "        unk_token = alternative_unk_token\n",
    "\n",
    "    # PAD 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    # 하지만 UNK 토큰과는 같을 수 있습니다.\n",
    "    if pad_token == eos_token:\n",
    "        pad_token = unk_token\n",
    "        \n",
    "    assert bos_token != eos_token, \"다른 BOS 토큰을 선택하세요.\"\n",
    "    assert unk_token != eos_token, \"다른 UNK 토큰을 선택하세요.\"\n",
    "\n",
    "    # BOS, PAD, UNK 토큰을 위한 딕셔너리를 만듭니다.\n",
    "    # EOS 토큰은 원래 정의된 대로 유지합니다.\n",
    "    special_tokens_dict = {'bos_token': bos_token, \n",
    "                           'pad_token': pad_token, \n",
    "                           'unk_token': unk_token}\n",
    "    \n",
    "    # 새로운 특수 토큰을 추가합니다.\n",
    "    if special_tokens is not None:\n",
    "        if isinstance(special_tokens, list):\n",
    "            special_tokens_dict.update({'additional_special_tokens': special_tokens})\n",
    "        \n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    # 새로운 일반 토큰을 추가합니다.\n",
    "    if tokens is not None:\n",
    "        if isinstance(tokens, list):\n",
    "            tokenizer.add_tokens(tokens)\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9377ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jinja_template(tokenizer):\n",
    "    return (\"{% for message in messages %}\"\n",
    "            f\"{{{{'{tokenizer.bos_token}' + message['role'] + '\\n' + message['content'] + '{tokenizer.eos_token}' + '\\n'}}}}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            f\"{{{{ '{tokenizer.bos_token}assistant\\n' }}}}\"\n",
    "            \"{% endif %}\")\n",
    "\n",
    "def add_template(tokenizer, chat_template=None):\n",
    "    # 채팅 템플릿이 주어지지 않으면 BOS와 EOS 토큰을 사용해 \n",
    "    # 채팅 템플릿을 만듭니다.\n",
    "    if chat_template is None:\n",
    "        chat_template = jinja_template(tokenizer)\n",
    "        \n",
    "    # 채팅 템플릿을 토크나이저에 할당합니다.\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6869f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "def modify_model(model, tokenizer):    \n",
    "    # 새로운 토크나이저의 크기가 어휘사전 크기를 초과한다면\n",
    "    # 같은 배수가 되도록 유지하면서 크기를 바꿉니다.\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        pad_to_multiple_of = get_multiple_of(model.vocab_size)\n",
    "        model.resize_token_embeddings(len(tokenizer), \n",
    "                                      pad_to_multiple_of=pad_to_multiple_of)    \n",
    "\n",
    "    # 모델 설정의 토큰 ID를 업데이트합니다.\n",
    "    if getattr(model, \"config\", None) is not None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1b3459c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_opt = modify_tokenizer(tokenizer_opt)\n",
    "tokenizer_opt = add_template(tokenizer_opt)\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4b7bf5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|im_start|>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d96ae3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50266"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "04a2e433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ec72d660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f9e77014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '</s>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_opt.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "197e5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of Argentina?</s>\n",
      "<|im_start|>assistant\n",
      "Buenos Aires.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = ds_msg['messages'][0]\n",
    "print(tokenizer_opt.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea9914",
   "metadata": {},
   "source": [
    "#### 사용자 정의 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "41741481",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "response_template = '##[YODA]##>'\n",
    "tokenizer_opt = modify_tokenizer(tokenizer_opt, special_tokens=[response_template])\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2ab49d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.gen_formatting_func.<locals>.formatting_func(examples, add_generation_prompt=False)>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func_builder(response_template):\n",
    "    def formatting_func(examples, add_generation_prompt=False):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples['prompt'])):\n",
    "            text = f\"{examples['prompt'][i]}\"\n",
    "            try:\n",
    "                text += f\" {response_template} {examples['completion'][i]}{tokenizer_opt.eos_token}\"\n",
    "            except KeyError:\n",
    "                if add_generation_prompt:\n",
    "                    text += f\" {response_template} \"\n",
    "            output_texts.append(text)\n",
    "        return output_texts\n",
    "    return formatting_func\n",
    "\n",
    "yoda_formatting_func = formatting_func_builder(response_template)\n",
    "yoda_formatting_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "46d5d7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The birch canoe slid on the smooth planks. ##[YODA]##> On the smooth planks, the birch canoe slid. Yes, hrrrm.</s>'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_seqs = yoda_formatting_func(dataset)\n",
    "formatted_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b45c968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 133, 23629, 611, 31728, 13763, 15, 5, 6921, 563, 2258, 4, 1437, 50266, 374, 5, 6921, 563, 2258, 6, 5, 23629, 611, 31728, 13763, 4, 3216, 6, 1368, 28015, 22900, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt(formatted_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4fba69d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##[YODA]##>'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "482f5c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Force is strong in you. ##[YODA]##> ', 'I am your father! ##[YODA]##> ']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_formatting_func({'prompt': ['The Force is strong in you.', \n",
    "                                 'I am your father!']}, \n",
    "                     add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934abad6",
   "metadata": {},
   "source": [
    "#### 특수 토큰 만세!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1db3646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer_llama.pad_token = tokenizer_llama.unk_token\n",
    "tokenizer_llama.pad_token_id = tokenizer_llama.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cabcec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User: Hello\n",
      "\n",
      "### Assistant: Hi, how can I help you?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b327b52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer_llama.tokenize(prompt, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(prompt, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))[6:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "56c09206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e2683ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddddfc8b0e1c40019a74c438910a2cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13,  2277, 29937,  4007,\n",
       "         22137, 29901,  6324, 29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_ds = Dataset.from_dict({'text': [prompt]})\n",
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])\n",
    "\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "bad_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "bad_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=bad_collator)\n",
    "bad_batch = next(iter(bad_dloader))\n",
    "bad_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "817f5914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁', 29871),\n",
       " ('<0x0A>', 13),\n",
       " ('##', 2277),\n",
       " ('#', 29937),\n",
       " ('▁Ass', 4007),\n",
       " ('istant', 22137),\n",
       " (':', 29901)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_response_template = \"\\n### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(modified_response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(modified_response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "402905c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13,  2277, 29937,  4007,\n",
       "         22137, 29901,  6324, 29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  6324, 29892,   920,   508,   306,  1371,   366, 29973]])}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_token_ids = token_ids[2:]\n",
    "fixed_collator = DataCollatorForCompletionOnlyLM(fixed_token_ids, tokenizer=tokenizer_llama)\n",
    "fixed_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=fixed_collator)\n",
    "fixed_batch = next(iter(fixed_dloader))\n",
    "fixed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "86d77bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokenizer_llama.add_special_tokens({'additional_special_tokens': [response_template]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cfb6c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4664c57b3b9841aa83cb6951ec4e3456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f07db988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13, 32000, 29871,  6324,\n",
       "         29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 29871,  6324,\n",
       "         29892,   920,   508,   306,  1371,   366, 29973]])}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "special_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=special_collator)\n",
    "special_batch = next(iter(special_dloader))\n",
    "special_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c75e1",
   "metadata": {},
   "source": [
    "### 다음 장에서는\n",
    "\n",
    "채팅 템플릿은 야생의 LLM을 길들이고 인간과 적절한 대화를 할 수 있는 방법을 가르치기 위한 핵심입니다. 대화 사이에 큐 사인(또는 특수 토큰)을 적절하게 배치하면 명령 키워드에 따라 어떻게 응답해야 하는지 학습할 수 있습니다. 하지만 훈련 절차에 위험이 없는 것은 아닙니다. 활성화, 그레이디언트, 옵티마이저 모두 자신의 작업을 수행하기 위해 상당한 양의 RAM을 필요로 합니다. 메모리를 많이 소비하는 이런 구성 요소들을 만족시키려면 기술과 노력이 모두 필요합니다. 훈련 루프를 구성하려면 두려움을 떨쳐내야 합니다. 도전이 가득한 다음 장을 놓치지 마세요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
