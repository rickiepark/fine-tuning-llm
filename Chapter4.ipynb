{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e1044d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06813f-79d8-4c0a-a823-c3dc2da65292",
   "metadata": {
    "id": "2e06813f-79d8-4c0a-a823-c3dc2da65292"
   },
   "source": [
    "## 4장 데이터셋 포맷팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Io3oVGeZLq7-",
   "metadata": {
    "id": "Io3oVGeZLq7-"
   },
   "source": [
    "사용된 패키지\n",
    "\n",
    "* torch 2.9.0\n",
    "* transformers 5.2.0\n",
    "* datasets 4.0.0\n",
    "* bitsandbytes 0.49.0\n",
    "* trl 0.26.2\n",
    "* peft 1.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf67d8",
   "metadata": {
    "id": "2faf67d8"
   },
   "source": [
    "### 스포일러\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- 적절한 채팅 템플릿의 중요성을 이해합니다.\n",
    "- 사용자 정의 포맷팅 함수와 템플릿을 포함해 몇 가지 포맷팅 옵션에 대해 논의합니다.\n",
    "- 토크나이저와 모델의 임베딩 층을 설정합니다.\n",
    "- 패킹(packing)된 데이터셋과 데이터 로딩을 위한 다양한 데이터 콜레이터(data collator)를 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628f687",
   "metadata": {
    "id": "9628f687"
   },
   "source": [
    "### 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb20be1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bb20be1",
    "outputId": "54b45994-5078-409a-d1fd-079a2316d295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, trl\n",
      "Successfully installed bitsandbytes-0.49.1 trl-0.26.2\n"
     ]
    }
   ],
   "source": [
    "# 코랩에서 실행하는 경우\n",
    "!pip install bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144dfaf",
   "metadata": {
    "id": "2144dfaf"
   },
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "xsccv4IsW0fH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsccv4IsW0fH",
    "outputId": "f5442c9c-a604-45d8-d442-6b5b857f1213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-10 04:58:47--  https://raw.githubusercontent.com/rickiepark/fine-tuning-llm/refs/heads/main/compatibility_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15950 (16K) [text/plain]\n",
      "Saving to: ‘compatibility_functions.py’\n",
      "\n",
      "compatibility_funct 100%[===================>]  15.58K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-10 04:58:48 (129 MB/s) - ‘compatibility_functions.py’ saved [15950/15950]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 깃허브에서 compatibility_functions.py 파일을 다운로드합니다.\n",
    "!wget https://raw.githubusercontent.com/rickiepark/fine-tuning-llm/refs/heads/main/compatibility_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a9ba58",
   "metadata": {
    "id": "22a9ba58",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, \\\n",
    "    DataCollatorForLanguageModeling, DataCollatorWithPadding, \\\n",
    "    DataCollatorWithFlattening, BitsAndBytesConfig\n",
    "from trl.data_utils import pack_dataset\n",
    "from trl.extras.dataset_formatting import FORMAT_MAPPING, \\\n",
    "    conversations_formatting_function\n",
    "from compatibility_functions import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd6389",
   "metadata": {
    "id": "80bd6389"
   },
   "source": [
    "### 목표\n",
    "\n",
    "LLM에게 데이터셋의 구조와 단서를 제공하기 위해 포맷팅을 합니다. 적절한 태그와 특수 토큰으로 각 구성 요소(사용자 프롬프트와 모델이 완성할 텍스트)를 감싸서 모델의 동작을 쉽게 조정(예를 들면 지시 미세 튜닝)할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bf61f",
   "metadata": {
    "id": "380bf61f"
   },
   "source": [
    "### 포맷팅의 핵심\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/base_prompt.png?raw=True)\n",
    "<center>그림 4.1 베이스 모델의 다음 토큰 예측</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/fine_tuned_prompt.png?raw=True)\n",
    "<center>그림 4.2 응답 템플릿을 사용하는 미세 튜닝된 모델</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_prompt_new.png?raw=True)\n",
    "<center>그림 4.3 채팅 템플릿을 사용하는 채팅 모델</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_example_new.png?raw=True)\n",
    "<center>그림 4.4 채팅 템플릿의 일반적인 구조</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a379eb0",
   "metadata": {
    "id": "0a379eb0"
   },
   "source": [
    "### 이전 장에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3ec5f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "b10330b323f3438599adde89daf6cf5d",
      "7fa902fba87444e8b1199ce1751f0298",
      "952656acae654ae48302a001e70a5d62",
      "66d3c48279dd412ead0aeccf2a98d20f",
      "1f6736cb54b8415584ab309f910ea0cd",
      "ec125d9860284aa8b79557a81f222a00",
      "278187706470404a847dca82544bff73",
      "495a6b42379e450b8655f1e5ead6dcf8",
      "fde1fb3f85f94864aa0be8b042532b56",
      "962edee8911e4fbeab88cf4b9a347a52",
      "dc88eb5b90c14063843f5969a724cae0",
      "43284bddf42d41b99d6e07de1b8b2229",
      "cefe734f7d2f491e83cca6cc2fda3177",
      "e4e10264c4b44235b52ff4e90af80cb3",
      "b9b43500ed6440ea835f6a095baa5620",
      "e514d05486bf40449f4bdf62843dc7c5",
      "692b5eb8ace44ae4832096e410e659d7",
      "68b158e4413443bd90b430f305a7d816",
      "4a8ada99e518441ab6e5009dbd9e61b9",
      "ce22abf279ff4d9985a303bd54bad147",
      "2e86e03112fe4f4a9a9d4bd48ad0b1ad",
      "0999fa80de14478a852d2ea96a4e0384",
      "112d376da17c453bad9cf0d8e854827d",
      "f550a7a6c3cc42bd98cfcacf71f5884a",
      "35e8addb55254feea4f3c91d15b6c5e6",
      "f4a497a7b0214a9ab2bbf2e5bedd58ef",
      "d36499110075411ebfe22e2b8a0e3c49",
      "fd6f912123234b369d906cad5085d693",
      "44689b417dd54c6d8b52ba1d58e89209",
      "97b6f90a3a3d470dac143006e70ae876",
      "4923976659654a54a1cc1e440b6fb77c",
      "2026d1a2f202469e8a4a4823086710f8",
      "5a06e68190e846c59ce2f6846ad225c4",
      "7a2e1004a2b24d9cb79fb5adf56793f1",
      "1b3603fe8a1b4cc7b408c4ea555c2fa7",
      "8fcb26cb63a04dea87e444fb25e13059",
      "f2a5907144694fdfa828d066ea967582",
      "8bf45331118d4724acbc09a915d4cb8a",
      "0c99ba41de884378bbc0bc93949dae65",
      "38a66d0cddc54fc8ac1e4567eddac75a",
      "5d3e92ee64fe40f1afc1af66952a742a",
      "8027ef9996274751839bbaa5dec931e5",
      "9b6a763588a24de89dfa4684e9eb0807",
      "666ee64b0d96479c9995c3cb22b60e5f"
     ]
    },
    "id": "9d3ec5f2",
    "outputId": "579cde41-3f80-44bb-bf1a-05915cf5c543"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10330b323f3438599adde89daf6cf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43284bddf42d41b99d6e07de1b8b2229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d376da17c453bad9cf0d8e854827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2e1004a2b24d9cb79fb5adf56793f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='cuda:0',\n",
    "                                                dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)\n",
    "\n",
    "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model_q4, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e76cbf",
   "metadata": {
    "id": "c3e76cbf"
   },
   "source": [
    "### 템플릿 적용하기\n",
    "\n",
    "****\n",
    "**\"템플릿 적용하기\" 절의 요약**\n",
    "\n",
    "데이터셋 포맷팅에 세 가지 옵션이 있습니다:\n",
    "1. 데이터셋의 포맷이 `STTrainer` 클래스가 지원하는 두 개의 포맷 중 하나인 경우\n",
    "   - 토크나이저가 채팅 템플릿을 가지고 있어야 합니다.\n",
    "   - 포맷팅 함수를 정의하거나 훈련 전에 데이터셋을 포맷팅할 필요가 없습니다.\n",
    "2. 사용자 정의 포맷팅 함수를 사용하고 싶은 경우(BYOFF 절 참고)\n",
    "   - `SFTTrainer` 클래스(5장 참조)의 `formatting_func` 매개변수로 사용자 정의 함수를 전달해야 합니다.\n",
    "   - 사용자 정의 포맷팅 함수가 배치 데이터를 다룰 수 있어야 합니다.\n",
    "     - `batched=True`로 데이터셋의 `map()` 메서드를 호출하여 테스트하세요.\n",
    "    - 훈련 전에 데이터셋에 이 함수를 적용할 필요는 없습니다.\n",
    "    - 토크나이저가 이미 채팅 템플릿을 가지고 있는 경우\n",
    "      - 사용자 정의 함수에서 `apply_chat_template()` 메서드를 호출할 수 있습니다.\n",
    "      - 템플릿의 일반적인 포맷(지시 템플릿과 응답 템플릿)을 고수하세요.\n",
    "      - 템플릿에 `EOS` 토큰이 포함되어 있지 않다면 포맷팅된 출력 끝에 `EOS` 토큰을 추가할 수 있습니다.\n",
    "   - 토크나이저가 채팅 템플릿을 가지고 있지 않은 경우\n",
    "     - 지시 템플릿과 응답 템플릿을 포함하여 일반적인 포맷을 자유롭게 정의할 수 있습니다('고급 방법 - BYOT' 절 참고).\n",
    "3. 데이터셋이 이미 포맷팅된 경우\n",
    "   - `SFTTrainer` 클래스(5장 참조)의 `dataset_text_field` 매개변수에 포맷팅된 데이터를 담고 있는 열을 전달해야 합니다.\n",
    "   - 사용자 정의 포맷팅 함수를 사용하여 데이터셋을 전처리 하더라도 훈련 클래스는 이를 사용하지 않습니다.\n",
    "   - 데이터가 토크나이저의 템플릿과 호환되는지 확인하세요.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f92fae6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323,
     "referenced_widgets": [
      "43419d5e31174f91b8bd62f26c2d1d68",
      "8e3a3b252dc24e7892752bd326dc5204",
      "944be2d493c2494c85202e9d3202d45b",
      "193ba1bd162d4c16a2d55254544c4a3b",
      "be281fc7a2a34c1291234f60bf7c2c8d",
      "579eaa1e1058410bb519cd2c8e2d22f5",
      "4f5c1053790d498d94d378351f36e874",
      "225d1eb10c804429932f9a084ed4b3fc",
      "901a1cb3fbff4c33a25b869ffd251d02",
      "fcda5362aefb4454857aa12668e1b88a",
      "09212c0c69f54da99699fd2ebd690770",
      "20c39e980ed7439992b19b25294881f5",
      "3993cf4681204a5db20350a74ecc323b",
      "9567bdfe5c6941b9b6039ea07f2f334b",
      "2d27c91125594e5f80b2b90cc72dc4d6",
      "724665b674274844ad6706b03aca9822",
      "8415c0b847664f3ab67d90a45eb468ca",
      "e986357811a143f9a086fb86f81b8b5d",
      "0f5c24ff4a234a548924d8436461fc39",
      "9e1532fe65df4539a215ba4aaba92046",
      "5e73207479fc48139c5d7dc19b6d85ff",
      "cea7c11c072a4870a35daa252eeb0b6c",
      "ad8283d7baa54dcebcf3ad54461104c9",
      "a1173e5f08a9404c93df1388fb6ec6fd",
      "b2388daebd62433f8d995e599d97cc5a",
      "4ba9c9eea3b74531a38471d572530f1a",
      "918c207c5cbc4f359328cb7d3c912d2a",
      "ff44d63c86524eb8b8f1c7fc1542e4ff",
      "9c9d87a696f24deca173d483d79c166f",
      "66e2171c9fba4554bdf2a8a60f4ee9f7",
      "01d070502171406b9c7b1378724f1a39",
      "8cf8956c0a2d41e29f5ce0eead76bc70",
      "be697cddad304b748d65f81a41e2b8ad",
      "829d4be8cbfd4b638de422ab5140ecb6",
      "a2ad733a72ce436ca2fdbe834269b8fe",
      "635f6f8d8843492cac470d6f2f4a7f7e",
      "2074bfc8a2df47be91d4fedaac717b84",
      "956adae0d73e4691877fcfdbb9fb7299",
      "2e6dc2e1eb7c403aab5b59bac035c747",
      "a33b2dad7cfd4762948f061dfaa931dc",
      "53437ace361d42aba13dac6c98a0201a",
      "1405895fe9584a71a8bfbcb6fbe84764",
      "b1b2e784ca784a359ac28744461c2527",
      "fc66af5498c543179b1d70bf0742493b",
      "8b84f828f16541c99c00e3e8c5b98f4f",
      "a41813d371b74ded84dd0613f1514b3b",
      "2c06ab1205924cdb976b1d0d2962bc3e",
      "16f6d46d35b94ee2a748e3af4967939b",
      "2ed4c083462449db877d89f56183569a",
      "a27af1fe202f4e2da67c76f80c74989d",
      "c72f94e29d394cca8b585a31ae9b4bb4",
      "c70c8a10de4d43fd837c1984382ce0eb",
      "2509c8bf16a846539be80dac4020d8d5",
      "d19d2939f3ec4466ac3d1a4c7ce84412",
      "7d84c84d82ec47318b925b1ba7d6e231"
     ]
    },
    "id": "8f92fae6",
    "outputId": "d889770c-a3f0-4d5c-8375-d776ae119c31",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43419d5e31174f91b8bd62f26c2d1d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c39e980ed7439992b19b25294881f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8283d7baa54dcebcf3ad54461104c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829d4be8cbfd4b638de422ab5140ecb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b84f828f16541c99c00e3e8c5b98f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "print(tokenizer_phi.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e134f99c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e134f99c",
    "outputId": "103ab2a5-ab26-4050-c369-4b5a1b306531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
    "    {'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
    "    {'role': 'assistant', 'content': 'Buenos Aires.'}\n",
    "]\n",
    "\n",
    "formatted = tokenizer_phi.apply_chat_template(conversation=messages,\n",
    "                                              tokenize=False,\n",
    "                                              add_generation_prompt=False)\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5014cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a5014cf",
    "outputId": "c640a15a-97b2-4ae6-c720-a809742b4483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_input = tokenizer_phi.apply_chat_template(conversation=messages[:-1],\n",
    "                                                    tokenize=False,\n",
    "                                                    add_generation_prompt=True)\n",
    "print(inference_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3a356",
   "metadata": {
    "id": "a7c3a356"
   },
   "source": [
    "#### 지원 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a40284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60a40284",
    "outputId": "6912402c-fe14-4450-ef71-1e6870651b7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': List({'content': Value('string'), 'role': Value('string')})}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_ds = Dataset.from_list([{'messages': messages}])\n",
    "conversation_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48fd0626",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48fd0626",
    "outputId": "3af9ad8f-5ce1-45e2-8c98-1c991b8d3ef4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORMAT_MAPPING['chatml'] == conversation_ds.features['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cRF_rzf_sUF0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRF_rzf_sUF0",
    "outputId": "3bdca4df-093b-4ba1-eee5-b53825d417ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chatml': List({'content': Value('string'), 'role': Value('string')}),\n",
       " 'instruction': {'completion': Value('string'), 'prompt': Value('string')}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORMAT_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0129a7b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0129a7b9",
    "outputId": "a83c0a06-ddd7-4a8b-f80b-f4d21e2f44a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-361160568.py:3: FutureWarning: `conversations_formatting_function` is deprecated and will be removed in TRL 0.27. Please use `tokenizer.apply_chat_template()` directly instead.\n",
      "  formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n"
     ]
    }
   ],
   "source": [
    "# conversations_formatting_function() 함수는 trl 0.27 버전에서 삭제될 예정입니다.\n",
    "# 대신 다음 셀에서처럼 apply_chat_template() 메서드를 직접 호출하세요.\n",
    "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
    "\n",
    "print(formatting_func(conversation_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71tUn-EDPzvK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71tUn-EDPzvK",
    "outputId": "eb35806d-6cad-4075-cf48-71110b8a1c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# trl 0.27 버전부터\n",
    "formatted = tokenizer_phi.apply_chat_template(conversation=conversation_ds[0]['messages'],\n",
    "                                              tokenize=False,\n",
    "                                              add_generation_prompt=False)\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525cc9c",
   "metadata": {
    "id": "c525cc9c"
   },
   "source": [
    "```python\n",
    "# 대화 포맷을 위한 포맷팅 함수\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[messages_field][0], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[messages_field])):\n",
    "            output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n",
    "        return output_texts\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371de955",
   "metadata": {
    "id": "371de955"
   },
   "source": [
    "**중요 업데이트**: 안타깝게도 최근 버전의 `trl` 라이브러리는 (`prompt`와 `completion` 열을 필요로 하는) 지시 포맷을 더이상 지원하지 않습니다. 이로 인해 채팅 템플릿이 올바르게 적용되지 않습니다. 이 문제를 피하기 위해 대화 포맷을 사용하는 것이 좋습니다.\n",
    "\n",
    "만약 데이터셋이 지시 포맷으로 구성되어 있다면 다음에 나오는 `format_dataset()` 함수를 사용해 손쉽게 대화 포맷으로 바꿀 수 있습니다. 이 함수는 이전 버전의 `trl` 패키지를 참고하여 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "By3fhvwiRJNe",
   "metadata": {
    "id": "By3fhvwiRJNe"
   },
   "outputs": [],
   "source": [
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8c7c80",
   "metadata": {
    "id": "fb8c7c80"
   },
   "outputs": [],
   "source": [
    "batch_prompts_completions = {\n",
    "    'prompt': ['What is the capital of Argentina?',\n",
    "               'What is the capital of the United States?'],\n",
    "    'completion': ['Buenos Aires.',\n",
    "                    'Washington D.C.']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "298fa759",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "298fa759",
    "outputId": "a65406c0-26ec-40ad-ab42-692fd1e46c82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
       "  {'role': 'assistant', 'content': 'Buenos Aires.'}],\n",
       " [{'role': 'user', 'content': 'What is the capital of the United States?'},\n",
       "  {'role': 'assistant', 'content': 'Washington D.C.'}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = format_dataset(batch_prompts_completions)['messages']\n",
    "batch_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50481b56",
   "metadata": {
    "id": "50481b56"
   },
   "source": [
    "#### BYOFF (Bring Your Own Formatting Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c55159",
   "metadata": {
    "id": "b7c55159"
   },
   "outputs": [],
   "source": [
    "def byo_formatting_func1(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efbffc17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "92905998a8bd4eb2a8168a353b7655c2",
      "9f8662eb13464a0e9b81ea8c8ae34680",
      "f57eccff73fc4513ba9be31a692ec0e3",
      "ffd0301e72b14369a02603e3cb3e9581",
      "e94cffeadbb44b249030e9fe1a9cfc34",
      "123d9f72c5ea4a70a4387def4432aaa2",
      "ad0a42d6617f4d169b739a4a90c048d2",
      "62aa0d4279fa4c9699f7e1b4c1f5a6ab",
      "09c0093e309144058bffe3a2fb9942f8",
      "1badadd379f546fea8cffc2072a3bab8",
      "0625fc0b5bb84ca5a8405b8016499594"
     ]
    },
    "id": "efbffc17",
    "outputId": "a720e02b-efd4-4697-dc87-79e29c2f9f6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92905998a8bd4eb2a8168a353b7655c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_msg = Dataset.from_dict({'messages': batch_messages})\n",
    "ds_msg.map(lambda v: tokenizer_phi(byo_formatting_func1(v)), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6488d9c0",
   "metadata": {
    "id": "6488d9c0"
   },
   "outputs": [],
   "source": [
    "def byo_formatting_func2(examples):\n",
    "    response_template = '### Answer:'\n",
    "    text = f\"### Question: {examples['prompt']}\\n{response_template} {examples['completion']}\"\n",
    "    text += tokenizer_phi.eos_token\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1f7c32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a1f7c32",
    "outputId": "bb3e797d-6fca-44a8-b0e8-a88606a6910a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the capital of Argentina?\n",
      "### Answer: Buenos Aires.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "ds_prompt = Dataset.from_dict(batch_prompts_completions)\n",
    "print(byo_formatting_func2(ds_prompt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bf14c94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "6dece67ebfc747038cec9bac563ef5e8",
      "2b4b91f1de8e422098451cd34679d69d",
      "b94719a4a1494f18bd079b3a768a14a7",
      "53863967327d4a88ad5362072f83be5e",
      "da2803e6c6ad4077a1da7d12179d95c8",
      "439536468bed431493effbbc4a69a74a",
      "7d56b7f7eb1e40dc9a069dfa749f8d6a",
      "8c9cc2efaebb4dfa8ae6e33ba7649cef",
      "2da0ad36f7bb445da94a59dab64e89d4",
      "82bd7c68f1044f6fae1499d051c842e0",
      "76a5799e0cfb472c88ef16804306f67c"
     ]
    },
    "id": "5bf14c94",
    "outputId": "6eb5d8ec-03e1-488f-e6b8-fdf3f18f8bc0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dece67ebfc747038cec9bac563ef5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named input_ids expected length 2 but got length 44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1676773618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 이 코드는 예외를 일으킵니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyo_formatting_func2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3316\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0munprocessed_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0munprocessed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m                             \u001b[0mcheck_if_shard_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3687\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_original_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_original_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named input_ids expected length 2 but got length 44"
     ]
    }
   ],
   "source": [
    "# 이 코드는 예외를 일으킵니다.\n",
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func2(v)), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9591c014",
   "metadata": {
    "id": "9591c014"
   },
   "outputs": [],
   "source": [
    "def byo_formatting_func3(examples):\n",
    "    output_texts = []\n",
    "    response_template = '### Answer:'\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"### Question: {examples['prompt'][i]}\\n {response_template} {examples['completion'][i]}\"\n",
    "        text += tokenizer_phi.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0aeda6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "45d2cd5a099c47eca07b44ffe52d429b",
      "8f3051ea7b5a42afa26603af0fd0f5cd",
      "390114f2ad5241c1bd37e309f88c25cd",
      "f163698ec7c44d8981308d90bb13d5aa",
      "2ba5c40fa53c4f23924ab0c3ab84bc51",
      "fa8ffd6cf35d4fd5becbe608df3d4b9e",
      "6c0afdb6c24c48048ffa6ce834c18a79",
      "15ec92ee26f045ccbac3326a3f9c5158",
      "cf57e876a4cf485485e90c01f9ddceb6",
      "94c591a7064c43fd9687827664a09e18",
      "6e0d9342e6be4e41969bbbb8f3b6116e"
     ]
    },
    "id": "d0aeda6d",
    "outputId": "332ea4c9-b5df-4df7-eed2-9ad611707516"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d2cd5a099c47eca07b44ffe52d429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func3(v)), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b04e6a",
   "metadata": {
    "id": "c0b04e6a"
   },
   "source": [
    "#### BYOFD (Bring Your Own Formatted Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6a04d4",
   "metadata": {
    "id": "1a6a04d4"
   },
   "outputs": [],
   "source": [
    "def byofd_formatting_func(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {'text': output_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5772c71a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "6ecf9d23018b4348b890bbee5f0c10c4",
      "37749e98f7554d8fb1375c63e1dfb87d",
      "717780195da447d19411310842d9e5cc",
      "c87aede25d4747f5836b08338cbc3a9a",
      "faa8e4d542264a398d0aa4ff8fa541d0",
      "2d29e0ac96d24568bd203d8d5b17b69a",
      "5f8b52861f134c42933682f46d87c02d",
      "f94b7d5a9ddf480781ef9468c92ead99",
      "5a200432984a4c44aaf5bb13bd7e9a11",
      "6190555e79fc4bcb9547b581b80e18b1",
      "7e131995ebb442f59c1bcc2ed42dc733"
     ]
    },
    "id": "5772c71a",
    "outputId": "f26f48c4-342c-4e2e-f974-3cf1976be351"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecf9d23018b4348b890bbee5f0c10c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Column(['<|user|>\\nWhat is the capital of Argentina?<|end|>\\n<|assistant|>\\nBuenos Aires.<|end|>\\n<|endoftext|>', '<|user|>\\nWhat is the capital of the United States?<|end|>\\n<|assistant|>\\nWashington D.C.<|end|>\\n<|endoftext|>'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ds = ds_msg.map(byofd_formatting_func, batched=True)\n",
    "formatted_ds['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78305ca1",
   "metadata": {
    "id": "78305ca1"
   },
   "source": [
    "#### 결론\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/formatting_flow.png?raw=True)\n",
    "\n",
    "<center>그림 4.5 - 적절한 포맷팅 방법 선택하기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753ab0a",
   "metadata": {
    "id": "4753ab0a"
   },
   "source": [
    "### 토크나이저\n",
    "\n",
    "****\n",
    "**\"토크나이저\" 절의 요약**\n",
    "- 토크나이저의 어휘사전은 일반적으로 모델의 임베딩 층 크기보다 작습니다.\n",
    "  - 크기 차이는 임베딩 층의 크기를 바꾸지 않고도 새로운 토큰을 추가할 수 있는 빈 슬롯 때문입니다.\n",
    "  - 효율적인 메모리 할당을 위해 임베딩 층의 크기는 2의 거듭제곱의 배수(32, 64 등)인 경우가 많습니다.\n",
    "- `EOS` 토큰은 다른 것 말고 텍스트의 끝을 나타내는데만 사용해야 합니다.\n",
    "  - 패딩을 위해 `EOS` 토큰을 사용하면 토큰 생성이 끊임없이 계속되는 문제가 발생할 수 있습니다.\n",
    "- `PAD` 토큰을 정의하지 않는 경우가 많지만 여전히 이 토큰이 필요합니다.\n",
    "  - `EOS` 토큰을 `PAD` 토큰으로 할당하지 마세요.\n",
    "  - `UNK` 토큰이 정의되어 있다면 이를 `PAD` 토큰으로 할당해도 괜찮습니다.\n",
    "  - `UNK` 토큰이 정의되어 있지 않다면 `PAD` 토큰을 위해 새로운 특수 토큰을 만드세요.\n",
    "  - 주의: `PAD` 토큰을 정의하지 않은 채로 두면 많은 라이브러리에서 기본적으로 `EOS` 토큰을 패딩 토큰으로 할당합니다!!\n",
    "- 생성 모델의 경우 패딩은 왼쪽에 추가되어야 합니다.\n",
    "  - 오른쪽에 패딩하면 모델이 패딩 토큰의 시퀀스를 생성하도록 훈련됩니다.\n",
    "  - `SFTTrainer` 클래스에서 보고된 오버플로 문제 때문에 많은 튜토리얼에서 `tokenizer.padding_side='right'`를 사용합니다.\n",
    "    - 표준 패딩이 아니라 패킹이나 패킹 역할을 하는 콜레이터(\"패킹된 데이터셋\" 절 참조)를 사용하는 경우에만 괜찮습니다.\n",
    "- 새로운 특수 토큰을 만든다면 (임베딩 층의 빈 슬롯을 사용하기 때문에) 이론적으로 임베딩 층도 미세 튜닝해야 합니다.\n",
    "  - 실제로는 임베딩을 동결하더라도 모델이 잘 동작할 수 있습니다.\n",
    "  - (해당 임베딩을 훈련하지 않았으므로) 새로운 토큰 표현이 랜덤하지만, 모델의 훈련 가능한 다른 부분이 이런 임베딩을 있는 그대로 사용하는 방법을 학습할 수 있습니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29252e77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9615964d7728490ab50a8635847dccc5",
      "6c8cd5089ecf434487bdafef98c64e07",
      "3879eec96fa64ec6ba7f5d69961972a6",
      "76c077128272491ab6e03439111de47d",
      "e068d8c89cd24f60aefc9ec888001a32",
      "962ee821496743c196fe0442cfa0f89f",
      "330952a8c8ba4c469e79c2359d5b62f2",
      "9c02058bba1540f1a4f6e4fef1cf8c8a",
      "01917d01b4554b96b2ab069d06213c3a",
      "5b1644ef4b16426382095d3e8354f39d",
      "360f62c50b7941cf9f163209493e536a"
     ]
    },
    "id": "29252e77",
    "outputId": "18dfc3fd-c376-46d1-a104-412b456fe055"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9615964d7728490ab50a8635847dccc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "config_phi = AutoConfig.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4328bccd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4328bccd",
    "outputId": "65805cdb-00a6-459f-bb3e-aaa5021f5dbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2803, 29915, 29879, 5993, 675, 445, 10541, 29991], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi(\"Let's tokenize this sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f1150",
   "metadata": {
    "id": "b82f1150"
   },
   "source": [
    "#### 어휘사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8a0766d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8a0766d",
    "outputId": "1a522007-8951-421d-baf8-ce07b8dcf229"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32011, 32064)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_phi), config_phi.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e74a2f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e74a2f6",
    "outputId": "5ee285dd-81e7-4e83-aaef-ed5fe4352d0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001),\n",
       " ('<|endoftext|>', 32000)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "163e5b4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "163e5b4e",
    "outputId": "c21be4b3-f895-41f0-aad4-26ca76f754b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', 32000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.eos_token, tokenizer_phi.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7b370",
   "metadata": {
    "id": "c0d7b370"
   },
   "source": [
    "#### 특수 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a1658f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a1658f5",
    "outputId": "f1f85be7-5d31-44e6-bd1c-5292244687ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<|endoftext|>', '<unk>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d361cad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d361cad",
    "outputId": "128bb65c-bec9-4822-b707-08591810529b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95090969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95090969",
    "outputId": "29d568d9-1e8a-4129-bf33-477aaaeb1063"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.cls_token, tokenizer_phi.sep_token, tokenizer_phi.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f65debf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f65debf1",
    "outputId": "1dc855d1-81c0-44e8-8921-30807fd810f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.add_special_tokens({'cls_token': '<cls>', 'sep_token': '<sep>', 'mask_token': '<mask>'})\n",
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2dc17b7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dc17b7d",
    "outputId": "9b21e46e-08c5-4133-8408-b7efdaa9899e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<mask>', 32013),\n",
       " ('<sep>', 32012),\n",
       " ('<cls>', 32011),\n",
       " ('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001),\n",
       " ('<|endoftext|>', 32000)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b3026",
   "metadata": {
    "id": "a33b3026"
   },
   "source": [
    "#### `EOS` 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "add45042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "add45042",
    "outputId": "b255cb3d-44c4-4b6f-8557-1e3e4fd31ffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<unk>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token = tokenizer_phi.unk_token\n",
    "tokenizer_phi.pad_token_id = tokenizer_phi.unk_token_id\n",
    "\n",
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cb4f4",
   "metadata": {
    "id": "167cb4f4"
   },
   "source": [
    "```python\n",
    "# 수정된 패딩 토큰을 위해 모델 설정을 업데이트합니다.\n",
    "if getattr(model, \"config\", None) is not None:\n",
    "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "if (getattr(model, \"generation_config\", None) s not None):\n",
    "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333bb99",
   "metadata": {
    "id": "3333bb99"
   },
   "source": [
    "#### `PAD` 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfe01c37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfe01c37",
    "outputId": "c6f7fa4b-315b-4239-9040-996b61ebb3dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 'left')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token, tokenizer_phi.padding_side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d36e92",
   "metadata": {
    "id": "50d36e92"
   },
   "source": [
    "### 데이터 콜레이터\n",
    "\n",
    "****\n",
    "**\"데이터 콜레이터\" 절의 요약**\n",
    "- `SFTTrainer` 클래스(5장 참조)의 `data_collator` 매개변수를 지정할 수 있습니다.\n",
    "- `DataCollatorForLanguageModeling`가 `SFTTrainer` 클래스의 기본 콜레이터입니다.\n",
    "  - 자동으로 토큰 ID를 레이블로 복제합니다.\n",
    "  - 모델이 자동으로 처리하므로 레이블을 이동시키지 않습니다.\n",
    "  - 전체 텍스트(프롬프트와 완성)를 레이블에 포함시키므로 지시 미세 튜닝에 이상적입니다.\n",
    "- 지시 모델이나 채팅 모델을 추가적으로 미세 튜닝한다면 `DataCollatorForCompletionOnlyLM`을 사용해 모델의 응답(완성)으로만 모델을 훈련할 수 있습니다.\n",
    "  - 이 콜레이터도 토큰 ID를 레이블로 복제하지만 프롬프트 토큰에 해당하는 ID는 -100으로 마스킹합니다.\n",
    "  - 단일 상호작용(한 쌍의 프롬프트와 완성)에서는 응답 템플릿만으로 완성의 위치를 찾을 수 있습니다.\n",
    "  - 다중 상호작용(여러 쌍의 프롬프트와 완성)에서는 프롬프트 토큰을 찾고 마스킹하기 위해 지시 템플릿과 응답 템플릿이 모두 필요합니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eeacb63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "referenced_widgets": [
      "ba13c1ca40134c9ba22cc646a7d77088",
      "2f4a8b8396a64e239d53cb1788570e58",
      "8c2de28bac5a48cd984cbc73494ca62e",
      "c7a5df74894d4330ae278c5761a57d5f",
      "5ad531b5d55a446a88a82b70c68fee32",
      "96232029da624ba69311fcfb2a536fd1",
      "c209a6ee59c24117935ae7dd753cc412",
      "c263a12a690c4e2c94a9939c4bbfd05e",
      "14b63ecccddb4d1b8c5110a2db9aaf11",
      "d7afe51a2801477195593c16c40e8e42",
      "382b08c08f5d44e9bb93ce8910abbee8",
      "19ebc9a7b1b84df6bdca61322fc94183",
      "c913da6231e441a8a72c1250740324e4",
      "cf373f2652a54695aacab617a0420cfd",
      "fbfa2c9f8ffe4b7f953329bd9cfc7d05",
      "db40f593cc7b46cc8ddaf1f318263ec5",
      "40265d98d2b9494898e51882ff393d18",
      "6d1fd2cdf86342e6b271d9afec90ad3f",
      "8037b46f2a3f4049b30940cb84821ddf",
      "234f804cd7ad4f4c8140c5e527f6d61c",
      "cc6ccfcf12a44ed39bee1e6ee62e4908",
      "6196f263de2946bd930cb28a42b517c6",
      "3b438ca8575a499ab39614b3fe8c2a61",
      "36c154c29af5448399d4b0b89c0f2b4c",
      "833f021a60184400ac56e9ba6511c695",
      "4d6ac89dcbf04255a534e83d91de053d",
      "66fdee83ecf24d529099429c32213f3e",
      "38ba6551fa6b4da7a749f092de6e311d",
      "9df82f6703d0452cb0ddde2dea109192",
      "f86edc1e325b495abc163df73f9a2f2a",
      "72595019dbc747deba06b5a8d605ac7b",
      "1402435313904ea18cef4b28f8a3b401",
      "4357b3ec695a49ebbc8c6502f009aff0",
      "2bd77536988446b799b9847d64e7244a",
      "ad8a3aaeb28c4768ae04e5a3e6b33d3a",
      "dce43b2be4bc458fb7907769179587bc",
      "78d20702862c4ef3aa4b7296e3c525b4",
      "07e5764c8a14452598a8f8f30ac7ccd6",
      "aa5bda4504f1470395e8af21695d798b",
      "0669a223fd25470fb05057019d9ad469",
      "68183f22173c499a8b1925e9f20720ae",
      "629239265e544f7aa40131511f3f5f52",
      "ebf33d3acb6a4701af62780481d8f463",
      "1bdab3e658254f3081512cd30d4131a2"
     ]
    },
    "id": "8eeacb63",
    "outputId": "79a23cbb-2e93-446e-d215-8d210b9fe74d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba13c1ca40134c9ba22cc646a7d77088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ebc9a7b1b84df6bdca61322fc94183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentences.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b438ca8575a499ab39614b3fe8c2a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd77536988446b799b9847d64e7244a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(720,\n",
       " {'messages': [{'content': 'The birch canoe slid on the smooth planks.',\n",
       "    'role': 'user'},\n",
       "   {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "    'role': 'assistant'}]})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "# 프롬프트/완성 쌍을 대화 메시지로 변환합니다.\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "PGr-lDrb8kZO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "a8fb725a2e474082b36cf58919ac866d",
      "f8d852e88d8249f7be456c50da127eec",
      "e1fd0956ee28418caf6526c10d8b8224",
      "a032c2e71602494e8c0bb51a4de8f2ff",
      "4b8e97f5352b4a28a490d43c5b1dd694",
      "1646a6620a9f4c2d8adac080392a7cc7",
      "c134a00bdfdd499bb5204c98ecf02ec3",
      "93bd32f0e2684d0488ce15e2640c7b70",
      "aeed861ac9814fdf8a95f11a75f5a444",
      "13b01d31705f4359beb6c551b9fdc598",
      "24eb54eb389944e4a74f3e091a510528"
     ]
    },
    "id": "PGr-lDrb8kZO",
    "outputId": "e2723df7-5ca5-4de7-a1fe-18afdaf0707b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fb725a2e474082b36cf58919ac866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "# conversations_formatting_function() 함수는 trl 0.27 버전에서 삭제될 예정입니다.\n",
    "# 대신 apply_chat_template() 메서드를 사용하세요.\n",
    "# formatting_func = conversations_formatting_function(tokenizer_phi,\n",
    "#                                                     messages_field='messages')\n",
    "def formatting_func(row):\n",
    "    return tokenizer_phi.apply_chat_template(row[\"messages\"], tokenize=False,\n",
    "                                             add_generation_prompt=False)\n",
    "\n",
    "dataset = dataset.map(lambda row: {'text': formatting_func(row)},\n",
    "                      batched=True, batch_size=32)\n",
    "sequences = dataset['text']\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e400ae2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c1141490f1124a37a89a95fcd2b754f6",
      "003e8ab54690483fac0cc45bf569dae7",
      "427de29745704a1b96fcfc4cfe6f2c01",
      "3d7fc8fba93c447dbc375cd7195b468d",
      "d98d590188944b3bb3e3cc3bda248054",
      "ac8059caa83a4f0184e73f2aa5fd59e3",
      "8b6938914b684040b51a43ed1e5ca634",
      "d0272537aac446d39020664b8767fb97",
      "b8f8c271d9cb4f918a59b072dc15f5ba",
      "0857da25ec094ddfa235102defa765b9",
      "c71474cb66854e64948ed3721cfb98f0"
     ]
    },
    "id": "2e400ae2",
    "outputId": "cb61eb61-844d-4eda-ed70-453db5512d2f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1141490f1124a37a89a95fcd2b754f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(lambda row: tokenizer_phi(row['text']))\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a276bd5",
   "metadata": {
    "id": "6a276bd5"
   },
   "source": [
    "#### `DataCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fcce437",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fcce437",
    "outputId": "a8422ca6-6261-4ae4-e30b-7258dcc8ba12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_collator = DataCollatorWithPadding(tokenizer_phi)\n",
    "pad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=pad_collator)\n",
    "pad_batch = next(iter(pad_dloader))\n",
    "pad_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a36a82",
   "metadata": {
    "id": "07a36a82"
   },
   "source": [
    "#### 레이블은 어디 있나요?\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/shift_labels.png?raw=True)\n",
    "\n",
    "<center>그림 4.6 입력과 한 위치 이동한 레이블</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606662ee",
   "metadata": {
    "id": "606662ee"
   },
   "source": [
    "#### `DataCollatorForLanguageModeling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b22713c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b22713c7",
    "outputId": "8cd0769f-3857-48eb-c7f1-eb28c0e742c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_collator = DataCollatorForLanguageModeling(tokenizer_phi, mlm=False)\n",
    "lm_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=lm_collator)\n",
    "lm_batch = next(iter(lm_dloader))\n",
    "lm_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ef2f1",
   "metadata": {
    "id": "2b3ef2f1"
   },
   "source": [
    "#### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V075PzI5ZGo-",
   "metadata": {
    "id": "V075PzI5ZGo-"
   },
   "source": [
    "**중요**: `DataCollatorForCompletionOnlyLM`는 `trl` 0.20 버전에서 삭제되었습니다. 이 콜레이터는 사용자 프롬프트를 마스킹하여 완성 부분으로만 모델을 훈련합니다. 응답 템플릿을 사용해 완성의 시작 부분을 감지하고 손실 계산에서 프롬프트 토큰을 제외시킵니다.\n",
    "\n",
    "최신 버전에서는 이 로직이 내장되었습니다. `SFTConfig` 객체에 `completion_only_loss` 또는 `assistant_only_loss`가 설정되면 손실 계산에서 프롬프트 토큰이 자동으로 무시됩니다. 이 방식이 더 간단하지만 호환되는 채팅 템플릿에 의존하므로 유연성은 떨어집니다.\n",
    "\n",
    "유연성을 유지하면서 내부 마스킹 처리 과정을 보여주기 위해 `DataCollatorForCompletionOnlyLM` 구현을 (이 장의 시작 부분에서 임포트한) `compatibility_functions.py`에 복사하여 완성 기반 훈련에 이를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9c58443",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9c58443",
    "outputId": "0ddba77f-23b1-4692-d9cd-4c31b28a2a17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>' # 토큰 ID 32001\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ebcd7dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "1ebcd7dd",
    "outputId": "6a16844d-be83-4b6a-fe95-b83b422c6820"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels'][0]\n",
    "valid_tokens = (labels >= 0)\n",
    "tokenizer_phi.decode(labels[valid_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b7d1d",
   "metadata": {
    "id": "281b7d1d"
   },
   "source": [
    "##### 다중 상호작용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58067e2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bdc61d8cb65a48a9a4e301588e835b49",
      "8a86e8ed11794355b71493019fb328de",
      "f6008289ebbc47388b44db2bc5c0a9aa",
      "f2dd216b341145069781a92fea0a862b",
      "255fd2e6a56d4ba4b2f7c872643d1323",
      "c1c39160a8e54c2ea4bb65e6626bad3c",
      "221c2457d3bc4a79afcfc4669b886403",
      "88652d80064d4a36a6a3c59365e651cd",
      "daf27ae6a1a94f83b449d2634f0054f7",
      "5eb0817a50004849a61e81ebc551af0f",
      "9781246921814558951bc530439469ef"
     ]
    },
    "id": "58067e2e",
    "outputId": "8745225c-7e6f-4f42-ec8e-44ad292a0ab7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc61d8cb65a48a9a4e301588e835b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_chat = \"\"\"<|user|>Hello\n",
    "<|assistant|>How are you?\n",
    "<|user|>I'm fine! You?\n",
    "<|assistant|>I'm fine too!\n",
    "<|endoftext|>\"\"\"\n",
    "\n",
    "dummy_ds = Dataset.from_dict({'text': [dummy_chat]})\n",
    "dummy_ds = dummy_ds.map(lambda row: tokenizer_phi(row['text'])).select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7408873c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7408873c",
    "outputId": "c5b28ecc-cebc-4199-815c-204d96613357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18bc8194",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "18bc8194",
    "outputId": "3adaf955-81e5-46ea-f5f8-52b19886c0dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c1a80c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c1a80c5",
    "outputId": "460d7a1d-6b7e-4acf-b3f0-fbe807e82216"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  1128,   526,   366, 29973,    13,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_template = '<|user|>'\n",
    "response_template = '<|assistant|>'\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
    "                                                      response_template=response_template,\n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a052edea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "a052edea",
    "outputId": "3b37bf94-01f3-47f3-a0de-a807a066517a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"How are you?\\n I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203234de",
   "metadata": {
    "id": "203234de"
   },
   "source": [
    "#### 레이블 이동\n",
    "\n",
    "```python\n",
    "if labels is not None:\n",
    "    # 모델 병렬화를 위해 레이블을 올바른 장치로 이동시킵니다.\n",
    "    labels = labels.to(lm_logits.device)\n",
    "    # 다음 토큰 예측을 수행하므로 예측 점수와 입력 아이디를 하나씩 이동시킵니다.\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742073f",
   "metadata": {
    "id": "f742073f"
   },
   "source": [
    "### 패킹된 데이터셋\n",
    "\n",
    "****\n",
    "**\"패킹된 데이터셋\" 절의 요약**\n",
    "- 패킹은 시퀀스를 연결하여 동일한 크기의 청크로 나눕니다.\n",
    "  - 패딩 토큰이 사용되지 않습니다.\n",
    "  - 각 청크의 길이는 모델의 최대 시퀀스 길이를 넘어서는 안됩니다.\n",
    "- 패킹은 기본적으로 `SFTTrainer`에서 지원합니다.\n",
    "  - `packing` 매개변수를 `True`로 지정합니다.\n",
    "  - `pack_dataset()` 함수를 사용하여 패킹을 처리합니다.\n",
    "  - `packing_strategy`를 `wrapped`로 지정하여 원본 패킹 동작을 근사할 수 있습니다.\n",
    "  - 기본적으로 패킹과 콜레이터를 동시에 사용할 수 없습니다.\n",
    "- 일부 콜레이터는 효과적으로 시퀀스를 패킹할 수 있습니다.\n",
    "  - 이 경우 `packing` 매개변수를 `False`로 지정해야 하며 콜레이터가 패킹을 수행합니다.\n",
    "  - `DataCollatorWithFlattening`는 `DataCollatorForLanguageModeling`의 패킹 버전입니다.\n",
    "  - `DataCollatorForCompletionOnlyLM`에는 완성 전용 콜레이터가 패킹 같은 기능을 수행하도록 만드는 새로운 매개변수(`padding_free`)가 있습니다.\n",
    "  - 특정 모델(예를 들면, Llama, Phi, Mistral, Gemma, OLMo 등)은 플래시 어텐션 2로 이런 콜레이터를 지원합니다.\n",
    "    - 이런 모델은 `position_ids`를 사용하여 패킹된 원본 시퀀스 사이의 경계를 표시합니다.\n",
    "****\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packed_seq.png?raw=True)\n",
    "\n",
    "<center>그림 4.7 패킹된 시퀀스</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad7519b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad7519b0",
    "outputId": "75e1f426-8c27-49cd-aae7-959af9a34040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['text']\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc4a53a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "ec1e615e1f0343078e286ae40a9fd034",
      "8833b0f4debf47eab6c59b16ead284fb",
      "1794ceb087044601b1feaff5bc0e3916",
      "91211060fa67486dbfe8b718e09c339d",
      "f45841b956ea4d7ab8ec7c39064a7392",
      "2bf0bf3f1e094c908c85bf93c0607874",
      "d7b98044e4ef44ba83fc3114f6c9dd80",
      "cd5b5e9da6194059a6f82e1b39ea43e5",
      "7a457c2375844d8799667109064d7d1a",
      "a32f9dc0bfc44291a7f383cb60635e5d",
      "c58c1f9e30a94e368fde8d48d236cc09"
     ]
    },
    "id": "bc4a53a8",
    "outputId": "4bc2131d-f854-41df-c791-47f957f7ebf9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1e615e1f0343078e286ae40a9fd034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 341\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_dataset = pack_dataset(tokenized_dataset, seq_length=64,\n",
    "                              strategy='wrapped')\n",
    "packed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00d4e10a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "00d4e10a",
    "outputId": "9bf902ea-b04a-4ef9-c6e0-4b4fce1f1c43"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|user|> The birch canoe slid on the smooth planks.<|end|><|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|><|user|> Glue the sheet to the dark blue background.<|end|><|assistant|> Glue the sheet to the dark blue background, you must.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = packed_dataset['input_ids']\n",
    "tokenizer_phi.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad3c82",
   "metadata": {
    "id": "21ad3c82"
   },
   "source": [
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packing_flow.png?raw=True)\n",
    "\n",
    "<center>그림 4.8 올바른 데이터 설정 선택하기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5af2d",
   "metadata": {
    "id": "79f5af2d"
   },
   "source": [
    "#### 패킹을 위한 콜레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8df4a",
   "metadata": {
    "id": "9ad8df4a"
   },
   "source": [
    "##### `DataCollatorWithFlattening`\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/collator_flat.png?raw=True)\n",
    "\n",
    "<center>그림 4.9 패킹 유사 콜레이터</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bfc8432",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bfc8432",
    "outputId": "418a6e1e-8347-4eec-c7eb-96245e9a508f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'labels': tensor([[ -100,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "          36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_collator = DataCollatorWithFlattening()\n",
    "flat_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=flat_collator)\n",
    "flat_batch = next(iter(flat_dloader))\n",
    "flat_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ecb7453",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ecb7453",
    "outputId": "3f05d462-9a42-4205-860f-16e921f6f51b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 66]), tensor(38))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_batch['input_ids'].shape, flat_batch['position_ids'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81fdd2",
   "metadata": {
    "id": "af81fdd2"
   },
   "source": [
    "##### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f900880b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f900880b",
    "outputId": "307fb057-a1d8-4931-f61f-5bc6f4bca4d5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "           434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "         32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]]), 'cu_seq_lens_q': tensor([[ 0, 38, 66]], dtype=torch.int32), 'cu_seq_lens_k': tensor([[ 0, 38, 66]], dtype=torch.int32), 'max_length_k': tensor([38]), 'max_length_q': tensor([38])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>'\n",
    "completion_nopad_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                                            tokenizer=tokenizer_phi,\n",
    "                                                            padding_free=True)\n",
    "completion_nopad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_nopad_collator)\n",
    "completion_nopad_batch = next(iter(completion_nopad_dloader))\n",
    "completion_nopad_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e818",
   "metadata": {
    "id": "aed9e818"
   },
   "source": [
    "### 고급 방법: BYOT (Bring Your Own Template)\n",
    "\n",
    "****\n",
    "**\"고급 방법: BYOT\" 절의 요약**\n",
    "\n",
    "- 모든 템플릿은 응답 템플릿을 정의해야 하며, 이상적으로는 EOS 토큰으로 끝나야 합니다.\n",
    "- 토크나이저의 `EOS`, `PAD`, `UNK` 토큰을 다시 확인하세요.\n",
    "  - `EOS` 토큰은 `PAD` 및 `UNK` 토큰과 달라야 합니다.\n",
    "  - `PAD` 및 `UNK` 토큰은 같을 수 있습니다.\n",
    "- 꼭 필요할 때 임베딩 층의 크기를 바꿉니다(\"빈 슬롯\"이 모두 사용된 경우).\n",
    "  - 모델의 `resize_token_embeddings()`를 호출할 때, `pad_to_multiple_of` 매개변수를 사용해 크기가 2의 거듭제곱의 배수로 유지되도록 합니다.\n",
    "- 진자 템플릿을 직접 만들고 싶지 않다면 ChatML과 같은 기본 템플릿을 사용할 수 있습니다. `trl` 패키지의 `setup_chat_format()` 함수를 사용할 수 있지만 몇 가지 단점이 있습니다(`setup_chat_format()` 함수는 `trl` 0.26.0 버전에서 삭제되었습니다. 대신 `SFTConfig(..., chat_template_path=\"...\")`를 사용하세요).\n",
    "  - `EOS` 토큰을 `PAD` 토큰에 할당합니다(나중에 수동으로 수정해야 합니다).\n",
    "  - 더 짧게 만들기 위해서라도 모델의 임베딩 층 크기를 기본적으로 조정합니다(적절한 `resize_to_multiple_of`를 선택하면 크기 변경을 피할 수 있습니다).\n",
    "- 토크나이저를 위해 진자 템플릿을 만드는 대신 포맷팅 함수를 사용하여 사용자 정의 템플릿을 정의하고 적용할 수 있습니다.\n",
    "  - `SFTTrainer` 클래스(5장 참조)에서 `formatting_func`을 지정하면 토크나이저가 채팅 템플릿을 가지고 있을 필요가 없습니다.\n",
    "- 응답 템플릿을 신중하게 선택하세요.\n",
    "  - 일반 단어(예: \"## Answer:\")를 사용하면 문제가 발생할 수 있습니다. 일부 토크나이저는 문맥에 의존적이어서 응답 템플릿을 여러 토큰으로 분할할 수 있기 때문입니다.\n",
    "  - 응답 템플릿을 위해 추가적인 특수 토큰을 만드는 것이 더 안전합니다. 이렇게 하면 응답 템플릿이 단일 토큰으로 인코딩되기 때문입니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23603681",
   "metadata": {
    "id": "23603681"
   },
   "source": [
    "#### 채팅 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30ccd450",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "a773e6ddcb6f4775aab0f566660dcc7b",
      "249b7c3a4c6f4550a0fc4ba7edd8bccd",
      "dbe77951710e4e65ba42b82ab00b572c",
      "c5286709a7c947aea26a3fc98a7d4671",
      "c21d347fe9b04767ba0d8ac330a4a725",
      "76aea6e8dae5464a9df225d14e5a6df0",
      "6a45e7b5316145b6ace4c02e648b0faf",
      "003b8b38791c44d9b409da29a67ea022",
      "6685f1ba35fb4a73b68ea5f19e47e166",
      "0a2e47995eb746b09cc1dd7e022651d3",
      "f3f67d9d839a41f58403f9ee1b55ed75",
      "9837c5e675294f5794ef355480975d64",
      "ddd9abd6d22c4e8380067d1f291be4ac",
      "f97de6284c6e41858694eca1c491db21",
      "cfcaaa1e8b714406adb7b865418042ed",
      "7c25fcf99acc4f32a266a1a23314096f",
      "72a9f77b5e664375ae6ad35a6b59edbf",
      "6d536ecde70447c4bcfcc3f7a0376869",
      "2ef5015d91ab4fcba7c1928d9a32ec67",
      "264d0f29273f485e8cad9080ac9ce8e4",
      "2da1b54c0d8144029de88f9d30d3ff70",
      "8f4a557e76cc413aaae6ae33907bf52e",
      "df227ac86c1e4abf9db239de1f4168f9",
      "c165c1bf2c9e469690f33321b87b4e1c",
      "59f735acfebd40a69512e41ed194b266",
      "a5bc45ed905d4288aac65e58409058af",
      "d7ee93c3211143938f9626277f09d336",
      "aa2cf3a04c4f44079328324b5fffdb11",
      "aa797941c0074526a5f40996f915affc",
      "f37ed4c0a919484e8f07f7d8adbf6197",
      "42a0a080926c430fafec212d3f2d9900",
      "355931092a644cb69f9f5fd164a59ac2",
      "4c391b5cfb3044c4901a4350690ef2a8",
      "c68574501d854aec9c296e98ee2f6a06",
      "9974315fa7fd43c4aeebee98bfe131c0",
      "07ea019d8ae6442692f30207a293f910",
      "f0a94d5f8dc946948c9c67cd0264906b",
      "b1545736f3e04b6594278ba2db120f9e",
      "4ef449ac4fbc47e8a36776fe871b7e59",
      "700e0a95bb054fbf950fa77b5a48e705",
      "f6a4f2dddf94462faab38420f44059f4",
      "6437d62055e14bbb9e5e25f6494258e2",
      "917c7a4449ba4fb8b7dc413448a2c186",
      "1317cfdbccf34da894cd80514f884215"
     ]
    },
    "id": "30ccd450",
    "outputId": "b2516bed-56ab-4c9d-def1-904f8388e4c2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a773e6ddcb6f4775aab0f566660dcc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9837c5e675294f5794ef355480975d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df227ac86c1e4abf9db239de1f4168f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68574501d854aec9c296e98ee2f6a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "print(tokenizer_opt.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55c44376",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c44376",
    "outputId": "75f85776-a5fd-43ea-8dad-df65c7e349ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '</s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '</s>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a1505",
   "metadata": {
    "id": "dd4a1505"
   },
   "source": [
    "**ChatML**\n",
    "****\n",
    "\n",
    "[ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)은 채팅 마크업 언어(Chat Markup Language)의 약어로 오픈AI에서 개발했습니다:\n",
    "\n",
    "_____\n",
    "\"_전통적으로 GPT 모델은 구조화되지 않은 텍스트를 사용했습니다. ChatGPT 모델은 대신에 채팅 마크업 언어(줄여서 ChatML)이라는 구조적인 포맷을 기대합니다. ChatML 문서는 일련의 메시지로 구성됩니다._\"\n",
    "_____\n",
    "\n",
    "앞서 소개한 대화 포맷과 비슷하게 각 메시지는 참여자의 역할과 그에 해당하는 콘텐츠로 구성됩니다. ChatML의 진자 템플릿은 다음과 같습니다:\n",
    "\n",
    "```\n",
    "{% for message in messages %}\n",
    "  {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n",
    "{% endfor %}\n",
    "```\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e439158",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e439158",
    "outputId": "007ec4bb-80a2-4632-d9c2-e53e74cdcfa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d70086b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d70086b2",
    "outputId": "96c2c26e-e7ed-49df-a76d-e2a51dbe3453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50272"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b91f663",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b91f663",
    "outputId": "1780f77b-af89-4019-ee24-c2a6d4c83743"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "pad_to_multiple_of = get_multiple_of(model_opt.config.vocab_size)\n",
    "pad_to_multiple_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4aff379c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aff379c",
    "outputId": "57995a32-a73d-483f-9485-812b40b7745e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.resize_token_embeddings(len(tokenizer_opt),\n",
    "                                  pad_to_multiple_of=pad_to_multiple_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82c9973e",
   "metadata": {
    "id": "82c9973e"
   },
   "outputs": [],
   "source": [
    "def modify_tokenizer(tokenizer,\n",
    "                     alternative_bos_token='<|im_start|>',\n",
    "                     alternative_unk_token='<unk>',\n",
    "                     special_tokens=None,\n",
    "                     tokens=None):\n",
    "    eos_token, bos_token = tokenizer.eos_token, tokenizer.bos_token\n",
    "    pad_token, unk_token = tokenizer.pad_token, tokenizer.unk_token\n",
    "\n",
    "    # BOS 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    if bos_token == eos_token:\n",
    "        bos_token = alternative_bos_token\n",
    "\n",
    "    # UNK 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    if unk_token == eos_token:\n",
    "        unk_token = alternative_unk_token\n",
    "\n",
    "    # PAD 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    # 하지만 UNK 토큰과는 같을 수 있습니다.\n",
    "    if pad_token == eos_token:\n",
    "        pad_token = unk_token\n",
    "\n",
    "    assert bos_token != eos_token, \"다른 BOS 토큰을 선택하세요.\"\n",
    "    assert unk_token != eos_token, \"다른 UNK 토큰을 선택하세요.\"\n",
    "\n",
    "    # BOS, PAD, UNK 토큰을 위한 딕셔너리를 만듭니다.\n",
    "    # EOS 토큰은 원래 정의된 대로 유지합니다.\n",
    "    special_tokens_dict = {'bos_token': bos_token,\n",
    "                           'pad_token': pad_token,\n",
    "                           'unk_token': unk_token}\n",
    "\n",
    "    # 새로운 특수 토큰을 추가합니다.\n",
    "    if special_tokens is not None:\n",
    "        if isinstance(special_tokens, list):\n",
    "            special_tokens_dict.update({'additional_special_tokens': special_tokens})\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # 새로운 일반 토큰을 추가합니다.\n",
    "    if tokens is not None:\n",
    "        if isinstance(tokens, list):\n",
    "            tokenizer.add_tokens(tokens)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9377ffe7",
   "metadata": {
    "id": "9377ffe7"
   },
   "outputs": [],
   "source": [
    "def jinja_template(tokenizer):\n",
    "    return (\"{% for message in messages %}\"\n",
    "            f\"{{{{'{tokenizer.bos_token}' + message['role'] + '\\n' + message['content'] + '{tokenizer.eos_token}' + '\\n'}}}}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            f\"{{{{ '{tokenizer.bos_token}assistant\\n' }}}}\"\n",
    "            \"{% endif %}\")\n",
    "\n",
    "def add_template(tokenizer, chat_template=None):\n",
    "    # 채팅 템플릿이 주어지지 않으면 BOS와 EOS 토큰을 사용해\n",
    "    # 채팅 템플릿을 만듭니다.\n",
    "    if chat_template is None:\n",
    "        chat_template = jinja_template(tokenizer)\n",
    "\n",
    "    # 채팅 템플릿을 토크나이저에 할당합니다.\n",
    "    tokenizer.chat_template = chat_template\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6869f622",
   "metadata": {
    "id": "6869f622"
   },
   "outputs": [],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "def modify_model(model, tokenizer):\n",
    "    # 새로운 토크나이저의 크기가 어휘사전 크기를 초과한다면\n",
    "    # 같은 배수가 되도록 유지하면서 크기를 바꿉니다.\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        pad_to_multiple_of = get_multiple_of(model.vocab_size)\n",
    "        model.resize_token_embeddings(len(tokenizer),\n",
    "                                      pad_to_multiple_of=pad_to_multiple_of)\n",
    "\n",
    "    # 모델 설정의 토큰 ID를 업데이트합니다.\n",
    "    if getattr(model, \"config\", None) is not None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b3459c8",
   "metadata": {
    "id": "1b3459c8"
   },
   "outputs": [],
   "source": [
    "tokenizer_opt = modify_tokenizer(tokenizer_opt)\n",
    "tokenizer_opt = add_template(tokenizer_opt)\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b7bf5da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b7bf5da",
    "outputId": "15fc7b4d-3309-4dc4-90a3-e6cd36f852fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|im_start|>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d96ae3a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d96ae3a5",
    "outputId": "99557f7c-d57a-4978-8aa9-7317ace93c36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50266"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04a2e433",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "04a2e433",
    "outputId": "f93c28ee-1062-4b80-d2fd-b02708dcac02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|im_start|>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec72d660",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec72d660",
    "outputId": "94a37470-dc33-415d-a45a-24bd71d036c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9e77014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9e77014",
    "outputId": "98cf1c7f-989b-4c77-dd56-ac95659f737d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '</s>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_opt.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "197e5d83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "197e5d83",
    "outputId": "8e594a17-1cfe-48c5-eba3-bc618be9cc48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of Argentina?</s>\n",
      "<|im_start|>assistant\n",
      "Buenos Aires.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = ds_msg['messages'][0]\n",
    "print(tokenizer_opt.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea9914",
   "metadata": {
    "id": "7aea9914"
   },
   "source": [
    "#### 사용자 정의 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41741481",
   "metadata": {
    "id": "41741481"
   },
   "outputs": [],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "response_template = '##[YODA]##>'\n",
    "tokenizer_opt = modify_tokenizer(tokenizer_opt, special_tokens=[response_template])\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ab49d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "2ab49d36",
    "outputId": "53bab5a5-2104-43b4-b3fe-44f439d63b5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>formatting_func_builder.&lt;locals&gt;.formatting_func</b><br/>def formatting_func(examples, add_generation_prompt=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-955622997.py</a>&lt;no docstring&gt;</pre></div>"
      ],
      "text/plain": [
       "<function __main__.formatting_func_builder.<locals>.formatting_func(examples, add_generation_prompt=False)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func_builder(response_template):\n",
    "    def formatting_func(examples, add_generation_prompt=False):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples['prompt'])):\n",
    "            text = f\"{examples['prompt'][i]}\"\n",
    "            try:\n",
    "                text += f\" {response_template} {examples['completion'][i]}{tokenizer_opt.eos_token}\"\n",
    "            except KeyError:\n",
    "                if add_generation_prompt:\n",
    "                    text += f\" {response_template} \"\n",
    "            output_texts.append(text)\n",
    "        return output_texts\n",
    "    return formatting_func\n",
    "\n",
    "yoda_formatting_func = formatting_func_builder(response_template)\n",
    "yoda_formatting_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46d5d7fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "46d5d7fa",
    "outputId": "cbba6a16-4f9f-4805-bc5a-927d4f9bd817"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The birch canoe slid on the smooth planks. ##[YODA]##> On the smooth planks, the birch canoe slid. Yes, hrrrm.</s>'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "\n",
    "formatted_seqs = yoda_formatting_func(dataset)\n",
    "formatted_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b45c968b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b45c968b",
    "outputId": "d8411fc9-40e1-4805-fbb9-ac5f533f3a7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 133, 23629, 611, 31728, 13763, 15, 5, 6921, 563, 2258, 4, 1437, 50266, 374, 5, 6921, 563, 2258, 6, 5, 23629, 611, 31728, 13763, 4, 3216, 6, 1368, 28015, 22900, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt(formatted_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fba69d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "4fba69d1",
    "outputId": "0396484c-0b2b-4d9d-fbdf-7414b6c38792"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'##[YODA]##>'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "482f5c0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "482f5c0e",
    "outputId": "1b0fa750-c93c-46c5-a7f3-c2f59547541d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Force is strong in you. ##[YODA]##> ', 'I am your father! ##[YODA]##> ']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_formatting_func({'prompt': ['The Force is strong in you.',\n",
    "                                 'I am your father!']},\n",
    "                     add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934abad6",
   "metadata": {
    "id": "934abad6"
   },
   "source": [
    "#### 특수 토큰 만세!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fV9GeS8C5DVs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360,
     "referenced_widgets": [
      "dd8eb8a78ab24733a2541e02721f6860",
      "aaff375b58034bbe8c9f807cd346324a",
      "0082a67c533b407ba7974aedefc5524b",
      "58a637f850d7404e8039418a66c03c53",
      "faa4d7f758b04d9c8684ae0a89f5e5bd",
      "754dd9736159439fa4efd77f018e0ae5",
      "68eebe06b5aa4d388e4424ecf1601095",
      "66a42100541e40e4892a2993cb2ec99f",
      "c1080419006549d5b60811d3d15faf86",
      "45479e405fc54fb4afa829a5f4ecc21a",
      "a900bc824b6d4a6f8509f68c7ba4bb1f",
      "f8c2e2ccdaa04251a42bea849361a1d4",
      "cceb87edf3954bd7a143787dd1747d16",
      "28255f9b92e64856af54ced5bfda2aef",
      "5dcf79ad52bd4cbaa9c9dcf2f259da89",
      "38774013256941e2a73de73941386e08",
      "b0859a216543431c9a7185967839fe37"
     ]
    },
    "id": "fV9GeS8C5DVs",
    "outputId": "a67c7c1f-a11a-41ac-9df1-328be5dc24a6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8eb8a78ab24733a2541e02721f6860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1db3646e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "8c0454b21e4b473fbe87550dd5b73097",
      "d86780dca312466e91ce28a384bf4841",
      "5cde7080db91494982aefccf318c609d",
      "a7847b7030c343fb8909f6bf1413c89c",
      "29e97e85a1cf4746b62915addd1c7bbd",
      "57148bdb44ab4a3498f6c3d910bc1d65",
      "5da9b5274a5a450ab25faf1effdbf2ce",
      "7ee778fc341e4a4eaf9ab0a64d31dbca",
      "c5c48c05856047bcb91a19fdeaac16a0",
      "4ab595b018884910a72a33a660788e9f",
      "37e6de91013d4ddf8b27674689b702d6",
      "93611a429c964f4ba77097bd6b0c9d06",
      "30ce6784ff87467aaf8e7145e7497377",
      "d2a9162a3f54489eaaf843d467d9e995",
      "89f688a2d8ea4f918ec3320305a96f85",
      "8b6e616a8f8a4ac9bebd2c095b1bc545",
      "452b25b4db5e4688b41cb6d618bc2bad",
      "12c8efef76794d3caeb031621064d827",
      "a47737076b334027b237b84af01654eb",
      "26feba9d22d04af7b244a90ddc671ad6",
      "db8678843878465d83e540a7947e8261",
      "604cb800c73f49e39768f62e823fd658",
      "3d8f6b836f1240e0835d1d02e8b549a1",
      "5db9bf06969a4fd6ab1f9f569e76256f",
      "ac664838515843c497451b93cd55c0fd",
      "f335828c101f4fd696e12fab7c1295cf",
      "d39f131c6b1c482bad5fa9e0053482e4",
      "d7746fe085104214bb3bf5b30678baab",
      "705f60d75e4141a1949f83cc9ce30f44",
      "14cabe9a997d474396b8c34dcc7cbb2d",
      "3f30994cb19b4b73955f02ac6908bc5d",
      "2ba41a50c1724a179321d42b214df5e7",
      "7b6414316c2f4d55b8bb1b8216732efd",
      "adbb28db1d404a7e9c751ba0f89abf47",
      "fe99f3e19769445b80436e562eae9833",
      "9fc283d51bb24d07aef88140c2a0756f",
      "a2180061d7304124b25d69503ca05ae4",
      "19436bf6da8144579554cbac9ad5c21c",
      "c0af53f770604a39ad2e96dbee956451",
      "b7ad49cb592245399d9932491adfe5fa",
      "f97c7552dea346c7b1647e537a3d375e",
      "a6b05d773a844fef85ea9dc84f0ac24d",
      "10ad54aa4bd54986949048436b7049d0",
      "33fdefac406e4184ac1be6b7b11c1431"
     ]
    },
    "id": "1db3646e",
    "outputId": "ab8bbd60-d056-41ea-918f-00ab7a4437d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0454b21e4b473fbe87550dd5b73097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93611a429c964f4ba77097bd6b0c9d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8f6b836f1240e0835d1d02e8b549a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb28db1d404a7e9c751ba0f89abf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer_llama.pad_token = tokenizer_llama.unk_token\n",
    "tokenizer_llama.pad_token_id = tokenizer_llama.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cabcec4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cabcec4b",
    "outputId": "108cb5c9-1582-4e39-d384-ebb9e8e892a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User: Hello\n",
      "\n",
      "### Assistant: Hi, how can I help you?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b327b52c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b327b52c",
    "outputId": "e541a1fa-c643-489f-cf37-97b44b8c3395"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer_llama.tokenize(prompt, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(prompt, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))[6:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "56c09206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56c09206",
    "outputId": "a93f51d4-971b-474b-c1a8-45250fc5ce8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2683ec6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233,
     "referenced_widgets": [
      "35e7e8a131d34b4abf7c7049b9bca168",
      "e539f817c490483bad5c0f0c3294c995",
      "ee122e212a2043078b7c8b3a6b96c6e0",
      "b5245bb100094e129b46659d6de13420",
      "0b93e15587a14e65924561f9553c1798",
      "c9ee96c045fc4c5fadd989f58a4b1dc7",
      "393cbd57a23b4257a2547857c099c87e",
      "a2b843dfb85842cb936a2532d5acc835",
      "88b13ee0ec364ff788ac15d65cd2bda8",
      "178b5c75aed047778de39510acc4510c",
      "243d0932596445e8b113e65a60739509"
     ]
    },
    "id": "e2683ec6",
    "outputId": "e436603c-7f41-46c9-824b-8329fc99b312"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e7e8a131d34b4abf7c7049b9bca168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/compatibility_functions.py:91: UserWarning: Could not find response key `### Assistant:` in the following instance: <s> ### User: Hello\n",
      "\n",
      "### Assistant: Hi, how can I help you?. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13,  2277, 29937,  4007,\n",
       "         22137, 29901,  6324, 29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_ds = Dataset.from_dict({'text': [prompt]})\n",
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])\n",
    "\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "bad_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "bad_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=bad_collator)\n",
    "bad_batch = next(iter(bad_dloader))\n",
    "bad_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "817f5914",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "817f5914",
    "outputId": "2a6fc6da-c063-45e2-cbfe-e14e5a1cf812"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁', 29871),\n",
       " ('<0x0A>', 13),\n",
       " ('##', 2277),\n",
       " ('#', 29937),\n",
       " ('▁Ass', 4007),\n",
       " ('istant', 22137),\n",
       " (':', 29901)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_response_template = \"\\n### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(modified_response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(modified_response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "402905c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402905c0",
    "outputId": "25375ea3-fd6d-46ac-fda8-40d4c9ef981d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13,  2277, 29937,  4007,\n",
       "         22137, 29901,  6324, 29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  6324, 29892,   920,   508,   306,  1371,   366, 29973]])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_token_ids = token_ids[2:]\n",
    "fixed_collator = DataCollatorForCompletionOnlyLM(fixed_token_ids, tokenizer=tokenizer_llama)\n",
    "fixed_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=fixed_collator)\n",
    "fixed_batch = next(iter(fixed_dloader))\n",
    "fixed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86d77bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86d77bdb",
    "outputId": "1d5413d7-7325-437a-e6bf-85bc22ec45cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokenizer_llama.add_special_tokens({'additional_special_tokens': [response_template]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfb6c648",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9fa5567d6f1042f88a12ac1de56696d7",
      "63a8bce0b4cf456bb4936f2f86df7a4d",
      "9b378840552745e3ad75b34909a545b0",
      "9a34c01127c44aa28fe67f0db4de01e1",
      "93ca0a4a29b74c7b8e04e05ce6bd4608",
      "f223f3fc1df44f83b776ddd9bb4899b2",
      "346af81e537947659758341717c6593b",
      "cca73d96c4034ecfa3de40418be43d51",
      "f8e0ee264d524e5faa6bc3df87e6e722",
      "6053bc84cbd249fe803b0e39c50f5207",
      "a8506a2ae80a4d3c9de7d5da41dec45c"
     ]
    },
    "id": "cfb6c648",
    "outputId": "9b8c2bcf-2252-4a1d-b568-72abcca3f6e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa5567d6f1042f88a12ac1de56696d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f07db988",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f07db988",
    "outputId": "f13dc2ca-7252-489d-dcf5-c55aeab41838"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13, 32000, 29871,  6324,\n",
       "         29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 29871,  6324,\n",
       "         29892,   920,   508,   306,  1371,   366, 29973]])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "special_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=special_collator)\n",
    "special_batch = next(iter(special_dloader))\n",
    "special_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c75e1",
   "metadata": {
    "id": "486c75e1"
   },
   "source": [
    "### 다음 장에서는\n",
    "\n",
    "채팅 템플릿은 야생의 LLM을 길들이고 인간과 적절한 대화를 할 수 있는 방법을 가르치기 위한 핵심입니다. 대화 사이에 큐 사인(또는 특수 토큰)을 적절하게 배치하면 명령 키워드에 따라 어떻게 응답해야 하는지 학습할 수 있습니다. 하지만 훈련 절차에 위험이 없는 것은 아닙니다. 활성화, 그레이디언트, 옵티마이저 모두 자신의 작업을 수행하기 위해 상당한 양의 RAM을 필요로 합니다. 메모리를 많이 소비하는 이런 구성 요소들을 만족시키려면 기술과 노력이 모두 필요합니다. 훈련 루프를 구성하려면 두려움을 떨쳐내야 합니다. 도전이 가득한 다음 장을 놓치지 마세요!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
