{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf87a32",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06813f-79d8-4c0a-a823-c3dc2da65292",
   "metadata": {
    "id": "2e06813f-79d8-4c0a-a823-c3dc2da65292"
   },
   "source": [
    "## 4장 데이터셋 포맷팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf67d8",
   "metadata": {
    "id": "2faf67d8"
   },
   "source": [
    "### 스포일러\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- 적절한 채팅 템플릿의 중요성을 이해합니다.\n",
    "- 사용자 정의 포맷팅 함수와 템플릿을 포함해 몇 가지 포맷팅 옵션에 대해 논의합니다.\n",
    "- 토크나이저와 모델의 임베딩 층을 설정합니다.\n",
    "- 패킹(packing)된 데이터셋과 데이터 로딩을 위한 다양한 데이터 콜레이터(data collator)를 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628f687",
   "metadata": {
    "id": "9628f687"
   },
   "source": [
    "### 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20be1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bb20be1",
    "outputId": "f884735d-5dfe-4a45-d100-b7c54dd2667b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting trl==0.19.1\n",
      "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.19.1) (1.10.0)\n",
      "Requirement already satisfied: transformers>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.19.1) (4.55.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.19.1) (5.9.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.19.1) (0.6.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl==0.19.1) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl==0.19.1) (0.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, trl\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.47.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 trl-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.55.2 peft==0.17.0 accelerate==1.10.0 trl==0.21.0 datasets==4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144dfaf",
   "metadata": {
    "id": "2144dfaf"
   },
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9ba58",
   "metadata": {
    "id": "22a9ba58",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, \\\n",
    "    DataCollatorForLanguageModeling, DataCollatorWithPadding, \\\n",
    "    DataCollatorWithFlattening, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "from trl.data_utils import pack_dataset\n",
    "from trl.extras.dataset_formatting import FORMAT_MAPPING, conversations_formatting_function\n",
    "from compatibility_functions import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd6389",
   "metadata": {
    "id": "80bd6389"
   },
   "source": [
    "### 목표\n",
    "\n",
    "LLM에게 데이터셋의 구조와 단서를 제공하기 위해 포맷팅을 합니다. 적절한 태그와 특수 토큰으로 각 구성 요소(사용자 프롬프트와 모델이 완성할 텍스트)를 감싸서 모델의 동작을 쉽게 조정(예를 들면 지시 미세 튜닝)할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bf61f",
   "metadata": {
    "id": "380bf61f"
   },
   "source": [
    "### 포맷팅의 핵심\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/base_prompt.png?raw=True)\n",
    "<center>그림 4.1 베이스 모델의 다음 토큰 예측</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/fine_tuned_prompt.png?raw=True)\n",
    "<center>그림 4.2 응답 템플릿을 사용하는 미세 튜닝된 모델</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_prompt_new.png?raw=True)\n",
    "<center>그림 4.3 채팅 템플릿을 사용하는 채팅 모델</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_example_new.png?raw=True)\n",
    "<center>그림 4.4 채팅 템플릿의 일반적인 구조</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a379eb0",
   "metadata": {
    "id": "0a379eb0"
   },
   "source": [
    "### 이전 장에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ec5f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "340d0daca3ce4384b8d748908f93f9db",
      "4f7ecc0921d341358b8b09b8703bd45a",
      "9530c9e5ae8046cfa4b7e33bef8d936d",
      "fb68739b1d3a4d6fa1b3a8825730c412",
      "978d611077364e3a8e14efef16fa6510",
      "4670d15bf5ca4a13877109a9863af531",
      "754a75fc6bfb4291941e6524eed64937",
      "71e0aa66b290424996ab6dd06151dc85",
      "687ae7ce6fcf4ea59acac074a13eb696",
      "3a49d7862a3a4279b37570da7a01a6ef",
      "24acbc3b2c48499fb814dad36ceb02cd",
      "36639e6835f644f59dd955a71d0b26be",
      "a035af91529e4d9eba2c21b8a9ffff0a",
      "b667484ccc6646d59135acbe89c20d36",
      "fe7066adeefd492f8b0d34a07ab47ece",
      "969fc900f0b242cfacafab979d3b4877",
      "9f3d224b343541db80f32265b2c51719",
      "4cacd893b1ec4ea4b51dd320282a633e",
      "718947ca4d454d34b02f88cbd9988b32",
      "cb597f3adaa34b1bb5a6c744656f358e",
      "6ab524e9139e44e98470c4f8a485c8c0",
      "5096277e37684631894b53a05ff4cb67",
      "5a9fc57af2fc4b36b15d5a2364005977",
      "ada53c14d5af4d5ab7c38357bd8c0a32",
      "389046610dc4458ebacef42ef97fc0bf",
      "1b9633ca06e04ce5b04152915655c971",
      "9bf0356fe1ec4ae1b2b0f1db292a100e",
      "0bb2e22bd63c4b0a990bf47ac2ba5122",
      "e3fa581882d044ec8a098baae4b78086",
      "19299eb353fd4c5ab0f402542b2787d0",
      "15725360aaea44bfb337b8a8b887df4d",
      "1650070ac88e49d29d42729bad23374d",
      "55c4b51bc53a4d988ac110a002f0f3ca",
      "44fabac67d18414c99503fe49f18c1b5",
      "a4ebda3b289b4c9ba2b4715aaf892583",
      "574fda18fdaf4a54830ed8862eb05736",
      "db21bd1d77094c8594d276fa64d09c04",
      "836ff1ed0345400b84d5f7eb158b6774",
      "bb208fb5d06f4d80943e2af28ee9b0db",
      "c5099bef50f8484495c06e8eb4f1544e",
      "2f7638fda0f64c73bcbb9156b749b917",
      "ed3c1494caeb4c12aa9f042a6cf33fbf",
      "0873f5238ac24d79a53a70ce86e92293",
      "aaa8a09bf38f408da1248b513aa1d373"
     ]
    },
    "id": "9d3ec5f2",
    "outputId": "78fec1bd-b0a2-47d9-c824-5b18a2fd2603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340d0daca3ce4384b8d748908f93f9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36639e6835f644f59dd955a71d0b26be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9fc57af2fc4b36b15d5a2364005977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fabac67d18414c99503fe49f18c1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='cuda:0',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)\n",
    "\n",
    "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model_q4, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e76cbf",
   "metadata": {
    "id": "c3e76cbf"
   },
   "source": [
    "### 템플릿 적용하기\n",
    "\n",
    "****\n",
    "**\"템플릿 적용하기\" 절의 요약**\n",
    "\n",
    "데이터셋 포맷팅에 세 가지 옵션이 있습니다:\n",
    "1. 데이터셋의 포맷이 `STTrainer` 클래스가 지원하는 두 개의 포맷 중 하나인 경우\n",
    "   - 토크나이저가 채팅 템플릿을 가지고 있어야 합니다.\n",
    "   - 포맷팅 함수를 정의하거나 훈련 전에 데이터셋을 포맷팅할 필요가 없습니다.\n",
    "2. 사용자 정의 포맷팅 함수를 사용하고 싶은 경우(BYOFF 절 참고)\n",
    "   - `SFTTrainer` 클래스(5장 참조)의 `formatting_func` 매개변수로 사용자 정의 함수를 전달해야 합니다.\n",
    "   - 사용자 정의 포맷팅 함수가 배치 데이터를 다룰 수 있어야 합니다.\n",
    "     - `batched=True`로 데이터셋의 `map()` 메서드를 호출하여 테스트하세요.\n",
    "    - 훈련 전에 데이터셋에 이 함수를 적용할 필요는 없습니다.\n",
    "    - 토크나이저가 이미 채팅 템플릿을 가지고 있는 경우\n",
    "      - 사용자 정의 함수에서 `apply_chat_template()` 메서드를 호출할 수 있습니다.\n",
    "      - 템플릿의 일반적인 포맷(지시 템플릿과 응답 템플릿)을 고수하세요.\n",
    "      - 템플릿에 `EOS` 토큰이 포함되어 있지 않다면 포맷팅된 출력 끝에 `EOS` 토큰을 추가할 수 있습니다.\n",
    "   - 토크나이저가 채팅 템플릿을 가지고 있지 않은 경우\n",
    "     - 지시 템플릿과 응답 템플릿을 포함하여 일반적인 포맷을 자유롭게 정의할 수 있습니다('고급 방법 - BYOT' 절 참고).\n",
    "3. 데이터셋이 이미 포맷팅된 경우\n",
    "   - `SFTTrainer` 클래스(5장 참조)의 `dataset_text_field` 매개변수에 포맷팅된 데이터를 담고 있는 열을 전달해야 합니다.\n",
    "   - 사용자 정의 포맷팅 함수를 사용하여 데이터셋을 전처리 하더라도 훈련 클래스는 이를 사용하지 않습니다.\n",
    "   - 데이터가 토크나이저의 템플릿과 호환되는지 확인하세요.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92fae6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329,
     "referenced_widgets": [
      "ebe31a452c484d2ba4c618505d30375f",
      "80a1dc2ea33345c8ad4af12b053e73ac",
      "347d500d935e40ecb2326afe5406416e",
      "8afc015e1d0246e79693ec827d9254fd",
      "246f10c93f1f4c62b73a1e1e29bafabb",
      "2ebc57c8d7ad43938e6301066f275021",
      "11afde8429c04dd0a2e5b2194e713f8c",
      "f33e0b6d17694cdab86deadf70d85f92",
      "d6094e737cea49e7bc2a46cc1429c7a9",
      "56b05392a5504089bbe88112af6f9c44",
      "79b78cbabb464d339693c1bb6e380821",
      "a8d50994f29f47fca5c56c615c3bcd96",
      "1c74793d8db14ebfa4ab808d99605fff",
      "b89c81f8f31b499eab7c23d9648059fe",
      "5395ff087f98443f94d9c8ca5bd8d2f6",
      "5d39a421237948cb9ed04657aef3baad",
      "58277632c868476da89d8a17187eca2e",
      "95b3d3cb79354ff5bf959fb5a769e0a6",
      "25e5476179a341e59f6354bbcb89a286",
      "246741f5f8124407b1821b0b94ce301c",
      "7f5f42a909434f60a4fc6c4386d0eb63",
      "3fcae19e2c644af1ba9bb6216da99423",
      "765c2fb006f94428a271b8c34930bd68",
      "8343d03cbe024ffb8248d960610b3bea",
      "e043c4c0268742da97a8fec8fee19195",
      "bb19c86e66e245119f384b97b510877a",
      "ef89ec271b4a442bad7d3d00f90a8b3f",
      "5fba4954020745b8a47cde85bbfc8ef8",
      "3f7cb7464bbb4ad799906675b844807f",
      "1d255cf01c64433cb49342527d5d8963",
      "4b5e8d30a01c4da38ebc45bb6041294e",
      "5d8f920e0d5541c78c366986c2f20aab",
      "03394c2f33fb419980ed29e20b9e8cad",
      "57380aaa4d054e6f8582d399b237ee59",
      "0b75d56ed7154280a790f9c093c37f69",
      "0708a38c1712489994c5a37ab74109b2",
      "ca5ac75cde144d58b9fcf02b05a1b9fa",
      "b688091896634f449dc1d1b6cf496942",
      "c918cfc037284de49a98c9a9bb93c9a9",
      "94fc7fa5db3f4edd88dd77ebbfa9096f",
      "af7f3251dcd94e52b226289da8c08113",
      "9965b7008f2a424fafb4d3516db1cf42",
      "3a7050cff7ba4888ad3e26c35c10bcee",
      "974a8291aae843d08350a336e74c4b9c",
      "ca4ff2a4cd9f4b83a81971e2be5fa35b",
      "4ad8ce2a787f41649e8506cb10b1c5a9",
      "e14208e3fdda4b81a54d585014312dfe",
      "87ea656522384b138c39ac3c5f2859c1",
      "6c8450a3113c442f8fac2af6d1e92aac",
      "df42b79f2840494d9028f4d7930c2cd2",
      "b41a1c2672f24ee8981f52f943345778",
      "e349ef3db3c84003a87a66654a3f92b0",
      "b8c7762a53f349beb9d034a36547f3b3",
      "cbf7eb578f7d42eab16399e104fc5b59",
      "a150e0f252534497832e7de33abfe9d5"
     ]
    },
    "id": "8f92fae6",
    "outputId": "a08162cb-e45e-45cd-845b-7c878bfb7339",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe31a452c484d2ba4c618505d30375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d50994f29f47fca5c56c615c3bcd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765c2fb006f94428a271b8c34930bd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57380aaa4d054e6f8582d399b237ee59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4ff2a4cd9f4b83a81971e2be5fa35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "print(tokenizer_phi.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134f99c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e134f99c",
    "outputId": "d9e7c771-1e21-4777-f0ac-d6ad8ef2eab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
    "    {'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
    "    {'role': 'assistant', 'content': 'Buenos Aires.'}\n",
    "]\n",
    "\n",
    "formatted = tokenizer_phi.apply_chat_template(conversation=messages,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=False)\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5014cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a5014cf",
    "outputId": "a3021758-fd16-4b1e-dfe6-f4830c46a164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_input = tokenizer_phi.apply_chat_template(conversation=messages[:-1],\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "print(inference_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3a356",
   "metadata": {
    "id": "a7c3a356"
   },
   "source": [
    "#### 지원 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a40284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60a40284",
    "outputId": "d4cf02ef-1d17-453d-e417-b233b54563b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': List({'content': Value('string'), 'role': Value('string')})}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_ds = Dataset.from_list([{'messages': messages}])\n",
    "conversation_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd0626",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48fd0626",
    "outputId": "95a33b13-9134-473c-adf4-3b01a02d810f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORMAT_MAPPING['chatml'] == conversation_ds.features['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129a7b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0129a7b9",
    "outputId": "d15da89e-cec3-4f4e-9ea0-8476e80f0f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
    "\n",
    "print(formatting_func(conversation_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525cc9c",
   "metadata": {
    "id": "c525cc9c"
   },
   "source": [
    "```python\n",
    "# 대화 포맷을 위한 포맷팅 함수\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[messages_field][0], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[messages_field])):\n",
    "            output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n",
    "        return output_texts\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371de955",
   "metadata": {
    "id": "371de955"
   },
   "source": [
    "**중요 업데이트**: 안타깝게도 최근 버전의 `trl` 라이브러리는 (`prompt`와 `completion` 열을 필요로 하는) 지시 포맷을 더이상 지원하지 않습니다. 이로 인해 채팅 템플릿이 올바르게 적용되지 않습니다. 이 문제를 피하기 위해 대화 포맷을 사용하는 것이 좋습니다.\n",
    "\n",
    "만약 데이터셋이 지시 포맷으로 구성되어 있다면 다음에 나오는 `format_dataset()` 함수를 사용해 손쉽게 대화 포맷으로 바꿀 수 있습니다. 이 함수는 이전 버전의 `trl` 패키지를 참고하여 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "By3fhvwiRJNe",
   "metadata": {
    "id": "By3fhvwiRJNe"
   },
   "outputs": [],
   "source": [
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return output_texts\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(converted_sample, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c7c80",
   "metadata": {
    "id": "fb8c7c80"
   },
   "outputs": [],
   "source": [
    "batch_prompts_completions = {\n",
    "    'prompt': ['What is the capital of Argentina?',\n",
    "               'What is the capital of the United States?'],\n",
    "    'completion': ['Buenos Aires.',\n",
    "                    'Washington D.C.']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fa759",
   "metadata": {
    "id": "298fa759"
   },
   "outputs": [],
   "source": [
    "batch_messages = format_dataset(batch_prompts_completions)['messages']\n",
    "batch_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50481b56",
   "metadata": {
    "id": "50481b56"
   },
   "source": [
    "#### BYOFF (Bring Your Own Formatting Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c55159",
   "metadata": {
    "id": "b7c55159"
   },
   "outputs": [],
   "source": [
    "def byo_formatting_func1(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbffc17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125,
     "referenced_widgets": [
      "aadd0a642a464e3cacf62c13ee696dca",
      "1a663b41db5f47b7b7c63050f693865d",
      "b2b5dc7a0aa645d88a3f7feb178b8f0a",
      "b69e4486c6ba487fa496f340e2805ab1",
      "dca345ade8cd444999eeaccbe74f27a1",
      "6d506c6b4fa440c1bd9a35495835ba80",
      "4a49497fb594475f99377f76ca25fc96",
      "9af8f235e3fc4c30abe488c4d10b45df",
      "5a31350837284ef288a979e680f6a73a",
      "99252b7a9204438f9546c33346dfe223",
      "4373be8c555d430996d8687d4bb93c48"
     ]
    },
    "id": "efbffc17",
    "outputId": "a11a8396-b7af-41b4-f093-0747bbadd4a4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadd0a642a464e3cacf62c13ee696dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_msg = Dataset.from_dict({'messages': batch_messages})\n",
    "ds_msg.map(lambda v: tokenizer_phi(byo_formatting_func1(v)), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488d9c0",
   "metadata": {
    "id": "6488d9c0"
   },
   "outputs": [],
   "source": [
    "def byo_formatting_func2(examples):\n",
    "    response_template = '### Answer:'\n",
    "    text = f\"### Question: {examples['prompt']}\\n{response_template} {examples['completion']}\"\n",
    "    text += tokenizer_phi.eos_token\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f7c32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a1f7c32",
    "outputId": "ffe18c7e-f575-4cff-a1ab-59a93ab74b47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the capital of Argentina?\n",
      "### Answer: Buenos Aires.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "ds_prompt = Dataset.from_dict(batch_prompts_completions)\n",
    "print(byo_formatting_func2(ds_prompt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf14c94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264,
     "referenced_widgets": [
      "1248478e4b8745ff89dd9d16835118ed",
      "932db0a0e5924ef99ca4d2edc90b0e97",
      "fbfa091efbe54c699b8f89497b9487b5",
      "64c4348cf8d246fd8f68205ed0f44c9a",
      "74d2d82cc5484b00a5256daeba7bccc6",
      "b59fb6905d114256a8c1b97af86356cb",
      "23f465871cb74643b77edc65f5afcf7b",
      "72bd55d910894dd1957b26d259d79be5",
      "66883d8ef65149899c5cab02eef541a1",
      "f52a01ecf90147dbad66837fbc72b6ae",
      "1c10d69528834fbb8be48f75186d38e0"
     ]
    },
    "id": "5bf14c94",
    "outputId": "fcadf00e-e302-4118-c0ba-22ce999ae3a4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1248478e4b8745ff89dd9d16835118ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named input_ids expected length 2 but got length 44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1676773618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 이 코드는 예외를 일으킵니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyo_formatting_func2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3316\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0munprocessed_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0munprocessed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m                             \u001b[0mcheck_if_shard_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3687\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_original_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_original_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named input_ids expected length 2 but got length 44"
     ]
    }
   ],
   "source": [
    "# 이 코드는 예외를 일으킵니다.\n",
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func2(v)), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591c014",
   "metadata": {
    "id": "9591c014"
   },
   "outputs": [],
   "source": [
    "def byo_formatting_func3(examples):\n",
    "    output_texts = []\n",
    "    response_template = '### Answer:'\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"### Question: {examples['prompt'][i]}\\n {response_template} {examples['completion'][i]}\"\n",
    "        text += tokenizer_phi.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aeda6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125,
     "referenced_widgets": [
      "6858da7a24084556885fe806e16203a9",
      "5ba834bedef948199abe126d89cb1d7b",
      "e3893018731845deb0e172080ba806d6",
      "78586b7b2bb9421bb4bd88356fe4ef11",
      "03dbb751fb55457fb76df2438facad10",
      "8436ce485f9844d48590506d074d9f65",
      "1fc51f06fa5743508ccd84ff6bc7e946",
      "bba3195a79564b1fbdcaa29922545486",
      "70b13ffce9cc40489dbda42493ea1746",
      "d79f41f66d884a9297b9efab2ec471c6",
      "2595de4af82b4170aed61448ac43af0a"
     ]
    },
    "id": "d0aeda6d",
    "outputId": "2d0add79-ab2f-432b-94fc-3abdd2a42714"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6858da7a24084556885fe806e16203a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func3(v)), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b04e6a",
   "metadata": {
    "id": "c0b04e6a"
   },
   "source": [
    "#### BYOFD (Bring Your Own Formatted Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a04d4",
   "metadata": {
    "id": "1a6a04d4"
   },
   "outputs": [],
   "source": [
    "def byofd_formatting_func(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {'text': output_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772c71a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "referenced_widgets": [
      "ba4dfdd35464440e995b2107bda317cc",
      "10c2b91490ad4ac1a530bc4b5a08a8d7",
      "f6fccc1060bc46d2a6d6d94914849a0e",
      "cc105db2e3c7493bbeb0e37ad20990da",
      "5012f5772bca40d39ce4fd7cea64ed43",
      "9acbea45246a43909f3d50f19b4d5cdf",
      "dfbc8cb1b51843deb82aeb7e4d60d572",
      "cb4980100d8f435d822fdc0fd5f5be4d",
      "7027a8953afe4f9589d8918e71da9f51",
      "3ff55b6f1b334dc1babb3cb515d43f15",
      "c4f00207358c4062aa8322496c241665"
     ]
    },
    "id": "5772c71a",
    "outputId": "db07d600-0fca-4842-9b18-f293447d1af3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4dfdd35464440e995b2107bda317cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Column(['<|user|>\\nWhat is the capital of Argentina?<|end|>\\n<|assistant|>\\nBuenos Aires.<|end|>\\n<|endoftext|>', '<|user|>\\nWhat is the capital of the United States?<|end|>\\n<|assistant|>\\nWashington D.C.<|end|>\\n<|endoftext|>'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ds = ds_msg.map(byofd_formatting_func, batched=True)\n",
    "formatted_ds['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78305ca1",
   "metadata": {
    "id": "78305ca1"
   },
   "source": [
    "#### 결론\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/formatting_flow.png?raw=True)\n",
    "\n",
    "<center>그림 4.5 - 적절한 포맷팅 방법 선택하기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753ab0a",
   "metadata": {
    "id": "4753ab0a"
   },
   "source": [
    "### 토크나이저\n",
    "\n",
    "****\n",
    "**\"토크나이저\" 절의 요약**\n",
    "- 토크나이저의 어휘사전은 일반적으로 모델의 임베딩 층 크기보다 작습니다.\n",
    "  - 크기 차이는 임베딩 층의 크기를 바꾸지 않고도 새로운 토큰을 추가할 수 있는 빈 슬롯 때문입니다.\n",
    "  - 효율적인 메모리 할당을 위해 임베딩 층의 크기는 2의 거듭제곱의 배수(32, 64 등)인 경우가 많습니다.\n",
    "- `EOS` 토큰은 다른 것 말고 텍스트의 끝을 나타내는데만 사용해야 합니다.\n",
    "  - 패딩을 위해 `EOS` 토큰을 사용하면 토큰 생성이 끊임없이 계속되는 문제가 발생할 수 있습니다.\n",
    "- `PAD` 토큰을 정의하지 않는 경우가 많지만 여전히 이 토큰이 필요합니다.\n",
    "  - `EOS` 토큰을 `PAD` 토큰으로 할당하지 마세요.\n",
    "  - `UNK` 토큰이 정의되어 있다면 이를 `PAD` 토큰으로 할당해도 괜찮습니다.\n",
    "  - `UNK` 토큰이 정의되어 있지 않다면 `PAD` 토큰을 위해 새로운 특수 토큰을 만드세요.\n",
    "  - 주의: `PAD` 토큰을 정의하지 않은 채로 두면 많은 라이브러리에서 기본적으로 `EOS` 토큰을 패딩 토큰으로 할당합니다!!\n",
    "- 생성 모델의 경우 패딩은 왼쪽에 추가되어야 합니다.\n",
    "  - 오른쪽에 패딩하면 모델이 패딩 토큰의 시퀀스를 생성하도록 훈련됩니다.\n",
    "  - `SFTTrainer` 클래스에서 보고된 오버플로 문제 때문에 많은 튜토리얼에서 `tokenizer.padding_side='right'`를 사용합니다.\n",
    "    - 표준 패딩이 아니라 패킹이나 패킹 역할을 하는 콜레이터(\"패킹된 데이터셋\" 절 참조)를 사용하는 경우에만 괜찮습니다.\n",
    "- 새로운 특수 토큰을 만든다면 (임베딩 층의 빈 슬롯을 사용하기 때문에) 이론적으로 임베딩 층도 미세 튜닝해야 합니다.\n",
    "  - 실제로는 임베딩을 동결하더라도 모델이 잘 동작할 수 있습니다.\n",
    "  - (해당 임베딩을 훈련하지 않았으므로) 새로운 토큰 표현이 랜덤하지만, 모델의 훈련 가능한 다른 부분이 이런 임베딩을 있는 그대로 사용하는 방법을 학습할 수 있습니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29252e77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158,
     "referenced_widgets": [
      "a9826f55c1e640e7b39a975c1b920469",
      "2b9766dbd61744e98884b0d3ad738683",
      "a529f14c595e45f4a8eb7b2e950031b2",
      "df97b34227f341e5b7beab0d463953b8",
      "940d2a663f0941ba8b1abba32f046cb2",
      "16a5c1bb9b814298a1b553efe4e5f83b",
      "a67ef1d5cd67499fbfaf3fbab4f5957e",
      "323a1a71f81a4f67a00bedbac8428c0d",
      "2d6f803920ed43a0a11cee108150175a",
      "8aaed0aca5674f64a2993783c507e505",
      "b0d2e8f5e4ce481aab15e4b555947b88",
      "f8cde5406aab4992a0ee04b6a46b7d14",
      "dcfd4ca088fc4855911686d9b52990ac",
      "f2b65b1331b7469cb58aac25c84532d3",
      "77ff11608a4e4700838e4c21ed29eb84",
      "bd86e29469694c12bf3ff50cfc0f05c4",
      "ebbe25d2b761480c855d7ec24a4e4aef",
      "6c74c09d0ac44bf0a801cbbf4a545090",
      "3a0be436393f44f7b25a73789cf99400",
      "30138ce30feb4d869e41c4592ce99481",
      "6bf225e791b8452b9027b4acb55ba1bd",
      "44422f0f8f6e4bc1ba9e11f30ddc5895"
     ]
    },
    "id": "29252e77",
    "outputId": "20d88f90-7f40-438e-d229-837a0b1285b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9826f55c1e640e7b39a975c1b920469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cde5406aab4992a0ee04b6a46b7d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "config_phi = AutoConfig.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328bccd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4328bccd",
    "outputId": "772ea77f-c8f2-4917-c263-43318a5f132f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2803, 29915, 29879, 5993, 675, 445, 10541, 29991], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi(\"Let's tokenize this sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f1150",
   "metadata": {
    "id": "b82f1150"
   },
   "source": [
    "#### 어휘사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0766d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8a0766d",
    "outputId": "11e65e45-cc00-4098-fbb8-8634f7224596"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32011, 32064)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_phi), config_phi.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74a2f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e74a2f6",
    "outputId": "17ffcdb0-f427-4d2c-ce40-72e715362ec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001),\n",
       " ('<|endoftext|>', 32000)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e5b4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "163e5b4e",
    "outputId": "b6edb746-5dcc-44e7-aa08-bc5691d43a2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', 32000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.eos_token, tokenizer_phi.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7b370",
   "metadata": {
    "id": "c0d7b370"
   },
   "source": [
    "#### 특수 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1658f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a1658f5",
    "outputId": "84e96a26-4661-43c1-aef2-b7f5a64d1578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<|endoftext|>', '<unk>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d361cad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d361cad",
    "outputId": "4560245a-317d-4c7b-a0ae-747d5b9cae18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95090969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95090969",
    "outputId": "0b041c9f-4e45-4018-afdc-c7260de33477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.cls_token, tokenizer_phi.sep_token, tokenizer_phi.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65debf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f65debf1",
    "outputId": "bd2be85c-1e6a-408f-c28d-c91b2eda4211"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.add_special_tokens({'cls_token': '<cls>', 'sep_token': '<sep>', 'mask_token': '<mask>'})\n",
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc17b7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dc17b7d",
    "outputId": "c01411f6-730d-4fd2-9894-7e64733d7ab0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<mask>', 32013),\n",
       " ('<sep>', 32012),\n",
       " ('<cls>', 32011),\n",
       " ('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b3026",
   "metadata": {
    "id": "a33b3026"
   },
   "source": [
    "#### `EOS` 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add45042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "add45042",
    "outputId": "a4b2dabe-94d3-41e9-e028-c79b62ef96e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<unk>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token = tokenizer_phi.unk_token\n",
    "tokenizer_phi.pad_token_id = tokenizer_phi.unk_token_id\n",
    "\n",
    "tokenizer_phi.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cb4f4",
   "metadata": {
    "id": "167cb4f4"
   },
   "source": [
    "```python\n",
    "# 수정된 패딩 토큰을 위해 모델 설정을 업데이트합니다.\n",
    "if getattr(model, \"config\", None) is not None:\n",
    "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "if (getattr(model, \"generation_config\", None) s not None):\n",
    "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333bb99",
   "metadata": {
    "id": "3333bb99"
   },
   "source": [
    "#### `PAD` 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe01c37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfe01c37",
    "outputId": "588db535-36fd-494d-d1ee-31d7bbd015ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 'left')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token, tokenizer_phi.padding_side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d36e92",
   "metadata": {
    "id": "50d36e92"
   },
   "source": [
    "### 데이터 콜레이터\n",
    "\n",
    "****\n",
    "**\"데이터 콜레이터\" 절의 요약**\n",
    "- `SFTTrainer` 클래스(5장 참조)의 `data_collator` 매개변수를 지정할 수 있습니다.\n",
    "- `DataCollatorForLanguageModeling`가 `SFTTrainer` 클래스의 기본 콜레이터입니다.\n",
    "  - 자동으로 토큰 ID를 레이블로 복제합니다.\n",
    "  - 모델이 자동으로 처리하므로 레이블을 이동시키지 않습니다.\n",
    "  - 전체 텍스트(프롬프트와 완성)를 레이블에 포함시키므로 지시 미세 튜닝에 이상적입니다.\n",
    "- 지시 모델이나 채팅 모델을 추가적으로 미세 튜닝한다면 `DataCollatorForCompletionOnlyLM`을 사용해 모델의 응답(완성)으로만 모델을 훈련할 수 있습니다.\n",
    "  - 이 콜레이터도 토큰 ID를 레이블로 복제하지만 프롬프트 토큰에 해당하는 ID는 -100으로 마스킹합니다.\n",
    "  - 단일 상호작용(한 쌍의 프롬프트와 완성)에서는 응답 템플릿만으로 완성의 위치를 찾을 수 있습니다.\n",
    "  - 다중 상호작용(여러 쌍의 프롬프트와 완성)에서는 프롬프트 토큰을 찾고 마스킹하기 위해 지시 템플릿과 응답 템플릿이 모두 필요합니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeacb63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "517851d34b6d4f9fbaaa01f6590c52dd",
      "ed19fdaa1e7949fc989f5d61878c3cc7",
      "75c4cd3109414778a899cd1f596d1d40",
      "d36383443d4e409fac19f4e61ffe3787",
      "0b3fb7e9b140464698d8f6bfa782c977",
      "392f7169030e4f01bdabc39911587152",
      "7e6154cf825f40ba83c264b334c24c73",
      "736863c18b6a42ca80ae184c6a9715da",
      "7c05457d402b4ce6ae1f76356a5bad29",
      "7b274263e03c4341b49e18f802fd1f3f",
      "e5bea008ce794461979385f063513d0d",
      "7290daf091e1492d8bcf1fc5814755d5",
      "53adcd86641e4500a245c294cfbd29a9",
      "dbc9f6aacb6d4aa0a7d3d80cd5cd3c66",
      "4655ae999ea2439f8693ba4a77402014",
      "b7ae9275ac3243c0a535bc12158eb69c",
      "cda26f5e241c47dbafaf8f725a1677ae",
      "b2460875bb22447f8c9dc772dcc362cb",
      "645764b2f5de46a18043ead03ac94ad9",
      "4c22da67ddf740038abcd2a2fe0d5d5e",
      "1d86011d55c24dff9f621138980270c3",
      "2d56b1863f814439917ec9bf15a3f5cc",
      "75c3d5bc62d74a0dac62fa2c5f67d4ff",
      "3f20a7614864466fae22d36767fbb7c8",
      "226487f57cea4ad8a06d9ccd2c7b88ea",
      "476da92ee96f4c8288af9190049698f1",
      "f6991e9cee7c4eb6a484444bb06e4ea0",
      "a4be441febf1408192d5b918cf4a1868",
      "5de5288e7baa446ca548c122e39b3c94",
      "4cffaa9ab1e242c4a4f00d66fc9b8427",
      "4c627f4e5e624bb08a4f4dd2c17dba4c",
      "c1dacd70895a470a8efbda50fbf5e97b",
      "d5ee54a529984ab38e24be5d768ebc56"
     ]
    },
    "id": "8eeacb63",
    "outputId": "f99b0c82-08bc-4c72-c2fb-71dfcc79f81e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517851d34b6d4f9fbaaa01f6590c52dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7290daf091e1492d8bcf1fc5814755d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentences.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c3d5bc62d74a0dac62fa2c5f67d4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(720,\n",
       " {'prompt': 'The birch canoe slid on the smooth planks.',\n",
       "  'completion': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "# 프롬프트/완성 쌍을 대화 메시지로 변환합니다.\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841e37c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "referenced_widgets": [
      "c85bf5764a10439d965b0212dd568bc0",
      "daef86445eb947bebf51cf7ddd565ead",
      "ba5152c1aa7e46f6b2dd3eac17eb1ef6",
      "efc0c785f04140d3acf8bbc8c19372c3",
      "69c53b100aa7465b976ab548715063f6",
      "3422a952f33b4f26b654e975871c8804",
      "7f95b39859d74a028c35a28758b203e5",
      "d455483eab9e40269a9227fa12564290",
      "42bb86fc6eea4336b3857c778793f343",
      "45a240e2230847c88d9829c398220617",
      "8153f1d081a14a2ea25f4e926eecfd08"
     ]
    },
    "id": "9841e37c",
    "outputId": "da2a1abd-0aba-4f72-c335-6f598fd6f860"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85bf5764a10439d965b0212dd568bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "formatting_func = conversations_formatting_function(tokenizer_phi,\n",
    "                                                    messages_field='messages')\n",
    "dataset = dataset.map(lambda row: {'text': formatting_func(row)},\n",
    "                      batched=True, batch_size=32)\n",
    "sequences = dataset['text']\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e400ae2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "69833e0a9e8d41dabc3c99b97b2c029f",
      "f97fd0a8ceb74ec08a965fd02f3aacbd",
      "bf8243990cb5478f9cbc8c5d966e8356",
      "6dcce96461224a1084e3ef4a3f251698",
      "e5a122c55c824d25a720b708f164c35c",
      "27718162d3574c42a7b5a9a68478402c",
      "105f8d0da4b04d52aa4d8eb78babdbe1",
      "44f50777a2184b18b41f5bb52412264d",
      "4b520335a3c24a3c9358751be1227766",
      "ab509dc224e04f5d832e868cfdd88754",
      "6579aa5a3f6a4e14bb3ef61c3cb6db64"
     ]
    },
    "id": "2e400ae2",
    "outputId": "88e19efe-4941-46cf-8697-f1e3dd5d43f9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69833e0a9e8d41dabc3c99b97b2c029f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(lambda row: tokenizer_phi(row['text']))\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a276bd5",
   "metadata": {
    "id": "6a276bd5"
   },
   "source": [
    "#### `DataCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcce437",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fcce437",
    "outputId": "c562c898-ed80-479a-8db9-b9541f429d7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_collator = DataCollatorWithPadding(tokenizer_phi)\n",
    "pad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=pad_collator)\n",
    "pad_batch = next(iter(pad_dloader))\n",
    "pad_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a36a82",
   "metadata": {
    "id": "07a36a82"
   },
   "source": [
    "#### 레이블은 어디 있나요?\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/shift_labels.png?raw=True)\n",
    "\n",
    "<center>그림 4.6 입력과 한 위치 이동한 레이블</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606662ee",
   "metadata": {
    "id": "606662ee"
   },
   "source": [
    "#### `DataCollatorForLanguageModeling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22713c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b22713c7",
    "outputId": "645cd818-2f3b-4d82-81f3-7ab648f86b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_collator = DataCollatorForLanguageModeling(tokenizer_phi, mlm=False)\n",
    "lm_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=lm_collator)\n",
    "lm_batch = next(iter(lm_dloader))\n",
    "lm_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ef2f1",
   "metadata": {
    "id": "2b3ef2f1"
   },
   "source": [
    "#### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c58443",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9c58443",
    "outputId": "f7a32c21-5d59-4e66-8838-102b1270f92d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>' # token id 32001\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebcd7dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "1ebcd7dd",
    "outputId": "2b211cf5-f753-4c0e-f1fa-17dbd5d083e6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels'][0]\n",
    "valid_tokens = (labels >= 0)\n",
    "tokenizer_phi.decode(labels[valid_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b7d1d",
   "metadata": {
    "id": "281b7d1d"
   },
   "source": [
    "##### 다중 상호작용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58067e2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f8302da183124ed49597c88e3669e7c2",
      "30471d0c535b405d87937d068b1181a7",
      "40c37bd4806c40f6bbb2d01dc322dc15",
      "c712bb811cd8462d8d5a8cb54dc3a143",
      "6527e917b39247f9a187115f728882fc",
      "6163abf3c23f46b1ad34515a9e00980e",
      "44946de45841480abe0a1e64565ae527",
      "9dad2e600ae24212a88f63c1b478a765",
      "e49c91c3f9ca4c25ad9679649c565376",
      "1af7eeb1411d4266b2f8bb1d0e375f3b",
      "774c99ffb331405aa41c3e0912b5db96"
     ]
    },
    "id": "58067e2e",
    "outputId": "64e1b9e3-f15c-4040-e322-61e30e1044ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8302da183124ed49597c88e3669e7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_chat = \"\"\"<|user|>Hello\n",
    "<|assistant|>How are you?\n",
    "<|user|>I'm fine! You?\n",
    "<|assistant|>I'm fine too!\n",
    "<|endoftext|>\"\"\"\n",
    "\n",
    "dummy_ds = Dataset.from_dict({'text': [dummy_chat]})\n",
    "dummy_ds = dummy_ds.map(lambda row: tokenizer_phi(row['text'])).select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408873c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7408873c",
    "outputId": "03846f34-8ba1-4e22-8d0e-38402e3911ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc8194",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "18bc8194",
    "outputId": "237c8ede-42d1-4e23-d2a0-37583b30c7e8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a80c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c1a80c5",
    "outputId": "36843c3b-0d9e-48d2-9795-715af3501b38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  1128,   526,   366, 29973,    13,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_template = '<|user|>'\n",
    "response_template = '<|assistant|>'\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
    "                                                      response_template=response_template,\n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052edea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "a052edea",
    "outputId": "061f996d-911d-4ab5-bee8-02b756d75d0f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"How are you?\\n I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203234de",
   "metadata": {
    "id": "203234de"
   },
   "source": [
    "#### 레이블 이동\n",
    "\n",
    "```python\n",
    "if labels is not None:\n",
    "    # 모델 병렬화를 위해 레이블을 올바른 장치로 이동시킵니다.\n",
    "    labels = labels.to(lm_logits.device)\n",
    "    # 다음 토큰 예측을 수행하므로 예측 점수와 입력 아이디를 하나씩 이동시킵니다.\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742073f",
   "metadata": {
    "id": "f742073f"
   },
   "source": [
    "### 패킹된 데이터셋\n",
    "\n",
    "****\n",
    "**\"패킹된 데이터셋\" 절의 요약**\n",
    "- 패킹은 시퀀스를 연결하여 동일한 크기의 청크로 나눕니다.\n",
    "  - 패딩 토큰이 사용되지 않습니다.\n",
    "  - 각 청크의 길이는 모델의 최대 시퀀스 길이를 넘어서는 안됩니다.\n",
    "- 패킹은 기본적으로 `SFTTrainer`에서 지원합니다.\n",
    "  - `packing` 매개변수를 `True`로 지정합니다.\n",
    "  - `pack_dataset()` 함수를 사용하여 패킹을 처리합니다.\n",
    "  - `packing_strategy`를 `wrapped`로 지정하여 원본 패킹 동작을 근사할 수 있습니다.\n",
    "  - 기본적으로 패킹과 콜레이터를 동시에 사용할 수 없습니다.\n",
    "- 일부 콜레이터는 효과적으로 시퀀스를 패킹할 수 있습니다.\n",
    "  - 이 경우 `packing` 매개변수를 `False`로 지정해야 하며 콜레이터가 패킹을 수행합니다.\n",
    "  - `DataCollatorWithFlattening`는 `DataCollatorForLanguageModeling`의 패킹 버전입니다.\n",
    "  - `DataCollatorForCompletionOnlyLM`에는 완성 전용 콜레이터가 패킹 같은 기능을 수행하도록 만드는 새로운 매개변수(`padding_free`)가 있습니다.\n",
    "  - 특정 모델(예를 들면, Llama, Phi, Mistral, Gemma, OLMo 등)은 플래시 어텐션 2로 이런 콜레이터를 지원합니다.\n",
    "    - 이런 모델은 `position_ids`를 사용하여 패킹된 원본 시퀀스 사이의 경계를 표시합니다.\n",
    "****\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packed_seq.png?raw=True)\n",
    "\n",
    "<center>그림 4.7 패킹된 시퀀스</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7519b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad7519b0",
    "outputId": "b57840dd-d755-4aa2-a559-6f80a4ecb54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['text']\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a53a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125,
     "referenced_widgets": [
      "db893aaf342e46af9a407b428bc2b596",
      "e5c76a0e7a8643c9afec2b433f5115eb",
      "33b6046b42574c8895704ddf45724308",
      "566ae292cda04ba380b034b8182960d7",
      "7d814eab829c4a84b0d072022ba12e3a",
      "a0eb1eee7e084d1f80752680dd960ae8",
      "4bec2f46ff65499db0838267d2d4d33b",
      "9696d86ed4354e5b9abccc56ebe26c98",
      "82c5a4b794064090a457da63753e5ab1",
      "2dcb229f590941dcb4ccd2a91d8e3489",
      "2f33e6262a7245f1bb0bbf5069cb4cc6"
     ]
    },
    "id": "bc4a53a8",
    "outputId": "5dd09afc-4f23-4c34-d3d8-5553ea55d8f1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db893aaf342e46af9a407b428bc2b596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 351\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_dataset = pack_dataset(tokenized_dataset, seq_length=64,\n",
    "                              strategy='wrapped')\n",
    "packed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4e10a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "00d4e10a",
    "outputId": "713e4843-4997-4472-c8e6-55d098a83052"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|user|> The birch canoe slid on the smooth planks.<|end|><|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|><|endoftext|><|user|> Glue the sheet to the dark blue background.<|end|><|assistant|> Glue the sheet to the dark blue background, you must'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = packed_dataset['input_ids']\n",
    "tokenizer_phi.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad3c82",
   "metadata": {
    "id": "21ad3c82"
   },
   "source": [
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packing_flow.png?raw=True)\n",
    "\n",
    "<center>그림 4.8 올바른 데이터 설정 선택하기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5af2d",
   "metadata": {
    "id": "79f5af2d"
   },
   "source": [
    "#### 패킹을 위한 콜레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8df4a",
   "metadata": {
    "id": "9ad8df4a"
   },
   "source": [
    "##### `DataCollatorWithFlattening`\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/collator_flat.png?raw=True)\n",
    "\n",
    "<center>그림 4.9 패킹 유사 콜레이터</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc8432",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bfc8432",
    "outputId": "0070d63f-2313-4ef5-d1bc-eb3a50504eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'labels': tensor([[ -100,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "          36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_collator = DataCollatorWithFlattening()\n",
    "flat_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=flat_collator)\n",
    "flat_batch = next(iter(flat_dloader))\n",
    "flat_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb7453",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ecb7453",
    "outputId": "2a73c9ec-97e3-482d-acbf-c65839119f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 66]), tensor(38))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_batch['input_ids'].shape, flat_batch['position_ids'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81fdd2",
   "metadata": {
    "id": "af81fdd2"
   },
   "source": [
    "##### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900880b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f900880b",
    "outputId": "53179289-63d5-4200-9320-5ba18dda3c32",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "           434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "         32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]]), 'cu_seq_lens_q': tensor([[ 0, 38, 66]], dtype=torch.int32), 'cu_seq_lens_k': tensor([[ 0, 38, 66]], dtype=torch.int32), 'max_length_k': tensor([38]), 'max_length_q': tensor([38])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>'\n",
    "completion_nopad_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                                            tokenizer=tokenizer_phi,\n",
    "                                                            padding_free=True)\n",
    "completion_nopad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_nopad_collator)\n",
    "completion_nopad_batch = next(iter(completion_nopad_dloader))\n",
    "completion_nopad_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e818",
   "metadata": {
    "id": "aed9e818"
   },
   "source": [
    "### 고급 방법: BYOT (Bring Your Own Template)\n",
    "\n",
    "****\n",
    "**\"고급 방법: BYOT\" 절의 요약**\n",
    "\n",
    "- 모든 템플릿은 응답 템플릿을 정의해야 하며, 이상적으로는 EOS 토큰으로 끝나야 합니다.\n",
    "- 토크나이저의 `EOS`, `PAD`, `UNK` 토큰을 다시 확인하세요.\n",
    "  - `EOS` 토큰은 `PAD` 및 `UNK` 토큰과 달라야 합니다.\n",
    "  - `PAD` 및 `UNK` 토큰은 같을 수 있습니다.\n",
    "- 꼭 필요할 때 임베딩 층의 크기를 바꿉니다(\"빈 슬롯\"이 모두 사용된 경우).\n",
    "  - 모델의 `resize_token_embeddings()`를 호출할 때, `pad_to_multiple_of` 매개변수를 사용해 크기가 2의 거듭제곱의 배수로 유지되도록 합니다.\n",
    "- 진자 템플릿을 직접 만들고 싶지 않다면 ChatML과 같은 기본 템플릿을 사용할 수 있습니다. `trl` 패키지의 `setup_chat_format()` 함수를 사용할 수 있지만 몇 가지 단점이 있습니다.\n",
    "  - `EOS` 토큰을 `PAD` 토큰에 할당합니다(나중에 수동으로 수정해야 합니다).\n",
    "  - 더 짧게 만들기 위해서라도 모델의 임베딩 층 크기를 기본적으로 조정합니다(적절한 `resize_to_multiple_of`를 선택하면 크기 변경을 피할 수 있습니다).\n",
    "- 토크나이저를 위해 진자 템플릿을 만드는 대신 포맷팅 함수를 사용하여 사용자 정의 템플릿을 정의하고 적용할 수 있습니다.\n",
    "  - `SFTTrainer` 클래스(5장 참조)에서 `formatting_func`을 지정하면 토크나이저가 채팅 템플릿을 가지고 있을 필요가 없습니다.\n",
    "- 응답 템플릿을 신중하게 선택하세요.\n",
    "  - 일반 단어(예: \"## Answer:\")를 사용하면 문제가 발생할 수 있습니다. 일부 토크나이저는 문맥에 의존적이어서 응답 템플릿을 여러 토큰으로 분할할 수 있기 때문입니다.\n",
    "  - 응답 템플릿을 위해 추가적인 특수 토큰을 만드는 것이 더 안전합니다. 이렇게 하면 응답 템플릿이 단일 토큰으로 인코딩되기 때문입니다.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23603681",
   "metadata": {
    "id": "23603681"
   },
   "source": [
    "#### 채팅 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccd450",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "46eaf4bda6ed41a5a1b2f25ca0a36625",
      "c58e89a2c3f04a64a356b9a1922abd6f",
      "8dc19201caec4d4d9bfc28d99089a7cf",
      "1840f64fdc6d4c3eb0ca3d771c27c2a0",
      "25b1c04332b44713b2944bf44d7f0dec",
      "af172dfe37a843dcaa4a0a81782e7c20",
      "1eac8151867344cba88be7dc56d47abe",
      "8ffd75af2da94bbdb55d3354067a5430",
      "8ee358d5c2604fdf9fd7225414c095a1",
      "711660a390c847ae9b94ade1779b3ac4",
      "8dcce805ef46449b96607dfe38da01f9",
      "530a2beeba744aaeafc377a81d26cb7d",
      "68a6ae01e9c34bd0a48ce83affc2fd01",
      "842ce0fa5ecc48a1ae4f948a942d60a7",
      "8d0b443f35b14852830d0f5401141440",
      "c4051fedf88148cbaf37eb4467e6ca94",
      "3280456e40ad4d63ba40e4f97c7a12ce",
      "28c72b00c3b546b5a651b042cafb0429",
      "d90edaa0acc24c10a501d7c192c521c6",
      "a462a96760b04cac8679618eb8b93448",
      "117c3400b64742ea88c481e152b8d836",
      "88946aac3c0a4704b97c318301884b88",
      "175398be964d41d18d236a541e0fd02f",
      "54c32731784042668727f38a46d99971",
      "db430f7830e145ef842910ccbda27f5f",
      "6038552150004f88b3e9fb93ee1dfc95",
      "78eeefc0effa4241b09c4112292bf320",
      "79ea15c7511c4c59a6347b99fe57c382",
      "b81817afc4164c4da3a9b8a43b95fa33",
      "7c799fc57da34656aa2d19d682aeb746",
      "082f27b6fde84850910a1e1a427ffe9b",
      "52da2eceeab8408bb726058a913b9e93",
      "40d212e3c9bb4cb49148c88d61be706e",
      "6eb8a4e9507b49949a842c628a16d457",
      "6221143c17b245c4acc65dfa1b9af5e0",
      "8d832c0c838947b1a935bf62ede7ff66",
      "9e70b02dc33341068113218f227bfeae",
      "bcfdcdaedb1b40648e45b3c276fde68b",
      "26b78dcf26194dd8a733f7c68d35bfdd",
      "ac3a772b35684f65828d954741fa2fd2",
      "687a2792b58c4fa6baaf4aeb158f0fa7",
      "3b36d154d22442d58382e5657897a861",
      "aebfe8794ec04863b898b23957936ef0",
      "22361968195d4bdd846ebf6fdf239dd1"
     ]
    },
    "id": "30ccd450",
    "outputId": "fee67da8-ea2a-468f-8ee0-17b79af56833"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eaf4bda6ed41a5a1b2f25ca0a36625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530a2beeba744aaeafc377a81d26cb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175398be964d41d18d236a541e0fd02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb8a4e9507b49949a842c628a16d457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "print(tokenizer_opt.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c44376",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c44376",
    "outputId": "fda270e7-1f3e-45ca-e052-426ac14b0dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '</s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '</s>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a1505",
   "metadata": {
    "id": "dd4a1505"
   },
   "source": [
    "**ChatML**\n",
    "****\n",
    "\n",
    "[ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)은 채팅 마크업 언어(Chat Markup Language)의 약어로 오픈AI에서 개발했습니다:\n",
    "\n",
    "_____\n",
    "\"_전통적으로 GPT 모델은 구조화되지 않은 텍스트를 사용했습니다. ChatGPT 모델은 대신에 채팅 마크업 언어(줄여서 ChatML)이라는 구조적인 포맷을 기대합니다. ChatML 문서는 일련의 메시지로 구성됩니다._\"\n",
    "_____\n",
    "\n",
    "앞서 소개한 대화 포맷과 비슷하게 각 메시지는 참여자의 역할과 그에 해당하는 콘텐츠로 구성됩니다. ChatML의 진자 템플릿은 다음과 같습니다:\n",
    "\n",
    "```\n",
    "{% for message in messages %}\n",
    "  {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n",
    "{% endfor %}\n",
    "```\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e439158",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e439158",
    "outputId": "4cb622f3-5f54-4f20-98f6-0d10a7954065"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70086b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d70086b2",
    "outputId": "32ad44a8-a718-4b83-c316-6f65c70c630d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50272"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91f663",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b91f663",
    "outputId": "79bb805f-6e44-46e4-a778-dba5ded55a96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "pad_to_multiple_of = get_multiple_of(model_opt.config.vocab_size)\n",
    "pad_to_multiple_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff379c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aff379c",
    "outputId": "2ae27b89-5d45-45d0-c745-c474b496c5a4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.resize_token_embeddings(len(tokenizer_opt),\n",
    "                                  pad_to_multiple_of=pad_to_multiple_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9973e",
   "metadata": {
    "id": "82c9973e"
   },
   "outputs": [],
   "source": [
    "def modify_tokenizer(tokenizer,\n",
    "                     alternative_bos_token='<|im_start|>',\n",
    "                     alternative_unk_token='<unk>',\n",
    "                     special_tokens=None,\n",
    "                     tokens=None):\n",
    "    eos_token, bos_token = tokenizer.eos_token, tokenizer.bos_token\n",
    "    pad_token, unk_token = tokenizer.pad_token, tokenizer.unk_token\n",
    "\n",
    "    # BOS 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    if bos_token == eos_token:\n",
    "        bos_token = alternative_bos_token\n",
    "\n",
    "    # UNK 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    if unk_token == eos_token:\n",
    "        unk_token = alternative_unk_token\n",
    "\n",
    "    # PAD 토큰은 EOS 토큰과 달라야 합니다.\n",
    "    # 하지만 UNK 토큰과는 같을 수 있습니다.\n",
    "    if pad_token == eos_token:\n",
    "        pad_token = unk_token\n",
    "\n",
    "    assert bos_token != eos_token, \"다른 BOS 토큰을 선택하세요.\"\n",
    "    assert unk_token != eos_token, \"다른 UNK 토큰을 선택하세요.\"\n",
    "\n",
    "    # BOS, PAD, UNK 토큰을 위한 딕셔너리를 만듭니다.\n",
    "    # EOS 토큰은 원래 정의된 대로 유지합니다.\n",
    "    special_tokens_dict = {'bos_token': bos_token,\n",
    "                           'pad_token': pad_token,\n",
    "                           'unk_token': unk_token}\n",
    "\n",
    "    # 새로운 특수 토큰을 추가합니다.\n",
    "    if special_tokens is not None:\n",
    "        if isinstance(special_tokens, list):\n",
    "            special_tokens_dict.update({'additional_special_tokens': special_tokens})\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # 새로운 일반 토큰을 추가합니다.\n",
    "    if tokens is not None:\n",
    "        if isinstance(tokens, list):\n",
    "            tokenizer.add_tokens(tokens)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377ffe7",
   "metadata": {
    "id": "9377ffe7"
   },
   "outputs": [],
   "source": [
    "def jinja_template(tokenizer):\n",
    "    return (\"{% for message in messages %}\"\n",
    "            f\"{{{{'{tokenizer.bos_token}' + message['role'] + '\\n' + message['content'] + '{tokenizer.eos_token}' + '\\n'}}}}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            f\"{{{{ '{tokenizer.bos_token}assistant\\n' }}}}\"\n",
    "            \"{% endif %}\")\n",
    "\n",
    "def add_template(tokenizer, chat_template=None):\n",
    "    # 채팅 템플릿이 주어지지 않으면 BOS와 EOS 토큰을 사용해\n",
    "    # 채팅 템플릿을 만듭니다.\n",
    "    if chat_template is None:\n",
    "        chat_template = jinja_template(tokenizer)\n",
    "\n",
    "    # 채팅 템플릿을 토크나이저에 할당합니다.\n",
    "    tokenizer.chat_template = chat_template\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869f622",
   "metadata": {
    "id": "6869f622"
   },
   "outputs": [],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "def modify_model(model, tokenizer):\n",
    "    # 새로운 토크나이저의 크기가 어휘사전 크기를 초과한다면\n",
    "    # 같은 배수가 되도록 유지하면서 크기를 바꿉니다.\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        pad_to_multiple_of = get_multiple_of(model.vocab_size)\n",
    "        model.resize_token_embeddings(len(tokenizer),\n",
    "                                      pad_to_multiple_of=pad_to_multiple_of)\n",
    "\n",
    "    # 모델 설정의 토큰 ID를 업데이트합니다.\n",
    "    if getattr(model, \"config\", None) is not None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3459c8",
   "metadata": {
    "id": "1b3459c8"
   },
   "outputs": [],
   "source": [
    "tokenizer_opt = modify_tokenizer(tokenizer_opt)\n",
    "tokenizer_opt = add_template(tokenizer_opt)\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bf5da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b7bf5da",
    "outputId": "05bf9900-6fec-49df-9d19-9bf62a46ecef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|im_start|>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ae3a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d96ae3a5",
    "outputId": "1ca648d5-9498-4f45-caf3-e8488f344800"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50266"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2e433",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "04a2e433",
    "outputId": "bb27ecfb-44d8-4a25-de78-6c1289e37897"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|im_start|>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72d660",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec72d660",
    "outputId": "e2c2b1e7-a35f-48ee-e84d-d6969079541e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e77014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9e77014",
    "outputId": "e1406870-4aad-42aa-e42a-92d965ac4515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '</s>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_opt.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e5d83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "197e5d83",
    "outputId": "7c92afe7-0834-46b2-b1be-13257b1a9b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of Argentina?</s>\n",
      "<|im_start|>assistant\n",
      "Buenos Aires.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = ds_msg['messages'][0]\n",
    "print(tokenizer_opt.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea9914",
   "metadata": {
    "id": "7aea9914"
   },
   "source": [
    "#### 사용자 정의 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41741481",
   "metadata": {
    "id": "41741481"
   },
   "outputs": [],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "response_template = '##[YODA]##>'\n",
    "tokenizer_opt = modify_tokenizer(tokenizer_opt, special_tokens=[response_template])\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab49d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "2ab49d36",
    "outputId": "03f198fc-719f-46ec-bebe-f11fadf14de5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>formatting_func_builder.&lt;locals&gt;.formatting_func</b><br/>def formatting_func(examples, add_generation_prompt=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-955622997.py</a>&lt;no docstring&gt;</pre></div>"
      ],
      "text/plain": [
       "<function __main__.formatting_func_builder.<locals>.formatting_func(examples, add_generation_prompt=False)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func_builder(response_template):\n",
    "    def formatting_func(examples, add_generation_prompt=False):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples['prompt'])):\n",
    "            text = f\"{examples['prompt'][i]}\"\n",
    "            try:\n",
    "                text += f\" {response_template} {examples['completion'][i]}{tokenizer_opt.eos_token}\"\n",
    "            except KeyError:\n",
    "                if add_generation_prompt:\n",
    "                    text += f\" {response_template} \"\n",
    "            output_texts.append(text)\n",
    "        return output_texts\n",
    "    return formatting_func\n",
    "\n",
    "yoda_formatting_func = formatting_func_builder(response_template)\n",
    "yoda_formatting_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5d7fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "46d5d7fa",
    "outputId": "4622a072-9848-43a7-8b55-f00e5f2b2039"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The birch canoe slid on the smooth planks. ##[YODA]##> On the smooth planks, the birch canoe slid. Yes, hrrrm.</s>'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "\n",
    "formatted_seqs = yoda_formatting_func(dataset)\n",
    "formatted_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c968b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b45c968b",
    "outputId": "4c07cadc-c7b7-4240-dbf7-4d1386c23d19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 133, 23629, 611, 31728, 13763, 15, 5, 6921, 563, 2258, 4, 1437, 50266, 374, 5, 6921, 563, 2258, 6, 5, 23629, 611, 31728, 13763, 4, 3216, 6, 1368, 28015, 22900, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt(formatted_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba69d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "4fba69d1",
    "outputId": "87099a9d-0ee6-4921-f57d-a65003c4a949"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'##[YODA]##>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f5c0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "482f5c0e",
    "outputId": "e613e89f-44ff-43db-f07e-684b59948c5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Force is strong in you. ##[YODA]##> ', 'I am your father! ##[YODA]##> ']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_formatting_func({'prompt': ['The Force is strong in you.',\n",
    "                                 'I am your father!']},\n",
    "                     add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934abad6",
   "metadata": {
    "id": "934abad6"
   },
   "source": [
    "#### 특수 토큰 만세!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fV9GeS8C5DVs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "8ef7028007ac4289a9437c11d5a6c7fe",
      "d6b601b496424bafb3a48624b6c7670e",
      "a11922212f3248f982795cb318630927",
      "7c005caa13c94d63ba8e7d5f949d63bb",
      "ec23760701f649fa8e8806281f0dc6ab",
      "cf560b9dce20401eb17ac30f543b96b5",
      "5c6c33ceed174118a9ec2704fb3c7644",
      "6c98cdbe869a4779b436e9df6558685f",
      "f05b102c8c4e4b44aadf43c2aa0d7a81",
      "032a79d68ec0425b8264e932d6da8c22",
      "555d1f523d334273ab36318a2e427648",
      "704bc494e8c4427a92c663f947fa0720",
      "a74fddd7527d417fbb63306c63799607",
      "a3a557d2c71f4b18ac523f8ee61ea57f",
      "d5b71d79b415447f81746fb65283b56c",
      "b941e6fc97fb488b870f859132d9f1c6",
      "b8f458c39e68446086262d1f3ef57dea",
      "b381206fd6f34beabfde3e9697a5e8cc",
      "988db8be14384092be03e0d15b2881b5",
      "47d19ccccbb7461a888750d8751447f5"
     ]
    },
    "id": "fV9GeS8C5DVs",
    "outputId": "dde4c6b5-720b-40db-8852-343ccb02f1ca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef7028007ac4289a9437c11d5a6c7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3646e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "be227e570b6f4c869392efd26b9bffea",
      "e368f050970f40618a1a317e8633fa6d",
      "96bfb55bc004418ab03638f681f40f7b",
      "4160f4fe48444325810b510d4582c610",
      "0f9327e1d1af4545b96f762cec6e9761",
      "04e20d7c203a4f73be6245ad8799793c",
      "de6ce36429a44feb997556eed9ad4366",
      "58c32f573f0c444a8c54700f0bc21ccf",
      "496f5c8c84c74ff2bc88b376ca2e7258",
      "22a133c3136d4ceea2291569106e72f1",
      "6f5aae69d11644ae9f6ed2d358cdb8da",
      "231f16b62abd4cd5b140a273a5ef350e",
      "ada16ee726c84fc482b31aa70595141d",
      "bf4940e1b996479e976c80f9b840be69",
      "4d7a8676c44344458308f75247555cca",
      "731cee3897104da99a0c5e1751129186",
      "4da946eaab894d8983f823436f7c1dd4",
      "4534560611a048509adb36971cdb397f",
      "51c5b4712b834404ae1a17fbeda53bc1",
      "2e5c0193464d4cd5907f841a703a47c2",
      "0991560afaf54e4fa1ca155fc013e70b",
      "b3929ef49cc94c8ebec0e34d1b46b9aa",
      "bc94ca4214314f5986a24dc86ba7975e",
      "69e98a50d96d4c669e0e972702ebc05e",
      "97128d86e67a4648a2f08b8737bd09e8",
      "90596b2a49e54803ad96534e28067cf7",
      "b4b1cb7e28214411a72614eb5842a619",
      "d78dfc69445b4d00bfa4f9068e462af4",
      "c135ab2ba52a4b8f87be858cee31ee8e",
      "1870f99e4f124fff9f4e5ae1631983a9",
      "d22d5ce03b3246f29e17e32ecc4941c9",
      "8d131c55c61e45dba1e3c4a062ee9683",
      "de527174ae044c72b25c768a28cd08f2",
      "c657c53422e14183a9852b63ffc6ad65",
      "1894508ce33c429ab53f12e48da60dd1",
      "0119fb9d71d34c8d9c280fd3b56e4023",
      "28011cc2e956475bb62700b7144e95d8",
      "2db5e903e87e40feb0c2c4e7a70ad86b",
      "56a64f6265b446c2a5e23e96f12ebbd2",
      "29fe091da92d427a8978ece24d619b9d",
      "5eec792c49b54bea92acfa1e278616cd",
      "3536741524c442e78046964eca076334",
      "c804cbcad2b74b84b95172f609f34504",
      "19bd661bd3b8425e95682d2baa8e214a"
     ]
    },
    "id": "1db3646e",
    "outputId": "f5731cec-3e1b-475b-a0a6-e91c679f21ec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be227e570b6f4c869392efd26b9bffea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231f16b62abd4cd5b140a273a5ef350e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc94ca4214314f5986a24dc86ba7975e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c657c53422e14183a9852b63ffc6ad65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer_llama.pad_token = tokenizer_llama.unk_token\n",
    "tokenizer_llama.pad_token_id = tokenizer_llama.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcec4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cabcec4b",
    "outputId": "e7ba77d4-8e3c-49cd-b3ba-db9a6d345c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User: Hello\n",
      "\n",
      "### Assistant: Hi, how can I help you?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327b52c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b327b52c",
    "outputId": "902d666c-06b3-4867-aaa2-51ae0a8c83ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer_llama.tokenize(prompt, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(prompt, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))[6:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c09206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56c09206",
    "outputId": "4f5afd1b-9cff-4b18-ee23-7f8ee01079c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2683ec6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "b222171fb3564c61b73d6c95f7cf93f8",
      "29f05af524da448dbe98385af1bfb9dc",
      "e68dd46ddaba4d6fac210e71de06c75e",
      "ed50d1f3ac7447c78b43f93ff4ad323f",
      "f1fbe220cb5540ef85c7ed4e51ad4547",
      "1d7a5b430fe241ffb726dd83eb1d8e6c",
      "4b1ee04b46ae47ce8d41f298fb616245",
      "4f914993d6aa4ae0b31dccbf6c148334",
      "1e8a220662e440d189105649a5a1bef2",
      "6b97fb1d5ed546c6ab3f796f618b8464",
      "d83d4caa7f7547fc9c9fee95e6f0f21f"
     ]
    },
    "id": "e2683ec6",
    "outputId": "539c22c2-2d5c-4103-d3c8-1a3f1febf82f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222171fb3564c61b73d6c95f7cf93f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/utils.py:153: UserWarning: Could not find response key `### Assistant:` in the following instance: <s> ### User: Hello\n",
      "\n",
      "### Assistant: Hi, how can I help you?. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13,  2277, 29937,  4007,\n",
       "         22137, 29901,  6324, 29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_ds = Dataset.from_dict({'text': [prompt]})\n",
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])\n",
    "\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "bad_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "bad_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=bad_collator)\n",
    "bad_batch = next(iter(bad_dloader))\n",
    "bad_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f5914",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "817f5914",
    "outputId": "0d148340-de2f-42e8-d646-968bae5e531a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁', 29871),\n",
       " ('<0x0A>', 13),\n",
       " ('##', 2277),\n",
       " ('#', 29937),\n",
       " ('▁Ass', 4007),\n",
       " ('istant', 22137),\n",
       " (':', 29901)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_response_template = \"\\n### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(modified_response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(modified_response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402905c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402905c0",
    "outputId": "0e053a24-0439-4c8f-fdfe-b92aa9901e20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13,  2277, 29937,  4007,\n",
       "         22137, 29901,  6324, 29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  6324, 29892,   920,   508,   306,  1371,   366, 29973]])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_token_ids = token_ids[2:]\n",
    "fixed_collator = DataCollatorForCompletionOnlyLM(fixed_token_ids, tokenizer=tokenizer_llama)\n",
    "fixed_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=fixed_collator)\n",
    "fixed_batch = next(iter(fixed_dloader))\n",
    "fixed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d77bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86d77bdb",
    "outputId": "a9164cc9-136f-40b5-f9ce-bc4f4cd06bb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokenizer_llama.add_special_tokens({'additional_special_tokens': [response_template]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6c648",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0190c1fa2b71445cbee14e91b6cd0391",
      "4e9fb6d9615f409eba7894c9511dd9cb",
      "3700f40e155043468753176d93d14668",
      "2aba32a9883f4e26b965bf7afd3c3c0d",
      "8235f545ec9a45d490374f88925eca4d",
      "f6d1e4b9d0c141039a95bbb5de933a34",
      "06f0ea99af5141479fdb42b5f40381d5",
      "1f772f70c6ff40d89722e52021dbab64",
      "dbcac06dd0f44518ac7c3ee14f0d5809",
      "20c5901dd9db44b3a7549c84871e5722",
      "d59efe1975e544d88997d27f3b776379"
     ]
    },
    "id": "cfb6c648",
    "outputId": "41166472-2998-40aa-ab6c-e59c3a8c516d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0190c1fa2b71445cbee14e91b6cd0391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07db988",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f07db988",
    "outputId": "8a1b5d1d-3d66-403b-89ae-65bd352f0bda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835,  4911, 29901, 15043,    13,    13, 32000, 29871,  6324,\n",
       "         29892,   920,   508,   306,  1371,   366, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 29871,  6324,\n",
       "         29892,   920,   508,   306,  1371,   366, 29973]])}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "special_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=special_collator)\n",
    "special_batch = next(iter(special_dloader))\n",
    "special_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c75e1",
   "metadata": {
    "id": "486c75e1"
   },
   "source": [
    "### 다음 장에서는\n",
    "\n",
    "채팅 템플릿은 야생의 LLM을 길들이고 인간과 적절한 대화를 할 수 있는 방법을 가르치기 위한 핵심입니다. 대화 사이에 큐 사인(또는 특수 토큰)을 적절하게 배치하면 명령 키워드에 따라 어떻게 응답해야 하는지 학습할 수 있습니다. 하지만 훈련 절차에 위험이 없는 것은 아닙니다. 활성화, 그레이디언트, 옵티마이저 모두 자신의 작업을 수행하기 위해 상당한 양의 RAM을 필요로 합니다. 메모리를 많이 소비하는 이런 구성 요소들을 만족시키려면 기술과 노력이 모두 필요합니다. 훈련 루프를 구성하려면 두려움을 떨쳐내야 합니다. 도전이 가득한 다음 장을 놓치지 마세요!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
