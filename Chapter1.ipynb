{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b08bb37-0c63-4a25-8dc3-6b32db313133",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8953a-3d20-4879-ae97-cf1211c92940",
   "metadata": {
    "id": "f1286a1d"
   },
   "source": [
    "## Chaper 1: Pay Attention to LLMs\n",
    "\n",
    "### Spoilers\n",
    "\n",
    "In this chapter, we’ll:\n",
    "\n",
    "- Briefly discuss the history of language models\n",
    "- Understand the basic elements of the Transformer architecture and the attention mechanism\n",
    "- Understand the different types of fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1d70e-1442-4040-896c-05ceccd6ffe2",
   "metadata": {
    "id": "15dd32eb"
   },
   "source": [
    "### Transformers\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/stacked_layers.png?raw=True)\n",
    "<center>Figure 1.1 - Transformer’s stacked \"layers\"</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/full_transformer.png?raw=True)\n",
    "<center>Figure 1.2 - Transformer architecture in detail</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/bert_embeddings.png?raw=True)\n",
    "<center>Figure 1.3 - Contextual word embeddings from BERT</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e70fd-2e12-4e89-ab9c-b9ac62564b42",
   "metadata": {
    "id": "3c552996"
   },
   "source": [
    "### Attention Is All You Need\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "<center>Equation 1.1 - Attention formula</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/translation_att.png?raw=True)\n",
    "<center>Figure 1.4 - Attention scores</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/multiple_keys_context.png?raw=True)\n",
    "<center>Figure 1.5 - Querying two-dimensional keys</center>\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{cos}\\theta = ||Q|| ||K|| = Q \\cdot K\n",
    "$$\n",
    "<center>Equation 1.2 - Cosine similarity, norms, and the dot product</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990aaf24-e470-4839-8111-18f1653e0ca5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "afad73ac85ea48f59c16e2a61f1510c8",
      "89e1e80f3b7341db90bbd8ba59a07fdf",
      "9cd977945b3e4a9187862714400afe4b",
      "d3fb8ebdeb9e487d8ecbf2e85941d096",
      "d227393470df454cb78d02185fb1e307",
      "ce403e58187f4c89b2ca60ae3076dda2",
      "2b3bddc81fee448da176fac3049eab9c",
      "592e09ec2eb94a2d8f1dfe4ad964d4c0",
      "48c8bb76b6804c04a5cd110b1c38d3bd",
      "cffc508084864ca48e14eab6c6ec42f0",
      "85b6de96cfef458e84932b5fcc1a715c",
      "ecd698c7df76419191fcff26867bdbc2",
      "0efbfdb15f35470ba74fe1c4df256dce",
      "6e3681031eb8463482e28a03f94046a9",
      "48493af27e6140abacbfb2b1e65bb596",
      "6b9cfcbf8d544f8da07fba6826bb8bbf",
      "5067a8bc961f47579ae39726cd3610f7",
      "84bc676c0b544794ac899fe908d27a88",
      "82cd1da7013d4a61961f4d32037794f3",
      "ddc92d1d304b4a9697f4cdbc48b5f707",
      "480d5223d8dc47efb58e0b13180fdcaf",
      "fab3b102763a4920a9717c5e3eb77eaa",
      "4db95aacdbfd4360a978a9bde335b63c",
      "22beab79575b4d0ea392b6312676fa4f",
      "02d8cb82073640e9a72bc6757096c9a4",
      "a81d4f474bb7451dbb3ff55a847d0341",
      "6b5ccf014094430e827aff9320af7d09",
      "f0d652fa12bf49e883c40e1d1137e6d8",
      "a84db27076d64e7aabb25e6f03138bbf",
      "0e36305b76be4f3dbc9f098451c18fb6",
      "62c5d8a3d5744c6689ea751eb844abbb",
      "a5aa873418764d8289339f65a924d1e6",
      "20fecd6fce2f4da1813050dcb24645a0",
      "e31849bd9370430f91459e151f2fa1df",
      "d5cbaf6b4030428bbc04aded9770cf2f",
      "8a3b538962f14ba4adeeaea308897d2f",
      "c0a47434d5534d44b834cff925badc3f",
      "d2d6b7de90b74ed7ab588e81496bb6f3",
      "b4d316a11da3401a83cedc60c2bfa31c",
      "e4935ccb704a4832ad655ff901622d05",
      "df36b4e2a36d428295d8a9f08a3edcf2",
      "711335e5486d46ba980c8e0b83fd0e85",
      "c6cbbc4672bc4ca6829751a5a8bbea10",
      "b2af3fef78aa4114b1de54ad9a271514",
      "8082e078485b4c3f8d7bad892453a366",
      "60eb3a957a584e9790f7672f411a8f34",
      "f32037d4df4b4838be12431412b8681d",
      "438607fa8cab43a6aaf8e14c246e564e",
      "e60c264e469b433db98a4a8b5f84e68e",
      "47f5a103b67b491fa59da8cb764ad87d",
      "205cf653b43a4279aeeaa68eecfdf712",
      "6a0f0939ed1f4082ac529ab9c634afee",
      "bdd577f0d89941db99b36137431c8629",
      "2ff9a3dc52cd448abe39a84d74f1e378",
      "14c42b5c7156442e8f910e5a6fb0a322"
     ]
    },
    "id": "b9fb7348",
    "outputId": "50196770-cbd3-472c-9e57-1c873fa3adf1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afad73ac85ea48f59c16e2a61f1510c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd698c7df76419191fcff26867bdbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db95aacdbfd4360a978a9bde335b63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31849bd9370430f91459e151f2fa1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8082e078485b4c3f8d7bad892453a366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "torch.manual_seed(13)\n",
    "# Made-up embedding and projection layers\n",
    "d_model = 1024\n",
    "embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "linear_query = nn.Linear(d_model, d_model)\n",
    "linear_key = nn.Linear(d_model, d_model)\n",
    "linear_value = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7cc785-1909-4982-a73d-ed05af60f229",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af735141",
    "outputId": "153f98be-41ff-4453-ffdf-8953cbd653fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3387,   263, 20254, 10541]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Just a dummy sentence'\n",
    "input_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db26e764-531c-420f-83e3-5884100a2527",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b190fc58",
    "outputId": "b3744d55-a05e-4b7c-9712-1378be3d5112"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(input_ids)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649c315e-3cf8-433a-83ca-1dc67d3c1b14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "794a0b59",
    "outputId": "f0b6b920-decc-4223-814e-c3edea9acb6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projections\n",
    "proj_key = linear_key(embeddings)\n",
    "proj_value = linear_value(embeddings)\n",
    "proj_query = linear_query(embeddings)\n",
    "# Attention scores\n",
    "dot_products = torch.matmul(proj_query, proj_key.transpose(-2, -1))\n",
    "scores = F.softmax(dot_products / np.sqrt(d_model), dim=-1)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3c5dc7-1dcf-4f4f-a591-2293084f0bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0269fc68",
    "outputId": "80b9024e-282c-42b2-c4b2-c125a15c50cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.matmul(scores, proj_value)\n",
    "context.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
