{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2778fdbd",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1286a1d",
   "metadata": {
    "id": "f1286a1d"
   },
   "source": [
    "## 1장 LLM 소개\n",
    "\n",
    "### 스포일러\n",
    "\n",
    "이 장에서는 다음과 같은 내용을 배웁니다.\n",
    "\n",
    "- 언어 모델(language model)의 간략한 역사\n",
    "- 트랜스포머(Transformer) 구조의 기본 구성 요소와 어텐션 메커니즘(attention mechanism)\n",
    "- 여러 종류의 미세 튜닝 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd32eb",
   "metadata": {
    "id": "15dd32eb"
   },
   "source": [
    "### 트랜스포머\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/stacked_layers.png?raw=True)\n",
    "<center>그림 1.1 트랜스포머 층의 스택(stack)</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/full_transformer.png?raw=True)\n",
    "<center>그림 1.2 자세한 트랜스포머 구조</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/bert_embeddings.png?raw=True)\n",
    "<center>그림 1.3 BERT의 문맥을 고려한 단어 임베딩</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c552996",
   "metadata": {
    "id": "3c552996"
   },
   "source": [
    "### Attention Is All You Need\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "<center>식 1.1 어텐션 공식</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/translation_att.png?raw=True)\n",
    "<center>그림 1.4 어텐션 점수</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/multiple_keys_context.png?raw=True)\n",
    "<center>그림 1.5 2차원 쿼리와 키</center>\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{cos}\\theta = ||Q|| ||K|| = Q \\cdot K\n",
    "$$\n",
    "<center>식 1.2 코사인 유사도에 노름을 곱하면 두 벡터의 점곱이 됩니다.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fb7348",
   "metadata": {
    "id": "b9fb7348"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "torch.manual_seed(13)\n",
    "# 임베딩과 투영 층을 만듭니다.\n",
    "d_model = 1024\n",
    "embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "linear_query = nn.Linear(d_model, d_model)\n",
    "linear_key = nn.Linear(d_model, d_model)\n",
    "linear_value = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af735141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af735141",
    "outputId": "67ccdbfa-080d-4e03-d3a0-02f1399244a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3387,   263, 20254, 10541]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Just a dummy sentence'\n",
    "input_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b190fc58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b190fc58",
    "outputId": "b319f473-f2be-477d-ee13-76dcc5b65da9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(input_ids)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794a0b59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "794a0b59",
    "outputId": "a881caf7-e64b-4b8d-84c4-930a47134404"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 투영\n",
    "proj_key = linear_key(embeddings)\n",
    "proj_value = linear_value(embeddings)\n",
    "proj_query = linear_query(embeddings)\n",
    "# 어텐션 점수\n",
    "dot_products = torch.matmul(proj_query, proj_key.transpose(-2, -1))\n",
    "scores = F.softmax(dot_products / np.sqrt(d_model), dim=-1)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0269fc68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0269fc68",
    "outputId": "955c803b-e140-4dac-fb36-bd1328454ba6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.matmul(scores, proj_value)\n",
    "context.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
