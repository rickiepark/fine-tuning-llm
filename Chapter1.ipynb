{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/fine-tuning-llm/blob/main/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1286a1d",
      "metadata": {
        "id": "f1286a1d"
      },
      "source": [
        "## Chaper 1: Pay Attention to LLMs\n",
        "\n",
        "### Spoilers\n",
        "\n",
        "In this chapter, we’ll:\n",
        "\n",
        "- Briefly discuss the history of language models\n",
        "- Understand the basic elements of the Transformer architecture and the attention mechanism\n",
        "- Understand the different types of fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dd32eb",
      "metadata": {
        "id": "15dd32eb"
      },
      "source": [
        "### Transformers\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/stacked_layers.png?raw=True)\n",
        "<center>Figure 1.1 - Transformer’s stacked \"layers\"</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/full_transformer.png?raw=True)\n",
        "<center>Figure 1.2 - Transformer architecture in detail</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/bert_embeddings.png?raw=True)\n",
        "<center>Figure 1.3 - Contextual word embeddings from BERT</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c552996",
      "metadata": {
        "id": "3c552996"
      },
      "source": [
        "### Attention Is All You Need\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "<center>Equation 1.1 - Attention formula</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/translation_att.png?raw=True)\n",
        "<center>Figure 1.4 - Attention scores</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/multiple_keys_context.png?raw=True)\n",
        "<center>Figure 1.5 - Querying two-dimensional keys</center>\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{cos}\\theta = ||Q|| ||K|| = Q \\cdot K\n",
        "$$\n",
        "<center>Equation 1.2 - Cosine similarity, norms, and the dot product</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b9fb7348",
      "metadata": {
        "id": "b9fb7348"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "vocab_size = len(tokenizer)\n",
        "\n",
        "torch.manual_seed(13)\n",
        "# Made-up embedding and projection layers\n",
        "d_model = 1024\n",
        "embedding_layer = nn.Embedding(vocab_size, d_model)\n",
        "linear_query = nn.Linear(d_model, d_model)\n",
        "linear_key = nn.Linear(d_model, d_model)\n",
        "linear_value = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "af735141",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af735141",
        "outputId": "67ccdbfa-080d-4e03-d3a0-02f1399244a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3387,   263, 20254, 10541]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "sentence = 'Just a dummy sentence'\n",
        "input_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b190fc58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b190fc58",
        "outputId": "b319f473-f2be-477d-ee13-76dcc5b65da9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "embeddings = embedding_layer(input_ids)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "794a0b59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "794a0b59",
        "outputId": "a881caf7-e64b-4b8d-84c4-930a47134404"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Projections\n",
        "proj_key = linear_key(embeddings)\n",
        "proj_value = linear_value(embeddings)\n",
        "proj_query = linear_query(embeddings)\n",
        "# Attention scores\n",
        "dot_products = torch.matmul(proj_query, proj_key.transpose(-2, -1))\n",
        "scores = F.softmax(dot_products / np.sqrt(d_model), dim=-1)\n",
        "scores.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0269fc68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0269fc68",
        "outputId": "955c803b-e140-4dac-fb36-bd1328454ba6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "context = torch.matmul(scores, proj_value)\n",
        "context.shape"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}